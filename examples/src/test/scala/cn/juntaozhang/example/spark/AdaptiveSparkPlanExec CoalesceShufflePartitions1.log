/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/bin/java -agentlib:jdwp=transport=dt_socket,address=127.0.0.1:55376,suspend=y,server=n -javaagent:/Users/juntzhang/Library/Caches/JetBrains/IdeaIC2024.3/captureAgent/debugger-agent.jar -Dkotlinx.coroutines.debug.enable.creation.stack.trace=false -Ddebugger.agent.enable.coroutines=true -Dkotlinx.coroutines.debug.enable.flows.stack.trace=true -Dkotlinx.coroutines.debug.enable.mutable.state.flows.stack.trace=true -Dfile.encoding=UTF-8 -classpath "/Users/juntzhang/Library/Application Support/JetBrains/IdeaIC2024.3/plugins/Scala/lib/runners.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/cat.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/crs-agent.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/js-engine.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/legacy8ujsse.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/openeddsa.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/openjsse.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/rhino-1.7R4.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/xml-factory-wrapper-1.0.0.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/rt.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/examples/target/scala-2.12/test-classes:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/examples/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/core/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/apache/avro/avro/1.11.0/avro-1.11.0.jar:/Users/juntzhang/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.13.4/jackson-core-2.13.4.jar:/Users/juntzhang/.m2/repository/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar:/Users/juntzhang/.m2/repository/org/apache/avro/avro-mapred/1.11.0/avro-mapred-1.11.0.jar:/Users/juntzhang/.m2/repository/org/apache/avro/avro-ipc/1.11.0/avro-ipc-1.11.0.jar:/Users/juntzhang/.m2/repository/org/tukaani/xz/1.8/xz-1.8.jar:/Users/juntzhang/.m2/repository/com/twitter/chill_2.12/0.10.0/chill_2.12-0.10.0.jar:/Users/juntzhang/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/Users/juntzhang/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/juntzhang/.m2/repository/com/twitter/chill-java/0.10.0/chill-java-0.10.0.jar:/Users/juntzhang/.m2/repository/org/apache/xbean/xbean-asm9-shaded/4.20/xbean-asm9-shaded-4.20.jar:/Users/juntzhang/.m2/repository/org/apache/hadoop/hadoop-client-api/3.3.2/hadoop-client-api-3.3.2.jar:/Users/juntzhang/.m2/repository/org/apache/hadoop/hadoop-client-runtime/3.3.2/hadoop-client-runtime-3.3.2.jar:/Users/juntzhang/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/launcher/target/scala-2.12/classes:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/common/kvstore/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/juntzhang/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.13.4/jackson-annotations-2.13.4.jar:/Users/juntzhang/.m2/repository/org/rocksdb/rocksdbjni/6.20.3/rocksdbjni-6.20.3.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/common/network-common/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/Users/juntzhang/.m2/repository/com/google/crypto/tink/tink/1.6.1/tink-1.6.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/common/network-shuffle/target/scala-2.12/classes:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/common/unsafe/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/Users/juntzhang/.m2/repository/org/apache/curator/curator-recipes/2.13.0/curator-recipes-2.13.0.jar:/Users/juntzhang/.m2/repository/org/apache/curator/curator-framework/2.13.0/curator-framework-2.13.0.jar:/Users/juntzhang/.m2/repository/org/apache/curator/curator-client/2.13.0/curator-client-2.13.0.jar:/Users/juntzhang/.m2/repository/org/apache/zookeeper/zookeeper/3.6.2/zookeeper-3.6.2.jar:/Users/juntzhang/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/juntzhang/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.6.2/zookeeper-jute-3.6.2.jar:/Users/juntzhang/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-plus/9.4.48.v20220622/jetty-plus-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-webapp/9.4.48.v20220622/jetty-webapp-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-xml/9.4.48.v20220622/jetty-xml-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-jndi/9.4.48.v20220622/jetty-jndi-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-security/9.4.48.v20220622/jetty-security-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-util/9.4.48.v20220622/jetty-util-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-server/9.4.48.v20220622/jetty-server-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-io/9.4.48.v20220622/jetty-io-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-http/9.4.48.v20220622/jetty-http-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-continuation/9.4.48.v20220622/jetty-continuation-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-servlet/9.4.48.v20220622/jetty-servlet-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-util-ajax/9.4.48.v20220622/jetty-util-ajax-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-proxy/9.4.48.v20220622/jetty-proxy-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-client/9.4.48.v20220622/jetty-client-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-servlets/9.4.48.v20220622/jetty-servlets-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/jakarta/servlet/jakarta.servlet-api/4.0.3/jakarta.servlet-api-4.0.3.jar:/Users/juntzhang/.m2/repository/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/Users/juntzhang/.m2/repository/org/apache/commons/commons-lang3/3.12.0/commons-lang3-3.12.0.jar:/Users/juntzhang/.m2/repository/org/apache/commons/commons-text/1.9/commons-text-1.9.jar:/Users/juntzhang/.m2/repository/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar:/Users/juntzhang/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/juntzhang/.m2/repository/org/apache/commons/commons-collections4/4.4/commons-collections4-4.4.jar:/Users/juntzhang/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/juntzhang/.m2/repository/org/slf4j/slf4j-api/1.7.32/slf4j-api-1.7.32.jar:/Users/juntzhang/.m2/repository/org/slf4j/jul-to-slf4j/1.7.32/jul-to-slf4j-1.7.32.jar:/Users/juntzhang/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.32/jcl-over-slf4j-1.7.32.jar:/Users/juntzhang/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.17.2/log4j-slf4j-impl-2.17.2.jar:/Users/juntzhang/.m2/repository/org/apache/logging/log4j/log4j-api/2.17.2/log4j-api-2.17.2.jar:/Users/juntzhang/.m2/repository/org/apache/logging/log4j/log4j-core/2.17.2/log4j-core-2.17.2.jar:/Users/juntzhang/.m2/repository/org/apache/logging/log4j/log4j-1.2-api/2.17.2/log4j-1.2-api-2.17.2.jar:/Users/juntzhang/.m2/repository/com/ning/compress-lzf/1.1/compress-lzf-1.1.jar:/Users/juntzhang/.m2/repository/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar:/Users/juntzhang/.m2/repository/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar:/Users/juntzhang/.m2/repository/com/github/luben/zstd-jni/1.5.2-1/zstd-jni-1.5.2-1.jar:/Users/juntzhang/.m2/repository/org/roaringbitmap/RoaringBitmap/0.9.25/RoaringBitmap-0.9.25.jar:/Users/juntzhang/.m2/repository/org/roaringbitmap/shims/0.9.25/shims-0.9.25.jar:/Users/juntzhang/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.2.0/scala-xml_2.12-1.2.0.jar:/Users/juntzhang/.m2/repository/org/scala-lang/scala-reflect/2.12.15/scala-reflect-2.12.15.jar:/Users/juntzhang/.m2/repository/org/json4s/json4s-jackson_2.12/3.7.0-M11/json4s-jackson_2.12-3.7.0-M11.jar:/Users/juntzhang/.m2/repository/org/json4s/json4s-core_2.12/3.7.0-M11/json4s-core_2.12-3.7.0-M11.jar:/Users/juntzhang/.m2/repository/org/json4s/json4s-ast_2.12/3.7.0-M11/json4s-ast_2.12-3.7.0-M11.jar:/Users/juntzhang/.m2/repository/org/json4s/json4s-scalap_2.12/3.7.0-M11/json4s-scalap_2.12-3.7.0-M11.jar:/Users/juntzhang/.m2/repository/org/glassfish/jersey/core/jersey-client/2.36/jersey-client-2.36.jar:/Users/juntzhang/.m2/repository/jakarta/ws/rs/jakarta.ws.rs-api/2.1.6/jakarta.ws.rs-api-2.1.6.jar:/Users/juntzhang/.m2/repository/org/glassfish/hk2/external/jakarta.inject/2.6.1/jakarta.inject-2.6.1.jar:/Users/juntzhang/.m2/repository/org/glassfish/jersey/core/jersey-common/2.36/jersey-common-2.36.jar:/Users/juntzhang/.m2/repository/jakarta/annotation/jakarta.annotation-api/1.3.5/jakarta.annotation-api-1.3.5.jar:/Users/juntzhang/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.3/osgi-resource-locator-1.0.3.jar:/Users/juntzhang/.m2/repository/org/glassfish/jersey/core/jersey-server/2.36/jersey-server-2.36.jar:/Users/juntzhang/.m2/repository/jakarta/validation/jakarta.validation-api/2.0.2/jakarta.validation-api-2.0.2.jar:/Users/juntzhang/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.36/jersey-container-servlet-2.36.jar:/Users/juntzhang/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.36/jersey-container-servlet-core-2.36.jar:/Users/juntzhang/.m2/repository/org/glassfish/jersey/inject/jersey-hk2/2.36/jersey-hk2-2.36.jar:/Users/juntzhang/.m2/repository/org/glassfish/hk2/hk2-locator/2.6.1/hk2-locator-2.6.1.jar:/Users/juntzhang/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.6.1/aopalliance-repackaged-2.6.1.jar:/Users/juntzhang/.m2/repository/org/glassfish/hk2/hk2-api/2.6.1/hk2-api-2.6.1.jar:/Users/juntzhang/.m2/repository/org/glassfish/hk2/hk2-utils/2.6.1/hk2-utils-2.6.1.jar:/Users/juntzhang/.m2/repository/org/javassist/javassist/3.25.0-GA/javassist-3.25.0-GA.jar:/Users/juntzhang/.m2/repository/io/netty/netty-all/4.1.74.Final/netty-all-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-buffer/4.1.74.Final/netty-buffer-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-codec/4.1.74.Final/netty-codec-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-common/4.1.74.Final/netty-common-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-handler/4.1.74.Final/netty-handler-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-tcnative-classes/2.0.48.Final/netty-tcnative-classes-2.0.48.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-resolver/4.1.74.Final/netty-resolver-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-transport/4.1.74.Final/netty-transport-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-transport-classes-epoll/4.1.74.Final/netty-transport-classes-epoll-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.74.Final/netty-transport-native-unix-common-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-transport-classes-kqueue/4.1.74.Final/netty-transport-classes-kqueue-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-transport-native-epoll/4.1.74.Final/netty-transport-native-epoll-4.1.74.Final-linux-x86_64.jar:/Users/juntzhang/.m2/repository/io/netty/netty-transport-native-epoll/4.1.74.Final/netty-transport-native-epoll-4.1.74.Final-linux-aarch_64.jar:/Users/juntzhang/.m2/repository/io/netty/netty-transport-native-kqueue/4.1.74.Final/netty-transport-native-kqueue-4.1.74.Final-osx-x86_64.jar:/Users/juntzhang/.m2/repository/io/netty/netty-transport-native-kqueue/4.1.74.Final/netty-transport-native-kqueue-4.1.74.Final-osx-aarch_64.jar:/Users/juntzhang/.m2/repository/com/clearspring/analytics/stream/2.9.6/stream-2.9.6.jar:/Users/juntzhang/.m2/repository/io/dropwizard/metrics/metrics-core/4.2.7/metrics-core-4.2.7.jar:/Users/juntzhang/.m2/repository/io/dropwizard/metrics/metrics-jvm/4.2.7/metrics-jvm-4.2.7.jar:/Users/juntzhang/.m2/repository/io/dropwizard/metrics/metrics-json/4.2.7/metrics-json-4.2.7.jar:/Users/juntzhang/.m2/repository/io/dropwizard/metrics/metrics-graphite/4.2.7/metrics-graphite-4.2.7.jar:/Users/juntzhang/.m2/repository/io/dropwizard/metrics/metrics-jmx/4.2.7/metrics-jmx-4.2.7.jar:/Users/juntzhang/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.13.4.1/jackson-databind-2.13.4.1.jar:/Users/juntzhang/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.13.4/jackson-module-scala_2.12-2.13.4.jar:/Users/juntzhang/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/Users/juntzhang/.m2/repository/org/apache/ivy/ivy/2.5.0/ivy-2.5.0.jar:/Users/juntzhang/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/juntzhang/.m2/repository/net/razorvine/pickle/1.2/pickle-1.2.jar:/Users/juntzhang/.m2/repository/net/sf/py4j/py4j/0.10.9.5/py4j-0.10.9.5.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/common/tags/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/apache/commons/commons-crypto/1.1.0/commons-crypto-1.1.0.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/streaming/target/scala-2.12/classes:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/mllib/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.12/1.1.2/scala-parser-combinators_2.12-1.1.2.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/sql/core/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/com/univocity/univocity-parsers/2.9.1/univocity-parsers-2.9.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/common/sketch/target/scala-2.12/classes:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/sql/catalyst/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/codehaus/janino/janino/3.0.16/janino-3.0.16.jar:/Users/juntzhang/.m2/repository/org/codehaus/janino/commons-compiler/3.0.16/commons-compiler-3.0.16.jar:/Users/juntzhang/.m2/repository/org/antlr/antlr4-runtime/4.8/antlr4-runtime-4.8.jar:/Users/juntzhang/.m2/repository/org/apache/arrow/arrow-vector/7.0.0/arrow-vector-7.0.0.jar:/Users/juntzhang/.m2/repository/org/apache/arrow/arrow-format/7.0.0/arrow-format-7.0.0.jar:/Users/juntzhang/.m2/repository/org/apache/arrow/arrow-memory-core/7.0.0/arrow-memory-core-7.0.0.jar:/Users/juntzhang/.m2/repository/com/google/flatbuffers/flatbuffers-java/1.12.0/flatbuffers-java-1.12.0.jar:/Users/juntzhang/.m2/repository/org/apache/arrow/arrow-memory-netty/7.0.0/arrow-memory-netty-7.0.0.jar:/Users/juntzhang/.m2/repository/org/apache/orc/orc-core/1.7.6/orc-core-1.7.6.jar:/Users/juntzhang/.m2/repository/org/apache/orc/orc-shims/1.7.6/orc-shims-1.7.6.jar:/Users/juntzhang/.m2/repository/io/airlift/aircompressor/0.21/aircompressor-0.21.jar:/Users/juntzhang/.m2/repository/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/Users/juntzhang/.m2/repository/org/threeten/threeten-extra/1.5.0/threeten-extra-1.5.0.jar:/Users/juntzhang/.m2/repository/org/apache/orc/orc-mapreduce/1.7.6/orc-mapreduce-1.7.6.jar:/Users/juntzhang/.m2/repository/org/apache/hive/hive-storage-api/2.7.2/hive-storage-api-2.7.2.jar:/Users/juntzhang/.m2/repository/org/apache/parquet/parquet-column/1.12.2/parquet-column-1.12.2.jar:/Users/juntzhang/.m2/repository/org/apache/parquet/parquet-common/1.12.2/parquet-common-1.12.2.jar:/Users/juntzhang/.m2/repository/org/apache/parquet/parquet-encoding/1.12.2/parquet-encoding-1.12.2.jar:/Users/juntzhang/.m2/repository/org/apache/parquet/parquet-hadoop/1.12.2/parquet-hadoop-1.12.2.jar:/Users/juntzhang/.m2/repository/org/apache/parquet/parquet-format-structures/1.12.2/parquet-format-structures-1.12.2.jar:/Users/juntzhang/.m2/repository/org/apache/parquet/parquet-jackson/1.12.2/parquet-jackson-1.12.2.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/mllib-local/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/scalanlp/breeze_2.12/1.2/breeze_2.12-1.2.jar:/Users/juntzhang/.m2/repository/org/scalanlp/breeze-macros_2.12/1.2/breeze-macros_2.12-1.2.jar:/Users/juntzhang/.m2/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/juntzhang/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/juntzhang/.m2/repository/com/github/wendykierp/JTransforms/3.1/JTransforms-3.1.jar:/Users/juntzhang/.m2/repository/pl/edu/icm/JLargeArrays/1.5/JLargeArrays-1.5.jar:/Users/juntzhang/.m2/repository/com/chuusai/shapeless_2.12/2.3.7/shapeless_2.12-2.3.7.jar:/Users/juntzhang/.m2/repository/org/typelevel/spire_2.12/0.17.0/spire_2.12-0.17.0.jar:/Users/juntzhang/.m2/repository/org/typelevel/spire-macros_2.12/0.17.0/spire-macros_2.12-0.17.0.jar:/Users/juntzhang/.m2/repository/org/typelevel/spire-platform_2.12/0.17.0/spire-platform_2.12-0.17.0.jar:/Users/juntzhang/.m2/repository/org/typelevel/spire-util_2.12/0.17.0/spire-util_2.12-0.17.0.jar:/Users/juntzhang/.m2/repository/org/typelevel/algebra_2.12/2.0.1/algebra_2.12-2.0.1.jar:/Users/juntzhang/.m2/repository/org/typelevel/cats-kernel_2.12/2.1.1/cats-kernel_2.12-2.1.1.jar:/Users/juntzhang/.m2/repository/org/scala-lang/modules/scala-collection-compat_2.12/2.1.1/scala-collection-compat_2.12-2.1.1.jar:/Users/juntzhang/.m2/repository/org/jpmml/pmml-model/1.4.8/pmml-model-1.4.8.jar:/Users/juntzhang/.m2/repository/org/glassfish/jaxb/jaxb-runtime/2.3.2/jaxb-runtime-2.3.2.jar:/Users/juntzhang/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/2.3.2/jakarta.xml.bind-api-2.3.2.jar:/Users/juntzhang/.m2/repository/com/sun/istack/istack-commons-runtime/3.0.8/istack-commons-runtime-3.0.8.jar:/Users/juntzhang/.m2/repository/dev/ludovic/netlib/blas/2.2.1/blas-2.2.1.jar:/Users/juntzhang/.m2/repository/dev/ludovic/netlib/lapack/2.2.1/lapack-2.2.1.jar:/Users/juntzhang/.m2/repository/dev/ludovic/netlib/arpack/2.2.1/arpack-2.2.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/sql/hive/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/apache/hive/hive-common/2.3.9/hive-common-2.3.9.jar:/Users/juntzhang/.m2/repository/commons-cli/commons-cli/1.5.0/commons-cli-1.5.0.jar:/Users/juntzhang/.m2/repository/jline/jline/2.14.6/jline-2.14.6.jar:/Users/juntzhang/.m2/repository/com/tdunning/json/1.8/json-1.8.jar:/Users/juntzhang/.m2/repository/com/github/joshelser/dropwizard-metrics-hadoop-metrics2-reporter/0.1.2/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/Users/juntzhang/.m2/repository/org/apache/hive/hive-exec/2.3.9/hive-exec-2.3.9-core.jar:/Users/juntzhang/.m2/repository/org/apache/hive/hive-vector-code-gen/2.3.9/hive-vector-code-gen-2.3.9.jar:/Users/juntzhang/.m2/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/Users/juntzhang/.m2/repository/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/Users/juntzhang/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/Users/juntzhang/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/juntzhang/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/Users/juntzhang/.m2/repository/org/apache/hive/hive-metastore/2.3.9/hive-metastore-2.3.9.jar:/Users/juntzhang/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/Users/juntzhang/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/juntzhang/.m2/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/Users/juntzhang/.m2/repository/com/zaxxer/HikariCP/2.5.1/HikariCP-2.5.1.jar:/Users/juntzhang/.m2/repository/org/datanucleus/datanucleus-api-jdo/4.2.4/datanucleus-api-jdo-4.2.4.jar:/Users/juntzhang/.m2/repository/org/datanucleus/datanucleus-rdbms/4.1.19/datanucleus-rdbms-4.1.19.jar:/Users/juntzhang/.m2/repository/commons-pool/commons-pool/1.5.4/commons-pool-1.5.4.jar:/Users/juntzhang/.m2/repository/commons-dbcp/commons-dbcp/1.4/commons-dbcp-1.4.jar:/Users/juntzhang/.m2/repository/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar:/Users/juntzhang/.m2/repository/javax/transaction/jta/1.1/jta-1.1.jar:/Users/juntzhang/.m2/repository/org/datanucleus/javax.jdo/3.2.0-m3/javax.jdo-3.2.0-m3.jar:/Users/juntzhang/.m2/repository/javax/transaction/transaction-api/1.1/transaction-api-1.1.jar:/Users/juntzhang/.m2/repository/org/apache/hive/hive-serde/2.3.9/hive-serde-2.3.9.jar:/Users/juntzhang/.m2/repository/org/apache/hive/hive-shims/2.3.9/hive-shims-2.3.9.jar:/Users/juntzhang/.m2/repository/org/apache/hive/shims/hive-shims-common/2.3.9/hive-shims-common-2.3.9.jar:/Users/juntzhang/.m2/repository/org/apache/hive/shims/hive-shims-0.23/2.3.9/hive-shims-0.23-2.3.9.jar:/Users/juntzhang/.m2/repository/org/apache/hive/shims/hive-shims-scheduler/2.3.9/hive-shims-scheduler-2.3.9.jar:/Users/juntzhang/.m2/repository/org/apache/hive/hive-llap-common/2.3.9/hive-llap-common-2.3.9.jar:/Users/juntzhang/.m2/repository/org/apache/hive/hive-llap-client/2.3.9/hive-llap-client-2.3.9.jar:/Users/juntzhang/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/Users/juntzhang/.m2/repository/org/apache/httpcomponents/httpcore/4.4.14/httpcore-4.4.14.jar:/Users/juntzhang/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/Users/juntzhang/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/Users/juntzhang/.m2/repository/joda-time/joda-time/2.10.13/joda-time-2.10.13.jar:/Users/juntzhang/.m2/repository/org/jodd/jodd-core/3.5.2/jodd-core-3.5.2.jar:/Users/juntzhang/.m2/repository/org/datanucleus/datanucleus-core/4.1.17/datanucleus-core-4.1.17.jar:/Users/juntzhang/.m2/repository/org/apache/thrift/libthrift/0.12.0/libthrift-0.12.0.jar:/Users/juntzhang/.m2/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/Users/juntzhang/.m2/repository/org/apache/derby/derby/10.14.2.0/derby-10.14.2.0.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/graphx/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/external/kafka-0-10/target/scala-2.12/classes:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/external/kafka-0-10-token-provider/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/apache/kafka/kafka-clients/2.8.1/kafka-clients-2.8.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/external/kafka-0-10-sql/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar:/Users/juntzhang/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/Users/juntzhang/.m2/repository/org/scalacheck/scalacheck_2.12/1.15.4/scalacheck_2.12-1.15.4.jar:/Users/juntzhang/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/Users/juntzhang/.m2/repository/org/scala-lang/scala-library/2.12.15/scala-library-2.12.15.jar:/Users/juntzhang/.m2/repository/com/github/scopt/scopt_2.12/3.7.1/scopt_2.12-3.7.1.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest_2.12/3.3.0-SNAP3/scalatest_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-core_2.12/3.3.0-SNAP3/scalatest-core_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-compatible/3.3.0-SNAP3/scalatest-compatible-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalactic/scalactic_2.12/3.3.0-SNAP3/scalactic_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-featurespec_2.12/3.3.0-SNAP3/scalatest-featurespec_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-flatspec_2.12/3.3.0-SNAP3/scalatest-flatspec_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-freespec_2.12/3.3.0-SNAP3/scalatest-freespec_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-funsuite_2.12/3.3.0-SNAP3/scalatest-funsuite_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-funspec_2.12/3.3.0-SNAP3/scalatest-funspec_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-propspec_2.12/3.3.0-SNAP3/scalatest-propspec_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-refspec_2.12/3.3.0-SNAP3/scalatest-refspec_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-wordspec_2.12/3.3.0-SNAP3/scalatest-wordspec_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-diagrams_2.12/3.3.0-SNAP3/scalatest-diagrams_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-matchers-core_2.12/3.3.0-SNAP3/scalatest-matchers-core_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-shouldmatchers_2.12/3.3.0-SNAP3/scalatest-shouldmatchers_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-mustmatchers_2.12/3.3.0-SNAP3/scalatest-mustmatchers_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatestplus/scalacheck-1-15_2.12/3.3.0.0-SNAP3/scalacheck-1-15_2.12-3.3.0.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatestplus/mockito-4-2_2.12/3.2.11.0/mockito-4-2_2.12-3.2.11.0.jar:/Users/juntzhang/.m2/repository/org/mockito/mockito-core/4.2.0/mockito-core-4.2.0.jar:/Users/juntzhang/.m2/repository/net/bytebuddy/byte-buddy/1.12.4/byte-buddy-1.12.4.jar:/Users/juntzhang/.m2/repository/net/bytebuddy/byte-buddy-agent/1.12.4/byte-buddy-agent-1.12.4.jar:/Users/juntzhang/.m2/repository/org/objenesis/objenesis/3.2/objenesis-3.2.jar:/Users/juntzhang/.m2/repository/org/scalatestplus/selenium-3-141_2.12/3.3.0.0-SNAP3/selenium-3-141_2.12-3.3.0.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-java/3.141.59/selenium-java-3.141.59.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-api/3.141.59/selenium-api-3.141.59.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-chrome-driver/3.141.59/selenium-chrome-driver-3.141.59.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-edge-driver/3.141.59/selenium-edge-driver-3.141.59.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-firefox-driver/3.141.59/selenium-firefox-driver-3.141.59.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-ie-driver/3.141.59/selenium-ie-driver-3.141.59.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-opera-driver/3.141.59/selenium-opera-driver-3.141.59.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-remote-driver/3.141.59/selenium-remote-driver-3.141.59.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-safari-driver/3.141.59/selenium-safari-driver-3.141.59.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-support/3.141.59/selenium-support-3.141.59.jar:/Users/juntzhang/.m2/repository/org/apache/commons/commons-exec/1.3/commons-exec-1.3.jar:/Users/juntzhang/.m2/repository/com/squareup/okhttp3/okhttp/3.11.0/okhttp-3.11.0.jar:/Users/juntzhang/.m2/repository/com/squareup/okio/okio/1.14.0/okio-1.14.0.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/htmlunit-driver/2.50.0/htmlunit-driver-2.50.0.jar:/Users/juntzhang/.m2/repository/net/sourceforge/htmlunit/htmlunit/2.50.0/htmlunit-2.50.0.jar:/Users/juntzhang/.m2/repository/xalan/xalan/2.7.2/xalan-2.7.2.jar:/Users/juntzhang/.m2/repository/xalan/serializer/2.7.2/serializer-2.7.2.jar:/Users/juntzhang/.m2/repository/org/apache/httpcomponents/httpmime/4.5.13/httpmime-4.5.13.jar:/Users/juntzhang/.m2/repository/net/sourceforge/htmlunit/htmlunit-core-js/2.50.0/htmlunit-core-js-2.50.0.jar:/Users/juntzhang/.m2/repository/net/sourceforge/htmlunit/neko-htmlunit/2.50.0/neko-htmlunit-2.50.0.jar:/Users/juntzhang/.m2/repository/xerces/xercesImpl/2.12.2/xercesImpl-2.12.2.jar:/Users/juntzhang/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/Users/juntzhang/.m2/repository/net/sourceforge/htmlunit/htmlunit-cssparser/1.7.0/htmlunit-cssparser-1.7.0.jar:/Users/juntzhang/.m2/repository/commons-net/commons-net/3.8.0/commons-net-3.8.0.jar:/Users/juntzhang/.m2/repository/org/brotli/dec/0.1.2/dec-0.1.2.jar:/Users/juntzhang/.m2/repository/com/shapesecurity/salvation2/3.0.0/salvation2-3.0.0.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/websocket/websocket-client/9.4.40.v20210413/websocket-client-9.4.40.v20210413.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/websocket/websocket-common/9.4.40.v20210413/websocket-common-9.4.40.v20210413.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/websocket/websocket-api/9.4.40.v20210413/websocket-api-9.4.40.v20210413.jar:/Users/juntzhang/.m2/repository/junit/junit/4.13.2/junit-4.13.2.jar:/Users/juntzhang/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/juntzhang/.m2/repository/com/github/sbt/junit-interface/0.13.3/junit-interface-0.13.3.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/core/target/spark-core_2.12-3.3.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/launcher/target/spark-launcher_2.12-3.3.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/common/kvstore/target/spark-kvstore_2.12-3.3.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/common/network-common/target/spark-network-common_2.12-3.3.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/common/network-shuffle/target/spark-network-shuffle_2.12-3.3.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/common/unsafe/target/spark-unsafe_2.12-3.3.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/common/tags/target/spark-tags_2.12-3.3.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/streaming/target/spark-streaming_2.12-3.3.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/mllib/target/spark-mllib_2.12-3.3.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/sql/core/target/spark-sql_2.12-3.3.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/common/sketch/target/spark-sketch_2.12-3.3.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/sql/catalyst/target/spark-catalyst_2.12-3.3.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/mllib-local/target/spark-mllib-local_2.12-3.3.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/sql/hive/target/spark-hive_2.12-3.3.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/graphx/target/spark-graphx_2.12-3.3.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/external/kafka-0-10/target/spark-streaming-kafka-0-10_2.12-3.3.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/external/kafka-0-10-token-provider/target/spark-token-provider-kafka-0-10_2.12-3.3.1.jar:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/external/kafka-0-10-sql/target/spark-sql-kafka-0-10_2.12-3.3.1.jar:/Applications/IntelliJ IDEA CE.app/Contents/lib/idea_rt.jar" org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner -s cn.juntaozhang.example.spark.PhysicalPlanSpec -testName "AdaptiveSparkPlanExec CoalesceShufflePartitions1" -showProgressMessages true
Testing started at 16:56 ...
Connected to the target VM, address: '127.0.0.1:55376', transport: 'socket'
16:56:14.775 [ScalaTest-run] INFO  org.apache.spark.SparkContext - Running Spark version 3.3.1
16:56:14.853 [ScalaTest-run] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16:56:14.908 [ScalaTest-run] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
16:56:14.908 [ScalaTest-run] INFO  org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
16:56:14.908 [ScalaTest-run] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
16:56:14.908 [ScalaTest-run] INFO  org.apache.spark.SparkContext - Submitted application: LogicalPlanTest
16:56:14.927 [ScalaTest-run] INFO  org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
16:56:14.936 [ScalaTest-run] INFO  org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
16:56:14.936 [ScalaTest-run] INFO  org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
16:56:14.991 [ScalaTest-run] INFO  org.apache.spark.SecurityManager - Changing view acls to: juntzhang
16:56:14.991 [ScalaTest-run] INFO  org.apache.spark.SecurityManager - Changing modify acls to: juntzhang
16:56:14.991 [ScalaTest-run] INFO  org.apache.spark.SecurityManager - Changing view acls groups to:
16:56:14.992 [ScalaTest-run] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to:
16:56:14.992 [ScalaTest-run] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(juntzhang); groups with view permissions: Set(); users  with modify permissions: Set(juntzhang); groups with modify permissions: Set()
16:56:15.067 [ScalaTest-run] DEBUG io.netty.util.internal.logging.InternalLoggerFactory - Using SLF4J as the default logging framework
16:56:15.073 [ScalaTest-run] DEBUG io.netty.util.internal.InternalThreadLocalMap - -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
16:56:15.073 [ScalaTest-run] DEBUG io.netty.util.internal.InternalThreadLocalMap - -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
16:56:15.083 [ScalaTest-run] DEBUG io.netty.channel.MultithreadEventLoopGroup - -Dio.netty.eventLoopThreads: 20
16:56:15.111 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - -Dio.netty.noUnsafe: false
16:56:15.112 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - Java version: 8
16:56:15.112 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - sun.misc.Unsafe.theUnsafe: available
16:56:15.112 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - sun.misc.Unsafe.copyMemory: available
16:56:15.113 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - java.nio.Buffer.address: available
16:56:15.113 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - direct buffer constructor: available
16:56:15.113 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - java.nio.Bits.unaligned: available, true
16:56:15.113 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
16:56:15.113 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - java.nio.DirectByteBuffer.<init>(long, int): available
16:56:15.113 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - sun.misc.Unsafe: available
16:56:15.113 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.tmpdir: /var/folders/dc/_x4vzy4100s9qz48fghk10h80000gq/T (java.io.tmpdir)
16:56:15.114 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.bitMode: 64 (sun.arch.data.model)
16:56:15.114 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - Platform: MacOS
16:56:15.114 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.maxDirectMemory: 15271460864 bytes
16:56:15.114 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.uninitializedArrayAllocationThreshold: -1
16:56:15.115 [ScalaTest-run] DEBUG io.netty.util.internal.CleanerJava6 - java.nio.ByteBuffer.cleaner(): available
16:56:15.115 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.noPreferDirect: false
16:56:15.115 [ScalaTest-run] DEBUG io.netty.channel.nio.NioEventLoop - -Dio.netty.noKeySetOptimization: false
16:56:15.115 [ScalaTest-run] DEBUG io.netty.channel.nio.NioEventLoop - -Dio.netty.selectorAutoRebuildThreshold: 512
16:56:15.120 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - org.jctools-core.MpscChunkedArrayQueue: available
16:56:15.135 [ScalaTest-run] DEBUG io.netty.util.ResourceLeakDetector - -Dio.netty.leakDetection.level: simple
16:56:15.135 [ScalaTest-run] DEBUG io.netty.util.ResourceLeakDetector - -Dio.netty.leakDetection.targetRecords: 4
16:56:15.137 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.numHeapArenas: 20
16:56:15.137 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.numDirectArenas: 20
16:56:15.137 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.pageSize: 8192
16:56:15.137 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxOrder: 11
16:56:15.137 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.chunkSize: 16777216
16:56:15.137 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.smallCacheSize: 256
16:56:15.137 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.normalCacheSize: 64
16:56:15.137 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxCachedBufferCapacity: 32768
16:56:15.137 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.cacheTrimInterval: 8192
16:56:15.137 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.cacheTrimIntervalMillis: 0
16:56:15.137 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.useCacheForAllThreads: true
16:56:15.137 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
16:56:15.165 [ScalaTest-run] DEBUG io.netty.channel.DefaultChannelId - -Dio.netty.processId: 93810 (auto-detected)
16:56:15.166 [ScalaTest-run] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv4Stack: false
16:56:15.166 [ScalaTest-run] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv6Addresses: false
16:56:15.171 [ScalaTest-run] DEBUG io.netty.util.NetUtilInitializations - Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
16:56:15.171 [ScalaTest-run] DEBUG io.netty.util.NetUtil - Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
16:56:15.173 [ScalaTest-run] DEBUG io.netty.channel.DefaultChannelId - -Dio.netty.machineId: be:d0:74:ff:fe:47:b6:64 (auto-detected)
16:56:15.216 [ScalaTest-run] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.allocator.type: pooled
16:56:15.217 [ScalaTest-run] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.threadLocalDirectBufferSize: 0
16:56:15.217 [ScalaTest-run] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.maxThreadLocalCharBufferSize: 16384
16:56:15.229 [ScalaTest-run] DEBUG org.apache.spark.network.server.TransportServer - Shuffle server started on port: 55389
16:56:15.236 [ScalaTest-run] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 55389.
16:56:15.237 [ScalaTest-run] DEBUG org.apache.spark.SparkEnv - Using serializer: class org.apache.spark.serializer.JavaSerializer
16:56:15.263 [ScalaTest-run] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
16:56:15.264 [ScalaTest-run] DEBUG org.apache.spark.MapOutputTrackerMasterEndpoint - init
16:56:15.302 [ScalaTest-run] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:56:15.318 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:56:15.318 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:56:15.321 [ScalaTest-run] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
16:56:15.351 [ScalaTest-run] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /private/var/folders/dc/_x4vzy4100s9qz48fghk10h80000gq/T/blockmgr-420a29ee-40f6-4d34-aefa-4258853e55ac
16:56:15.353 [ScalaTest-run] DEBUG org.apache.spark.storage.DiskBlockManager - Adding shutdown hook
16:56:15.354 [ScalaTest-run] DEBUG org.apache.spark.util.ShutdownHookManager - Adding shutdown hook
16:56:15.376 [ScalaTest-run] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 8.4 GiB
16:56:15.390 [ScalaTest-run] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:56:15.390 [ScalaTest-run] DEBUG org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - init
16:56:15.402 [ScalaTest-run] DEBUG org.apache.spark.SecurityManager - Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
16:56:15.556 [ScalaTest-run] DEBUG org.apache.spark.ui.JettyUtils - Using requestHeaderSize: 8192
16:56:15.578 [ScalaTest-run] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:56:15.661 [ScalaTest-run] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host 172.16.43.180
16:56:15.668 [ScalaTest-run] INFO  org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
16:56:15.683 [ScalaTest-run] DEBUG org.apache.spark.network.server.TransportServer - Shuffle server started on port: 55390
16:56:15.683 [ScalaTest-run] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55390.
16:56:15.683 [ScalaTest-run] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 172.16.43.180:55390
16:56:15.685 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:56:15.690 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 172.16.43.180, 55390, None)
16:56:15.693 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 172.16.43.180
16:56:15.693 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 172.16.43.180:55390 with 8.4 GiB RAM, BlockManagerId(driver, 172.16.43.180, 55390, None)
16:56:15.696 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 172.16.43.180, 55390, None)
16:56:15.697 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 172.16.43.180, 55390, None)
16:56:15.880 [ScalaTest-run] DEBUG org.apache.spark.SparkContext - Adding shutdown hook



16:56:16.042 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
16:56:16.043 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.app.name -> LogicalPlanTest
16:56:16.043 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.master -> local
16:56:16.043 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.internal.SharedState - Applying static initial session options to SparkConf: spark.sql.catalogImplementation -> hive
16:56:16.047 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/spark-warehouse'.
16:56:16.850 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: drop table if exists t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  droptableifexistst1 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  droptableifexistst1 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DropTableContext =>  drop table if exists t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  t1
16:56:17.061 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: spark_grouping_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleMultipartIdentifierContext =>  spark_grouping_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  spark_grouping_id
16:56:17.267 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
16:56:17.585 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 2.3.9) is file:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/spark-warehouse
16:56:17.737 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.stats.jdbc.timeout does not exist
16:56:17.737 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.stats.retries.wait does not exist
16:56:18.149 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG com.jolbox.bonecp.BoneCPDataSource - JDBC URL = jdbc:derby:;databaseName=metastore_db;create=true, Username = APP, partitions = 1, max (per partition) = 10, min (per partition) = 0, idle max age = 60 min, idle test period = 240 min, strategy = DEFAULT
16:56:19.779 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG com.jolbox.bonecp.BoneCPDataSource - JDBC URL = jdbc:derby:;databaseName=metastore_db;create=true, Username = APP, partitions = 1, max (per partition) = 10, min (per partition) = 0, idle max age = 60 min, idle test period = 240 min, strategy = DEFAULT
16:56:20.056 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.metastore.ObjectStore - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
16:56:20.056 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.metastore.ObjectStore - setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore juntzhang@172.16.43.180
org.apache.spark.sql.catalyst.analysis.ResolveCommandsWithIfExists => old【
'DropTable true, false
+- 'UnresolvedTableOrView [t1], DROP TABLE, true
】==> new【
NoopCommand DROP TABLE, [t1]
】


org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ =>【
ReturnAnswer
+- NoopCommand DROP TABLE, [t1]
】 ==> 【
PlanLater NoopCommand DROP TABLE, [t1]
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy =>【
NoopCommand DROP TABLE, [t1]
】 ==> 【
LocalTableScan <empty>
】

16:56:21.778 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: drop table if exists t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  droptableifexistst2 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  droptableifexistst2 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DropTableContext =>  drop table if exists t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  t2
org.apache.spark.sql.catalyst.analysis.ResolveCommandsWithIfExists => old【
'DropTable true, false
+- 'UnresolvedTableOrView [t2], DROP TABLE, true
】==> new【
NoopCommand DROP TABLE, [t2]
】


org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ =>【
ReturnAnswer
+- NoopCommand DROP TABLE, [t2]
】 ==> 【
PlanLater NoopCommand DROP TABLE, [t2]
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy =>【
NoopCommand DROP TABLE, [t2]
】 ==> 【
LocalTableScan <empty>
】

16:56:21.792 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: drop table if exists t3
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  droptableifexistst3 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  droptableifexistst3 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DropTableContext =>  drop table if exists t3
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  t3
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  t3
org.apache.spark.sql.catalyst.analysis.ResolveCommandsWithIfExists => old【
'DropTable true, false
+- 'UnresolvedTableOrView [t3], DROP TABLE, true
】==> new【
NoopCommand DROP TABLE, [t3]
】


org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ =>【
ReturnAnswer
+- NoopCommand DROP TABLE, [t3]
】 ==> 【
PlanLater NoopCommand DROP TABLE, [t3]
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy =>【
NoopCommand DROP TABLE, [t3]
】 ==> 【
LocalTableScan <empty>
】

org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences => old【
'Project ['id AS key#2, rand(-8318323901480374779) AS value1#3]
+- Range (0, 1000000, step=1, splits=Some(1))
】==> new【
Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
+- Range (0, 1000000, step=1, splits=Some(1))
】


org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences => old【
'RepartitionByExpression ['key]
+- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   +- Range (0, 1000000, step=1, splits=Some(1))
】==> new【
RepartitionByExpression [key#2L]
+- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   +- Range (0, 1000000, step=1, splits=Some(1))
】


16:56:22.058 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleTableIdentifierContext =>  t1 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableIdentifierContext =>  t1
org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ =>【
ReturnAnswer
+- CreateViewCommand `t1`, false, true, LocalTempView, true
      +- RepartitionByExpression [key#2L]
         +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
            +- Range (0, 1000000, step=1, splits=Some(1))
】 ==> 【
PlanLater CreateViewCommand `t1`, false, true, LocalTempView, true
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ =>【
CreateViewCommand `t1`, false, true, LocalTempView, true
   +- RepartitionByExpression [key#2L]
      +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
         +- Range (0, 1000000, step=1, splits=Some(1))
】 ==> 【
Execute CreateViewCommand
   +- CreateViewCommand `t1`, false, true, LocalTempView, true
         +- RepartitionByExpression [key#2L]
            +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
               +- Range (0, 1000000, step=1, splits=Some(1))
】

org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences => old【
'Project ['id AS key#8, rand(-5138763574403943414) AS value2#9]
+- Range (0, 1000000, step=1, splits=Some(1))
】==> new【
Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
+- Range (0, 1000000, step=1, splits=Some(1))
】


org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences => old【
'RepartitionByExpression ['key]
+- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
   +- Range (0, 1000000, step=1, splits=Some(1))
】==> new【
RepartitionByExpression [key#8L]
+- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
   +- Range (0, 1000000, step=1, splits=Some(1))
】


16:56:22.103 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleTableIdentifierContext =>  t2 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableIdentifierContext =>  t2
org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ =>【
ReturnAnswer
+- CreateViewCommand `t2`, false, true, LocalTempView, true
      +- RepartitionByExpression [key#8L]
         +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
            +- Range (0, 1000000, step=1, splits=Some(1))
】 ==> 【
PlanLater CreateViewCommand `t2`, false, true, LocalTempView, true
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ =>【
CreateViewCommand `t2`, false, true, LocalTempView, true
   +- RepartitionByExpression [key#8L]
      +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
         +- Range (0, 1000000, step=1, splits=Some(1))
】 ==> 【
Execute CreateViewCommand
   +- CreateViewCommand `t2`, false, true, LocalTempView, true
         +- RepartitionByExpression [key#8L]
            +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
               +- Range (0, 1000000, step=1, splits=Some(1))
】

16:56:22.110 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command:
select
 t1.key,
 t1.value1,
 t2.value2
from t1 join t2 on t1.key = t2.key
where t2.value2 < 0.001

org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  selectt1.key,t1.value1,t2.value2fromt1joint2ont1.key=t2.keywheret2.value2<0.001 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  selectt1.key,t1.value1,t2.value2fromt1joint2ont1.key=t2.keywheret2.value2<0.001 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryContext =>  selectt1.key,t1.value1,t2.value2fromt1joint2ont1.key=t2.keywheret2.value2<0.001
org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext =>  selectt1.key,t1.value1,t2.value2 fromt1joint2ont1.key=t2.key wheret2.value2<0.001
org.apache.spark.sql.catalyst.parser.SqlBaseParser$FromClauseContext =>  from t1joint2ont1.key=t2.key
org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableNameContext =>  t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$JoinRelationContext =>   join t2 ont1.key=t2.key
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  t1.key=t2.key
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComparisonContext =>  t1.key = t2.key
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  t1 . key
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  t2 . key
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableNameContext =>  t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext =>  selectt1.key,t1.value1,t2.value2 fromt1joint2ont1.key=t2.key wheret2.value2<0.001
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  t1.key
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  t1.key
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  t1 . key
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  t1.value1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  t1.value1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  t1 . value1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  t2.value2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  t2.value2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  t2 . value2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  t2.value2<0.001
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComparisonContext =>  t2.value2 < 0.001
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  t2 . value2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DecimalLiteralContext =>  0.001
org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryOrganizationContext =>
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations => old【
'Project ['t1.key, 't1.value1, 't2.value2]
+- 'Filter ('t2.value2 < 0.001)
   +- 'Join Inner, ('t1.key = 't2.key)
      :- 'UnresolvedRelation [t1], [], false
      +- 'UnresolvedRelation [t2], [], false
】==> new【
'Project ['t1.key, 't1.value1, 't2.value2]
+- 'Filter ('t2.value2 < 0.001)
   +- 'Join Inner, ('t1.key = 't2.key)
      :- SubqueryAlias t1
      :  +- View (`t1`, [key#2L,value1#3])
      :     +- RepartitionByExpression [key#2L]
      :        +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      :           +- Range (0, 1000000, step=1, splits=Some(1))
      +- SubqueryAlias t2
         +- View (`t2`, [key#8L,value2#9])
            +- RepartitionByExpression [key#8L]
               +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                  +- Range (0, 1000000, step=1, splits=Some(1))
】


org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences => old【
'Project ['t1.key, 't1.value1, 't2.value2]
+- 'Filter ('t2.value2 < 0.001)
   +- 'Join Inner, ('t1.key = 't2.key)
      :- SubqueryAlias t1
      :  +- View (`t1`, [key#2L,value1#3])
      :     +- RepartitionByExpression [key#2L]
      :        +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      :           +- Range (0, 1000000, step=1, splits=Some(1))
      +- SubqueryAlias t2
         +- View (`t2`, [key#8L,value2#9])
            +- RepartitionByExpression [key#8L]
               +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                  +- Range (0, 1000000, step=1, splits=Some(1))
】==> new【
'Project ['t1.key, 't1.value1, 't2.value2]
+- 'Filter (value2#9 < 0.001)
   +- Join Inner, (key#2L = key#8L)
      :- SubqueryAlias t1
      :  +- View (`t1`, [key#2L,value1#3])
      :     +- RepartitionByExpression [key#2L]
      :        +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      :           +- Range (0, 1000000, step=1, splits=Some(1))
      +- SubqueryAlias t2
         +- View (`t2`, [key#8L,value2#9])
            +- RepartitionByExpression [key#8L]
               +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                  +- Range (0, 1000000, step=1, splits=Some(1))
】


org.apache.spark.sql.catalyst.analysis.TypeCoercionBase$CombinedTypeCoercionRule => old【
'Project ['t1.key, 't1.value1, 't2.value2]
+- 'Filter (value2#9 < 0.001)
   +- Join Inner, (key#2L = key#8L)
      :- SubqueryAlias t1
      :  +- View (`t1`, [key#2L,value1#3])
      :     +- RepartitionByExpression [key#2L]
      :        +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      :           +- Range (0, 1000000, step=1, splits=Some(1))
      +- SubqueryAlias t2
         +- View (`t2`, [key#8L,value2#9])
            +- RepartitionByExpression [key#8L]
               +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                  +- Range (0, 1000000, step=1, splits=Some(1))
】==> new【
'Project ['t1.key, 't1.value1, 't2.value2]
+- Filter (value2#9 < cast(0.001 as double))
   +- Join Inner, (key#2L = key#8L)
      :- SubqueryAlias t1
      :  +- View (`t1`, [key#2L,value1#3])
      :     +- RepartitionByExpression [key#2L]
      :        +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      :           +- Range (0, 1000000, step=1, splits=Some(1))
      +- SubqueryAlias t2
         +- View (`t2`, [key#8L,value2#9])
            +- RepartitionByExpression [key#8L]
               +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                  +- Range (0, 1000000, step=1, splits=Some(1))
】


org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences => old【
'Project ['t1.key, 't1.value1, 't2.value2]
+- Filter (value2#9 < cast(0.001 as double))
   +- Join Inner, (key#2L = key#8L)
      :- SubqueryAlias t1
      :  +- View (`t1`, [key#2L,value1#3])
      :     +- RepartitionByExpression [key#2L]
      :        +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      :           +- Range (0, 1000000, step=1, splits=Some(1))
      +- SubqueryAlias t2
         +- View (`t2`, [key#8L,value2#9])
            +- RepartitionByExpression [key#8L]
               +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                  +- Range (0, 1000000, step=1, splits=Some(1))
】==> new【
Project [key#2L, value1#3, value2#9]
+- Filter (value2#9 < cast(0.001 as double))
   +- Join Inner, (key#2L = key#8L)
      :- SubqueryAlias t1
      :  +- View (`t1`, [key#2L,value1#3])
      :     +- RepartitionByExpression [key#2L]
      :        +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      :           +- Range (0, 1000000, step=1, splits=Some(1))
      +- SubqueryAlias t2
         +- View (`t2`, [key#8L,value2#9])
            +- RepartitionByExpression [key#8L]
               +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                  +- Range (0, 1000000, step=1, splits=Some(1))
】


org.apache.spark.sql.catalyst.analysis.ResolveTimeZone => old【
Project [key#2L, value1#3, value2#9]
+- Filter (value2#9 < cast(0.001 as double))
   +- Join Inner, (key#2L = key#8L)
      :- SubqueryAlias t1
      :  +- View (`t1`, [key#2L,value1#3])
      :     +- RepartitionByExpression [key#2L]
      :        +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      :           +- Range (0, 1000000, step=1, splits=Some(1))
      +- SubqueryAlias t2
         +- View (`t2`, [key#8L,value2#9])
            +- RepartitionByExpression [key#8L]
               +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                  +- Range (0, 1000000, step=1, splits=Some(1))
】==> new【
Project [key#2L, value1#3, value2#9]
+- Filter (value2#9 < cast(0.001 as double))
   +- Join Inner, (key#2L = key#8L)
      :- SubqueryAlias t1
      :  +- View (`t1`, [key#2L,value1#3])
      :     +- RepartitionByExpression [key#2L]
      :        +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      :           +- Range (0, 1000000, step=1, splits=Some(1))
      +- SubqueryAlias t2
         +- View (`t2`, [key#8L,value2#9])
            +- RepartitionByExpression [key#8L]
               +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                  +- Range (0, 1000000, step=1, splits=Some(1))
】


16:56:22.275 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: t3
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleMultipartIdentifierContext =>  t3 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  t3
org.apache.spark.sql.execution.datasources.DataSourceAnalysis => old【
'CreateTable `t3`, Overwrite
+- Project [key#2L, value1#3, value2#9]
   +- Filter (value2#9 < cast(0.001 as double))
      +- Join Inner, (key#2L = key#8L)
         :- SubqueryAlias t1
         :  +- View (`t1`, [key#2L,value1#3])
         :     +- RepartitionByExpression [key#2L]
         :        +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
         :           +- Range (0, 1000000, step=1, splits=Some(1))
         +- SubqueryAlias t2
            +- View (`t2`, [key#8L,value2#9])
               +- RepartitionByExpression [key#8L]
                  +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                     +- Range (0, 1000000, step=1, splits=Some(1))
】==> new【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#2L, value1#3, value2#9]
   +- Filter (value2#9 < cast(0.001 as double))
      +- Join Inner, (key#2L = key#8L)
         :- SubqueryAlias t1
         :  +- View (`t1`, [key#2L,value1#3])
         :     +- RepartitionByExpression [key#2L]
         :        +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
         :           +- Range (0, 1000000, step=1, splits=Some(1))
         +- SubqueryAlias t2
            +- View (`t2`, [key#8L,value2#9])
               +- RepartitionByExpression [key#8L]
                  +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                     +- Range (0, 1000000, step=1, splits=Some(1))
】


org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis => old【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#2L, value1#3, value2#9]
   +- Filter (value2#9 < cast(0.001 as double))
      +- Join Inner, (key#2L = key#8L)
         :- SubqueryAlias t1
         :  +- View (`t1`, [key#2L,value1#3])
         :     +- RepartitionByExpression [key#2L]
         :        +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
         :           +- Range (0, 1000000, step=1, splits=Some(1))
         +- SubqueryAlias t2
            +- View (`t2`, [key#8L,value2#9])
               +- RepartitionByExpression [key#8L]
                  +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                     +- Range (0, 1000000, step=1, splits=Some(1))
】==> new【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#2L, value1#3, value2#9]
   +- Filter (value2#9 < cast(0.001 as double))
      +- Join Inner, (key#2L = key#8L)
         :- RepartitionByExpression [key#2L]
         :  +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
         :     +- Range (0, 1000000, step=1, splits=Some(1))
         +- RepartitionByExpression [key#8L]
            +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
               +- Range (0, 1000000, step=1, splits=Some(1))
】


org.apache.spark.sql.catalyst.optimizer.PushDownPredicates => old【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#2L, value1#3, value2#9]
   +- Filter (value2#9 < cast(0.001 as double))
      +- Join Inner, (key#2L = key#8L)
         :- RepartitionByExpression [key#2L]
         :  +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
         :     +- Range (0, 1000000, step=1, splits=Some(1))
         +- RepartitionByExpression [key#8L]
            +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
               +- Range (0, 1000000, step=1, splits=Some(1))
】==> new【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#2L, value1#3, value2#9]
   +- Join Inner, (key#2L = key#8L)
      :- RepartitionByExpression [key#2L]
      :  +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      :     +- Range (0, 1000000, step=1, splits=Some(1))
      +- RepartitionByExpression [key#8L]
         +- Filter (value2#9 < cast(0.001 as double))
            +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
               +- Range (0, 1000000, step=1, splits=Some(1))
】


org.apache.spark.sql.catalyst.optimizer.ConstantFolding => old【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#2L, value1#3, value2#9]
   +- Join Inner, (key#2L = key#8L)
      :- RepartitionByExpression [key#2L]
      :  +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      :     +- Range (0, 1000000, step=1, splits=Some(1))
      +- RepartitionByExpression [key#8L]
         +- Filter (value2#9 < cast(0.001 as double))
            +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
               +- Range (0, 1000000, step=1, splits=Some(1))
】==> new【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#2L, value1#3, value2#9]
   +- Join Inner, (key#2L = key#8L)
      :- RepartitionByExpression [key#2L]
      :  +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      :     +- Range (0, 1000000, step=1, splits=Some(1))
      +- RepartitionByExpression [key#8L]
         +- Filter (value2#9 < 0.001)
            +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
               +- Range (0, 1000000, step=1, splits=Some(1))
】


16:56:22.389 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#2L = key#8L))
16:56:22.389 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#2L) | rightKeys:List(key#8L)
16:56:22.390 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#2L = key#8L))
16:56:22.390 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#2L) | rightKeys:List(key#8L)
org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ =>【
ReturnAnswer
+- CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
   +- Project [key#2L, value1#3, value2#9]
      +- Join Inner, (key#2L = key#8L)
         :- RepartitionByExpression [key#2L]
         :  +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
         :     +- Range (0, 1000000, step=1, splits=Some(1))
         +- RepartitionByExpression [key#8L]
            +- Filter (value2#9 < 0.001)
               +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                  +- Range (0, 1000000, step=1, splits=Some(1))
】 ==> 【
PlanLater CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ =>【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#2L, value1#3, value2#9]
   +- Join Inner, (key#2L = key#8L)
      :- RepartitionByExpression [key#2L]
      :  +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      :     +- Range (0, 1000000, step=1, splits=Some(1))
      +- RepartitionByExpression [key#8L]
         +- Filter (value2#9 < 0.001)
            +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
               +- Range (0, 1000000, step=1, splits=Some(1))
】 ==> 【
Execute CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- PlanLater Project [key#2L, value1#3, value2#9]
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ =>【
Project [key#2L, value1#3, value2#9]
+- Join Inner, (key#2L = key#8L)
   :- RepartitionByExpression [key#2L]
   :  +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   :     +- Range (0, 1000000, step=1, splits=Some(1))
   +- RepartitionByExpression [key#8L]
      +- Filter (value2#9 < 0.001)
         +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
            +- Range (0, 1000000, step=1, splits=Some(1))
】 ==> 【
Project [key#2L, value1#3, value2#9]
+- PlanLater Join Inner, (key#2L = key#8L)
】

16:56:22.407 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#2L = key#8L))
16:56:22.407 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#2L) | rightKeys:List(key#8L)
16:56:22.409 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#2L = key#8L))
16:56:22.409 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#2L) | rightKeys:List(key#8L)
org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ =>【
Join Inner, (key#2L = key#8L)
:- RepartitionByExpression [key#2L]
:  +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
:     +- Range (0, 1000000, step=1, splits=Some(1))
+- RepartitionByExpression [key#8L]
   +- Filter (value2#9 < 0.001)
      +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
         +- Range (0, 1000000, step=1, splits=Some(1))
】 ==> 【
SortMergeJoin [key#2L], [key#8L], Inner
:- PlanLater RepartitionByExpression [key#2L]
+- PlanLater RepartitionByExpression [key#8L]
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ =>【
RepartitionByExpression [key#2L]
+- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   +- Range (0, 1000000, step=1, splits=Some(1))
】 ==> 【
Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=24]
+- PlanLater Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ =>【
Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
+- Range (0, 1000000, step=1, splits=Some(1))
】 ==> 【
Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
+- PlanLater Range (0, 1000000, step=1, splits=Some(1))
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ =>【
Range (0, 1000000, step=1, splits=Some(1))
】 ==> 【
Range (0, 1000000, step=1, splits=1)
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ =>【
RepartitionByExpression [key#8L]
+- Filter (value2#9 < 0.001)
   +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
      +- Range (0, 1000000, step=1, splits=Some(1))
】 ==> 【
Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=29]
+- PlanLater Filter (value2#9 < 0.001)
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ =>【
Filter (value2#9 < 0.001)
+- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
   +- Range (0, 1000000, step=1, splits=Some(1))
】 ==> 【
Filter (value2#9 < 0.001)
+- PlanLater Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ =>【
Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
+- Range (0, 1000000, step=1, splits=Some(1))
】 ==> 【
Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
+- PlanLater Range (0, 1000000, step=1, splits=Some(1))
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ =>【
Range (0, 1000000, step=1, splits=Some(1))
】 ==> 【
Range (0, 1000000, step=1, splits=1)
】

16:56:22.447 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan - Adaptive execution enabled for plan:
Project [key#2L, value1#3, value2#9]
+- SortMergeJoin [key#2L], [key#8L], Inner
   :- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=46]
   :  +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   :     +- Range (0, 1000000, step=1, splits=1)
   +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=50]
      +- Filter (value2#9 < 0.001)
         +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
            +- Range (0, 1000000, step=1, splits=1)

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements =>【
Project [key#2L, value1#3, value2#9]
+- SortMergeJoin [key#2L], [key#8L], Inner
   :- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=46]
   :  +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   :     +- Range (0, 1000000, step=1, splits=1)
   +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=50]
      +- Filter (value2#9 < 0.001)
         +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
            +- Range (0, 1000000, step=1, splits=1)
】AQE ==> 【
Project [key#2L, value1#3, value2#9]
+- SortMergeJoin [key#2L], [key#8L], Inner
   :- Sort [key#2L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=46]
   :     +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   :        +- Range (0, 1000000, step=1, splits=1)
   +- Sort [key#8L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=50]
         +- Filter (value2#9 < 0.001)
            +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
               +- Range (0, 1000000, step=1, splits=1)
】

org.apache.spark.sql.execution.QueryExecution$ - org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan =>【
Execute CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#2L, value1#3, value2#9]
   +- SortMergeJoin [key#2L], [key#8L], Inner
      :- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=46]
      :  +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      :     +- Range (0, 1000000, step=1, splits=1)
      +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=50]
         +- Filter (value2#9 < 0.001)
            +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
               +- Range (0, 1000000, step=1, splits=1)
】 ==> 【
Execute CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- AdaptiveSparkPlan isFinalPlan=false
   +- Project [key#2L, value1#3, value2#9]
      +- SortMergeJoin [key#2L], [key#8L], Inner
         :- Sort [key#2L ASC NULLS FIRST], false, 0
         :  +- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=46]
         :     +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
         :        +- Range (0, 1000000, step=1, splits=1)
         +- Sort [key#8L ASC NULLS FIRST], false, 0
            +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=50]
               +- Filter (value2#9 < 0.001)
                  +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                     +- Range (0, 1000000, step=1, splits=1)
】

16:56:22.542 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.internal.io.FileCommitProtocol - Creating committer org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol; job 42917d05-8090-471f-a720-ca486bdc3c62; output=file:/Users/juntzhang/src/github/juntaozhang/spark/spark-v3.3.1/spark-warehouse/t3; dynamic=false
16:56:22.546 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.internal.io.FileCommitProtocol - Using (String, String, Boolean) constructor
16:56:22.574 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
16:56:22.611 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
16:56:22.611 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
16:56:22.866 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.doExecute ==> before get final plan:
Project [key#2L, value1#3, value2#9]
+- SortMergeJoin [key#2L], [key#8L], Inner
   :- Sort [key#2L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=46]
   :     +- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   :        +- Range (0, 1000000, step=1, splits=1)
   +- Sort [key#8L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=50]
         +- Filter (value2#9 < 0.001)
            +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
               +- Range (0, 1000000, step=1, splits=1)

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.CollapseCodegenStages =>【
Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=46]
+- Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   +- Range (0, 1000000, step=1, splits=1)
】AQE ==> 【
Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
+- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   +- *(1) Range (0, 1000000, step=1, splits=1)
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.CollapseCodegenStages =>【
Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=50]
+- Filter (value2#9 < 0.001)
   +- Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
      +- Range (0, 1000000, step=1, splits=1)
】AQE ==> 【
Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
+- *(2) Filter (value2#9 < 0.001)
   +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
      +- *(2) Range (0, 1000000, step=1, splits=1)
】

16:56:22.891 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec - Materialize query stage ShuffleQueryStageExec: 0, query stage plan:
Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
+- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   +- *(1) Range (0, 1000000, step=1, splits=1)

16:56:23.028 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator -
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean range_initRange_0;
/* 010 */   private long range_nextIndex_0;
/* 011 */   private TaskContext range_taskContext_0;
/* 012 */   private InputMetrics range_inputMetrics_0;
/* 013 */   private long range_batchEnd_0;
/* 014 */   private long range_numElementsTodo_0;
/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] range_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 016 */   private org.apache.spark.util.random.XORShiftRandom[] project_mutableStateArray_0 = new org.apache.spark.util.random.XORShiftRandom[1];
/* 017 */
/* 018 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 019 */     this.references = references;
/* 020 */   }
/* 021 */
/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 023 */     partitionIndex = index;
/* 024 */     this.inputs = inputs;
/* 025 */
/* 026 */     range_taskContext_0 = TaskContext.get();
/* 027 */     range_inputMetrics_0 = range_taskContext_0.taskMetrics().inputMetrics();
/* 028 */     range_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 029 */     range_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 030 */
/* 031 */     project_mutableStateArray_0[0] = new org.apache.spark.util.random.XORShiftRandom(-8318323901480374779L + partitionIndex);
/* 032 */   }
/* 033 */
/* 034 */   private void initRange(int idx) {
/* 035 */     java.math.BigInteger index = java.math.BigInteger.valueOf(idx);
/* 036 */     java.math.BigInteger numSlice = java.math.BigInteger.valueOf(1L);
/* 037 */     java.math.BigInteger numElement = java.math.BigInteger.valueOf(1000000L);
/* 038 */     java.math.BigInteger step = java.math.BigInteger.valueOf(1L);
/* 039 */     java.math.BigInteger start = java.math.BigInteger.valueOf(0L);
/* 040 */     long partitionEnd;
/* 041 */
/* 042 */     java.math.BigInteger st = index.multiply(numElement).divide(numSlice).multiply(step).add(start);
/* 043 */     if (st.compareTo(java.math.BigInteger.valueOf(Long.MAX_VALUE)) > 0) {
/* 044 */       range_nextIndex_0 = Long.MAX_VALUE;
/* 045 */     } else if (st.compareTo(java.math.BigInteger.valueOf(Long.MIN_VALUE)) < 0) {
/* 046 */       range_nextIndex_0 = Long.MIN_VALUE;
/* 047 */     } else {
/* 048 */       range_nextIndex_0 = st.longValue();
/* 049 */     }
/* 050 */     range_batchEnd_0 = range_nextIndex_0;
/* 051 */
/* 052 */     java.math.BigInteger end = index.add(java.math.BigInteger.ONE).multiply(numElement).divide(numSlice)
/* 053 */     .multiply(step).add(start);
/* 054 */     if (end.compareTo(java.math.BigInteger.valueOf(Long.MAX_VALUE)) > 0) {
/* 055 */       partitionEnd = Long.MAX_VALUE;
/* 056 */     } else if (end.compareTo(java.math.BigInteger.valueOf(Long.MIN_VALUE)) < 0) {
/* 057 */       partitionEnd = Long.MIN_VALUE;
/* 058 */     } else {
/* 059 */       partitionEnd = end.longValue();
/* 060 */     }
/* 061 */
/* 062 */     java.math.BigInteger startToEnd = java.math.BigInteger.valueOf(partitionEnd).subtract(
/* 063 */       java.math.BigInteger.valueOf(range_nextIndex_0));
/* 064 */     range_numElementsTodo_0  = startToEnd.divide(step).longValue();
/* 065 */     if (range_numElementsTodo_0 < 0) {
/* 066 */       range_numElementsTodo_0 = 0;
/* 067 */     } else if (startToEnd.remainder(step).compareTo(java.math.BigInteger.valueOf(0L)) != 0) {
/* 068 */       range_numElementsTodo_0++;
/* 069 */     }
/* 070 */   }
/* 071 */
/* 072 */   protected void processNext() throws java.io.IOException {
/* 073 */     // initialize Range
/* 074 */     if (!range_initRange_0) {
/* 075 */       range_initRange_0 = true;
/* 076 */       initRange(partitionIndex);
/* 077 */     }
/* 078 */
/* 079 */     while (true) {
/* 080 */       if (range_nextIndex_0 == range_batchEnd_0) {
/* 081 */         long range_nextBatchTodo_0;
/* 082 */         if (range_numElementsTodo_0 > 1000L) {
/* 083 */           range_nextBatchTodo_0 = 1000L;
/* 084 */           range_numElementsTodo_0 -= 1000L;
/* 085 */         } else {
/* 086 */           range_nextBatchTodo_0 = range_numElementsTodo_0;
/* 087 */           range_numElementsTodo_0 = 0;
/* 088 */           if (range_nextBatchTodo_0 == 0) break;
/* 089 */         }
/* 090 */         range_batchEnd_0 += range_nextBatchTodo_0 * 1L;
/* 091 */       }
/* 092 */
/* 093 */       int range_localEnd_0 = (int)((range_batchEnd_0 - range_nextIndex_0) / 1L);
/* 094 */       for (int range_localIdx_0 = 0; range_localIdx_0 < range_localEnd_0; range_localIdx_0++) {
/* 095 */         long range_value_0 = ((long)range_localIdx_0 * 1L) + range_nextIndex_0;
/* 096 */
/* 097 */         // common sub-expressions
/* 098 */
/* 099 */         final double project_value_1 = project_mutableStateArray_0[0].nextDouble();
/* 100 */
/* 101 */         range_mutableStateArray_0[1].reset();
/* 102 */
/* 103 */         range_mutableStateArray_0[1].write(0, range_value_0);
/* 104 */
/* 105 */         range_mutableStateArray_0[1].write(1, project_value_1);
/* 106 */         append((range_mutableStateArray_0[1].getRow()));
/* 107 */
/* 108 */         if (shouldStop()) {
/* 109 */           range_nextIndex_0 = range_value_0 + 1L;
/* 110 */           ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(range_localIdx_0 + 1);
/* 111 */           range_inputMetrics_0.incRecordsRead(range_localIdx_0 + 1);
/* 112 */           return;
/* 113 */         }
/* 114 */
/* 115 */       }
/* 116 */       range_nextIndex_0 = range_batchEnd_0;
/* 117 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(range_localEnd_0);
/* 118 */       range_inputMetrics_0.incRecordsRead(range_localEnd_0);
/* 119 */       range_taskContext_0.killTaskIfInterrupted();
/* 120 */     }
/* 121 */   }
/* 122 */
/* 123 */ }

16:56:23.206 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 200.32325 ms
16:56:23.230 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$inputRDDs$1$adapted
16:56:23.242 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$inputRDDs$1$adapted) is now cleaned +++
16:56:23.250 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
16:56:23.251 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
16:56:23.317 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 3 took 0.000493 seconds
16:56:23.320 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
16:56:23.321 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec - Materialize query stage ShuffleQueryStageExec: 1, query stage plan:
Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
+- *(2) Filter (value2#9 < 0.001)
   +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
      +- *(2) Range (0, 1000000, step=1, splits=1)

16:56:23.327 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (saveAsTable at PhysicalPlanSpec.scala:224) as input to shuffle 0
16:56:23.331 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 0 (saveAsTable at PhysicalPlanSpec.scala:224) with 1 output partitions
16:56:23.332 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 0 (saveAsTable at PhysicalPlanSpec.scala:224)
16:56:23.332 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
16:56:23.332 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator -
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean range_initRange_0;
/* 010 */   private long range_nextIndex_0;
/* 011 */   private TaskContext range_taskContext_0;
/* 012 */   private InputMetrics range_inputMetrics_0;
/* 013 */   private long range_batchEnd_0;
/* 014 */   private long range_numElementsTodo_0;
/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] range_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[3];
/* 016 */   private org.apache.spark.util.random.XORShiftRandom[] project_mutableStateArray_0 = new org.apache.spark.util.random.XORShiftRandom[1];
/* 017 */
/* 018 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 019 */     this.references = references;
/* 020 */   }
/* 021 */
/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 023 */     partitionIndex = index;
/* 024 */     this.inputs = inputs;
/* 025 */
/* 026 */     range_taskContext_0 = TaskContext.get();
/* 027 */     range_inputMetrics_0 = range_taskContext_0.taskMetrics().inputMetrics();
/* 028 */     range_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 029 */     range_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 030 */     range_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 031 */
/* 032 */     project_mutableStateArray_0[0] = new org.apache.spark.util.random.XORShiftRandom(-5138763574403943414L + partitionIndex);
/* 033 */   }
/* 034 */
/* 035 */   private void initRange(int idx) {
/* 036 */     java.math.BigInteger index = java.math.BigInteger.valueOf(idx);
/* 037 */     java.math.BigInteger numSlice = java.math.BigInteger.valueOf(1L);
/* 038 */     java.math.BigInteger numElement = java.math.BigInteger.valueOf(1000000L);
/* 039 */     java.math.BigInteger step = java.math.BigInteger.valueOf(1L);
/* 040 */     java.math.BigInteger start = java.math.BigInteger.valueOf(0L);
/* 041 */     long partitionEnd;
/* 042 */
/* 043 */     java.math.BigInteger st = index.multiply(numElement).divide(numSlice).multiply(step).add(start);
/* 044 */     if (st.compareTo(java.math.BigInteger.valueOf(Long.MAX_VALUE)) > 0) {
/* 045 */       range_nextIndex_0 = Long.MAX_VALUE;
/* 046 */     } else if (st.compareTo(java.math.BigInteger.valueOf(Long.MIN_VALUE)) < 0) {
/* 047 */       range_nextIndex_0 = Long.MIN_VALUE;
/* 048 */     } else {
/* 049 */       range_nextIndex_0 = st.longValue();
/* 050 */     }
/* 051 */     range_batchEnd_0 = range_nextIndex_0;
/* 052 */
/* 053 */     java.math.BigInteger end = index.add(java.math.BigInteger.ONE).multiply(numElement).divide(numSlice)
/* 054 */     .multiply(step).add(start);
/* 055 */     if (end.compareTo(java.math.BigInteger.valueOf(Long.MAX_VALUE)) > 0) {
/* 056 */       partitionEnd = Long.MAX_VALUE;
/* 057 */     } else if (end.compareTo(java.math.BigInteger.valueOf(Long.MIN_VALUE)) < 0) {
/* 058 */       partitionEnd = Long.MIN_VALUE;
/* 059 */     } else {
/* 060 */       partitionEnd = end.longValue();
/* 061 */     }
/* 062 */
/* 063 */     java.math.BigInteger startToEnd = java.math.BigInteger.valueOf(partitionEnd).subtract(
/* 064 */       java.math.BigInteger.valueOf(range_nextIndex_0));
/* 065 */     range_numElementsTodo_0  = startToEnd.divide(step).longValue();
/* 066 */     if (range_numElementsTodo_0 < 0) {
/* 067 */       range_numElementsTodo_0 = 0;
/* 068 */     } else if (startToEnd.remainder(step).compareTo(java.math.BigInteger.valueOf(0L)) != 0) {
/* 069 */       range_numElementsTodo_0++;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */   protected void processNext() throws java.io.IOException {
/* 074 */     // initialize Range
/* 075 */     if (!range_initRange_0) {
/* 076 */       range_initRange_0 = true;
/* 077 */       initRange(partitionIndex);
/* 078 */     }
/* 079 */
/* 080 */     while (true) {
/* 081 */       if (range_nextIndex_0 == range_batchEnd_0) {
/* 082 */         long range_nextBatchTodo_0;
/* 083 */         if (range_numElementsTodo_0 > 1000L) {
/* 084 */           range_nextBatchTodo_0 = 1000L;
/* 085 */           range_numElementsTodo_0 -= 1000L;
/* 086 */         } else {
/* 087 */           range_nextBatchTodo_0 = range_numElementsTodo_0;
/* 088 */           range_numElementsTodo_0 = 0;
/* 089 */           if (range_nextBatchTodo_0 == 0) break;
/* 090 */         }
/* 091 */         range_batchEnd_0 += range_nextBatchTodo_0 * 1L;
/* 092 */       }
/* 093 */
/* 094 */       int range_localEnd_0 = (int)((range_batchEnd_0 - range_nextIndex_0) / 1L);
/* 095 */       for (int range_localIdx_0 = 0; range_localIdx_0 < range_localEnd_0; range_localIdx_0++) {
/* 096 */         long range_value_0 = ((long)range_localIdx_0 * 1L) + range_nextIndex_0;
/* 097 */
/* 098 */         // common sub-expressions
/* 099 */
/* 100 */         final double project_value_1 = project_mutableStateArray_0[0].nextDouble();
/* 101 */
/* 102 */         do {
/* 103 */           boolean filter_value_0 = false;
/* 104 */           filter_value_0 = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(project_value_1, 0.001D) < 0;
/* 105 */           if (!filter_value_0) continue;
/* 106 */
/* 107 */           ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 108 */
/* 109 */           range_mutableStateArray_0[2].reset();
/* 110 */
/* 111 */           range_mutableStateArray_0[2].write(0, range_value_0);
/* 112 */
/* 113 */           range_mutableStateArray_0[2].write(1, project_value_1);
/* 114 */           append((range_mutableStateArray_0[2].getRow()));
/* 115 */
/* 116 */         } while(false);
/* 117 */
/* 118 */         if (shouldStop()) {
/* 119 */           range_nextIndex_0 = range_value_0 + 1L;
/* 120 */           ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(range_localIdx_0 + 1);
/* 121 */           range_inputMetrics_0.incRecordsRead(range_localIdx_0 + 1);
/* 122 */           return;
/* 123 */         }
/* 124 */
/* 125 */       }
/* 126 */       range_nextIndex_0 = range_batchEnd_0;
/* 127 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(range_localEnd_0);
/* 128 */       range_inputMetrics_0.incRecordsRead(range_localEnd_0);
/* 129 */       range_taskContext_0.killTaskIfInterrupted();
/* 130 */     }
/* 131 */   }
/* 132 */
/* 133 */ }

16:56:23.332 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:56:23.333 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ShuffleMapStage 0 (name=saveAsTable at PhysicalPlanSpec.scala:224;jobs=0))
16:56:23.334 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
16:56:23.334 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at saveAsTable at PhysicalPlanSpec.scala:224), which has no missing parents
16:56:23.334 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ShuffleMapStage 0)
16:56:23.355 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 25.96525 ms
16:56:23.356 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$inputRDDs$1$adapted
16:56:23.358 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$inputRDDs$1$adapted) is now cleaned +++
16:56:23.358 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
16:56:23.361 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
16:56:23.364 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 7 took 0.000055 seconds
16:56:23.431 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 13.9 KiB, free 8.4 GiB)
16:56:23.432 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_0 locally took 23 ms
16:56:23.432 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_0 without replication took 23 ms
16:56:23.459 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 8.4 GiB)
16:56:23.461 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 172.16.43.180, 55390, None)
16:56:23.462 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.16.43.180:55390 (size: 6.8 KiB, free: 8.4 GiB)
16:56:23.464 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:56:23.464 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
16:56:23.464 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_0_piece0 locally took 6 ms
16:56:23.464 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_0_piece0 without replication took 6 ms
16:56:23.464 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1513
16:56:23.475 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at saveAsTable at PhysicalPlanSpec.scala:224) (first 15 tasks are for partitions Vector(0))
16:56:23.476 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
16:56:23.497 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 0.0: 0
16:56:23.499 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 1 ms
16:56:23.500 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
16:56:23.502 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
16:56:23.502 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 7 (saveAsTable at PhysicalPlanSpec.scala:224) as input to shuffle 1
16:56:23.502 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 1 (saveAsTable at PhysicalPlanSpec.scala:224) with 1 output partitions
16:56:23.502 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 1 (saveAsTable at PhysicalPlanSpec.scala:224)
16:56:23.502 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
16:56:23.502 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:56:23.503 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ShuffleMapStage 1 (name=saveAsTable at PhysicalPlanSpec.scala:224;jobs=1))
16:56:23.503 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
16:56:23.503 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at saveAsTable at PhysicalPlanSpec.scala:224), which has no missing parents
16:56:23.503 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ShuffleMapStage 1)
16:56:23.507 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 14.4 KiB, free 8.4 GiB)
16:56:23.507 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_1 locally took 0 ms
16:56:23.507 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_1 without replication took 1 ms
16:56:23.508 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 8.4 GiB)
16:56:23.508 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 172.16.43.180, 55390, None)
16:56:23.508 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
16:56:23.508 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 172.16.43.180:55390 (size: 6.9 KiB, free: 8.4 GiB)
16:56:23.508 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
16:56:23.509 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:56:23.509 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
16:56:23.509 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_1_piece0 locally took 1 ms
16:56:23.509 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_1_piece0 without replication took 1 ms
16:56:23.509 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1513
16:56:23.509 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at saveAsTable at PhysicalPlanSpec.scala:224) (first 15 tasks are for partitions Vector(0))
16:56:23.509 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
16:56:23.523 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (172.16.43.180, executor driver, partition 0, PROCESS_LOCAL, 4567 bytes) taskResourceAssignments Map()
16:56:23.527 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 1.0: 0
16:56:23.527 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
16:56:23.527 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 1.0: NO_PREF, ANY
16:56:23.531 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 1
16:56:23.531 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
16:56:23.535 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
16:56:23.540 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 1
16:56:23.553 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_0
16:56:23.554 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
16:56:23.651 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for pmod(hash(input[0, bigint, false], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = false;
/* 032 */     int value_0 = -1;
/* 033 */     if (200 == 0) {
/* 034 */       isNull_0 = true;
/* 035 */     } else {
/* 036 */       int value_1 = 42;
/* 037 */       long value_2 = i.getLong(0);
/* 038 */       value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashLong(value_2, value_1);
/* 039 */
/* 040 */       int remainder_0 = value_1 % 200;
/* 041 */       if (remainder_0 < 0) {
/* 042 */         value_0=(remainder_0 + 200) % 200;
/* 043 */       } else {
/* 044 */         value_0=remainder_0;
/* 045 */       }
/* 046 */
/* 047 */     }
/* 048 */     if (isNull_0) {
/* 049 */       mutableStateArray_0[0].setNullAt(0);
/* 050 */     } else {
/* 051 */       mutableStateArray_0[0].write(0, value_0);
/* 052 */     }
/* 053 */     return (mutableStateArray_0[0].getRow());
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

16:56:23.652 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator -
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = false;
/* 032 */     int value_0 = -1;
/* 033 */     if (200 == 0) {
/* 034 */       isNull_0 = true;
/* 035 */     } else {
/* 036 */       int value_1 = 42;
/* 037 */       long value_2 = i.getLong(0);
/* 038 */       value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashLong(value_2, value_1);
/* 039 */
/* 040 */       int remainder_0 = value_1 % 200;
/* 041 */       if (remainder_0 < 0) {
/* 042 */         value_0=(remainder_0 + 200) % 200;
/* 043 */       } else {
/* 044 */         value_0=remainder_0;
/* 045 */       }
/* 046 */
/* 047 */     }
/* 048 */     if (isNull_0) {
/* 049 */       mutableStateArray_0[0].setNullAt(0);
/* 050 */     } else {
/* 051 */       mutableStateArray_0[0].write(0, value_0);
/* 052 */     }
/* 053 */     return (mutableStateArray_0[0].getRow());
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

16:56:23.664 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.812208 ms
16:56:23.960 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 0 with length 200
16:56:23.966 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 0: [66529,67811,69031,68526,69411,69475,68479,69493,68279,68392,68536,69072,68874,66989,69802,68516,69523,67983,68863,71312,68386,68840,69273,67800,69399,68753,68964,69297,67316,68505,67093,69004,68471,68815,67657,68877,68525,67553,71231,68126,68262,69042,68009,68247,68052,67220,69535,67592,69261,69985,68743,68840,68654,68970,69145,69618,69127,69441,67552,67182,68404,68237,67491,68554,69019,70460,69581,67303,66982,67976,67849,68779,68106,68641,68741,67733,68459,69363,68715,67974,69636,66458,68171,68228,68190,69433,69498,69032,68184,68090,68554,67652,68709,70494,68480,68408,68996,69111,69927,69108,69274,70796,68885,68442,69658,68295,68415,67610,69452,70410,67574,67699,67386,68863,68743,70848,67776,68592,69192,67896,67960,67867,69146,68670,70745,67836,68644,69503,68516,68027,68343,70098,68298,70041,67483,68412,69792,67905,68299,68506,70721,69797,68656,68460,69084,68319,67855,70268,68704,67813,70000,68164,67614,66682,68092,69707,70159,67937,69544,68226,68701,67352,68856,69144,69790,69341,67497,69076,68327,67044,70726,67862,68128,68555,67978,68972,70662,69865,67366,69553,68147,68532,66832,67939,67601,68529,68531,68641,69157,68112,70184,68200,68243,70717,68437,67235,69527,69078,69073,68873]
16:56:23.978 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1943 bytes result sent to driver
16:56:23.979 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 0
16:56:23.980 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
16:56:23.980 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
16:56:23.980 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
16:56:23.981 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (172.16.43.180, executor driver, partition 0, PROCESS_LOCAL, 4567 bytes) taskResourceAssignments Map()
16:56:23.982 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
16:56:23.982 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (1, 0) -> 1
16:56:23.983 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_1
16:56:23.983 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
16:56:23.983 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 472 ms on 172.16.43.180 (executor driver) (1/1)
16:56:23.984 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool
16:56:23.990 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for pmod(hash(input[0, bigint, false], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = false;
/* 032 */     int value_0 = -1;
/* 033 */     if (200 == 0) {
/* 034 */       isNull_0 = true;
/* 035 */     } else {
/* 036 */       int value_1 = 42;
/* 037 */       long value_2 = i.getLong(0);
/* 038 */       value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashLong(value_2, value_1);
/* 039 */
/* 040 */       int remainder_0 = value_1 % 200;
/* 041 */       if (remainder_0 < 0) {
/* 042 */         value_0=(remainder_0 + 200) % 200;
/* 043 */       } else {
/* 044 */         value_0=remainder_0;
/* 045 */       }
/* 046 */
/* 047 */     }
/* 048 */     if (isNull_0) {
/* 049 */       mutableStateArray_0[0].setNullAt(0);
/* 050 */     } else {
/* 051 */       mutableStateArray_0[0].write(0, value_0);
/* 052 */     }
/* 053 */     return (mutableStateArray_0[0].getRow());
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

16:56:23.991 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
16:56:23.992 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (saveAsTable at PhysicalPlanSpec.scala:224) finished in 0.650 s
16:56:23.993 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:56:23.993 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
16:56:23.993 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()
16:56:23.993 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:56:23.993 [dag-scheduler-event-loop] DEBUG org.apache.spark.MapOutputTrackerMaster - Increasing epoch to 1
16:56:23.998 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 0, remaining stages = 1
16:56:24.007 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#2L = key#8L))
16:56:24.007 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#2L) | rightKeys:List(key#8L)
org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ =>【
ReturnAnswer
+- Project [key#2L, value1#3, value2#9]
   +- Join Inner, (key#2L = key#8L)
      :- LogicalQueryStage RepartitionByExpression [key#2L], ShuffleQueryStage 0
      +- LogicalQueryStage RepartitionByExpression [key#8L], ShuffleQueryStage 1
】 ==> 【
PlanLater Project [key#2L, value1#3, value2#9]
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ =>【
Project [key#2L, value1#3, value2#9]
+- Join Inner, (key#2L = key#8L)
   :- LogicalQueryStage RepartitionByExpression [key#2L], ShuffleQueryStage 0
   +- LogicalQueryStage RepartitionByExpression [key#8L], ShuffleQueryStage 1
】 ==> 【
Project [key#2L, value1#3, value2#9]
+- PlanLater Join Inner, (key#2L = key#8L)
】

16:56:24.017 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#2L = key#8L))
16:56:24.017 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#2L) | rightKeys:List(key#8L)
16:56:24.017 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#2L = key#8L))
16:56:24.017 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#2L) | rightKeys:List(key#8L)
16:56:24.019 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats available as Statistics(sizeInBytes=22.9 MiB, rowCount=1.00E+6) for plan: ShuffleQueryStage 0
+- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
   +- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      +- *(1) Range (0, 1000000, step=1, splits=1)

16:56:24.019 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats not available for plan: ShuffleQueryStage 1
+- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
   +- *(2) Filter (value2#9 < 0.001)
      +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
         +- *(2) Range (0, 1000000, step=1, splits=1)

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ =>【
Join Inner, (key#2L = key#8L)
:- LogicalQueryStage RepartitionByExpression [key#2L], ShuffleQueryStage 0
+- LogicalQueryStage RepartitionByExpression [key#8L], ShuffleQueryStage 1
】 ==> 【
SortMergeJoin [key#2L], [key#8L], Inner
:- PlanLater LogicalQueryStage RepartitionByExpression [key#2L], ShuffleQueryStage 0
+- PlanLater LogicalQueryStage RepartitionByExpression [key#8L], ShuffleQueryStage 1
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ =>【
LogicalQueryStage RepartitionByExpression [key#2L], ShuffleQueryStage 0
】 ==> 【
ShuffleQueryStage 0
+- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
   +- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      +- *(1) Range (0, 1000000, step=1, splits=1)
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ =>【
LogicalQueryStage RepartitionByExpression [key#8L], ShuffleQueryStage 1
】 ==> 【
ShuffleQueryStage 1
+- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
   +- *(2) Filter (value2#9 < 0.001)
      +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
         +- *(2) Range (0, 1000000, step=1, splits=1)
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements =>【
Project [key#2L, value1#3, value2#9]
+- SortMergeJoin [key#2L], [key#8L], Inner
   :- ShuffleQueryStage 0
   :  +- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
   :     +- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   :        +- *(1) Range (0, 1000000, step=1, splits=1)
   +- ShuffleQueryStage 1
      +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
         +- *(2) Filter (value2#9 < 0.001)
            +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
               +- *(2) Range (0, 1000000, step=1, splits=1)
】AQE ==> 【
Project [key#2L, value1#3, value2#9]
+- SortMergeJoin [key#2L], [key#8L], Inner
   :- Sort [key#2L ASC NULLS FIRST], false, 0
   :  +- ShuffleQueryStage 0
   :     +- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
   :        +- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   :           +- *(1) Range (0, 1000000, step=1, splits=1)
   +- Sort [key#8L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
            +- *(2) Filter (value2#9 < 0.001)
               +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                  +- *(2) Range (0, 1000000, step=1, splits=1)
】

16:56:24.072 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 1 with length 200
16:56:24.073 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 1: [95,80,110,110,95,169,184,124,168,197,225,63,138,153,210,123,79,124,140,95,138,182,110,80,80,110,169,64,79,96,152,94,124,155,139,80,108,95,151,94,124,110,139,138,138,110,63,152,138,79,95,138,80,110,225,95,139,153,109,95,124,95,137,140,168,123,110,93,64,139,195,110,123,110,95,139,64,125,152,95,110,182,138,94,137,109,110,80,94,109,109,181,125,108,111,152,64,153,140,137,95,95,123,95,182,123,109,109,80,139,95,123,139,139,152,170,125,80,95,95,184,80,109,110,64,123,94,79,167,138,169,110,79,138,93,183,138,63,123,139,167,137,94,122,95,80,95,95,95,95,124,79,123,64,95,95,124,95,198,227,64,139,123,108,166,80,95,94,151,154,95,125,152,194,110,138,150,154,124,153,153,125,155,80,80,107,123,80,94,109,95,138,111,152,94,80,64,140,108,80]
16:56:24.074 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 1999 bytes result sent to driver
16:56:24.074 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (1, 0) -> 0
16:56:24.075 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
16:56:24.075 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
16:56:24.076 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 95 ms on 172.16.43.180 (executor driver) (1/1)
16:56:24.076 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool
16:56:24.076 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
16:56:24.076 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (saveAsTable at PhysicalPlanSpec.scala:224) finished in 0.572 s
16:56:24.076 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:56:24.076 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
16:56:24.076 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()
16:56:24.076 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:56:24.076 [dag-scheduler-event-loop] DEBUG org.apache.spark.MapOutputTrackerMaster - Increasing epoch to 2
16:56:24.077 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 1, remaining stages = 0
16:56:24.078 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#2L = key#8L))
16:56:24.078 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#2L) | rightKeys:List(key#8L)
org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ =>【
ReturnAnswer
+- Project [key#2L, value1#3, value2#9]
   +- Join Inner, (key#2L = key#8L)
      :- LogicalQueryStage RepartitionByExpression [key#2L], ShuffleQueryStage 0
      +- LogicalQueryStage RepartitionByExpression [key#8L], ShuffleQueryStage 1
】 ==> 【
PlanLater Project [key#2L, value1#3, value2#9]
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ =>【
Project [key#2L, value1#3, value2#9]
+- Join Inner, (key#2L = key#8L)
   :- LogicalQueryStage RepartitionByExpression [key#2L], ShuffleQueryStage 0
   +- LogicalQueryStage RepartitionByExpression [key#8L], ShuffleQueryStage 1
】 ==> 【
Project [key#2L, value1#3, value2#9]
+- PlanLater Join Inner, (key#2L = key#8L)
】

16:56:24.080 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#2L = key#8L))
16:56:24.080 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#2L) | rightKeys:List(key#8L)
16:56:24.080 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#2L = key#8L))
16:56:24.080 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#2L) | rightKeys:List(key#8L)
16:56:24.081 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats available as Statistics(sizeInBytes=22.9 MiB, rowCount=1.00E+6) for plan: ShuffleQueryStage 0
+- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
   +- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      +- *(1) Range (0, 1000000, step=1, splits=1)

16:56:24.081 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats available as Statistics(sizeInBytes=22.3 KiB, rowCount=952) for plan: ShuffleQueryStage 1
+- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
   +- *(2) Filter (value2#9 < 0.001)
      +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
         +- *(2) Range (0, 1000000, step=1, splits=1)


这里发生AQE计划转变
ShuffleQueryStage 1 发现满足
====================================================================================================

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ =>【
Join Inner, (key#2L = key#8L)
:- LogicalQueryStage RepartitionByExpression [key#2L], ShuffleQueryStage 0
+- LogicalQueryStage RepartitionByExpression [key#8L], ShuffleQueryStage 1
】 ==> 【
BroadcastHashJoin [key#2L], [key#8L], Inner, BuildRight, false
:- PlanLater LogicalQueryStage RepartitionByExpression [key#2L], ShuffleQueryStage 0
+- PlanLater LogicalQueryStage RepartitionByExpression [key#8L], ShuffleQueryStage 1
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ =>【
LogicalQueryStage RepartitionByExpression [key#2L], ShuffleQueryStage 0
】 ==> 【
ShuffleQueryStage 0
+- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
   +- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      +- *(1) Range (0, 1000000, step=1, splits=1)
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ =>【
LogicalQueryStage RepartitionByExpression [key#8L], ShuffleQueryStage 1
】 ==> 【
ShuffleQueryStage 1
+- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
   +- *(2) Filter (value2#9 < 0.001)
      +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
         +- *(2) Range (0, 1000000, step=1, splits=1)
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements =>【
Project [key#2L, value1#3, value2#9]
+- BroadcastHashJoin [key#2L], [key#8L], Inner, BuildRight, false
   :- ShuffleQueryStage 0
   :  +- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
   :     +- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   :        +- *(1) Range (0, 1000000, step=1, splits=1)
   +- ShuffleQueryStage 1
      +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
         +- *(2) Filter (value2#9 < 0.001)
            +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
               +- *(2) Range (0, 1000000, step=1, splits=1)
】AQE ==> 【
Project [key#2L, value1#3, value2#9]
+- BroadcastHashJoin [key#2L], [key#8L], Inner, BuildRight, false
   :- ShuffleQueryStage 0
   :  +- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
   :     +- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   :        +- *(1) Range (0, 1000000, step=1, splits=1)
   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=112]
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
            +- *(2) Filter (value2#9 < 0.001)
               +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                  +- *(2) Range (0, 1000000, step=1, splits=1)
】


======================================    Plan changed from  =======================================

16:56:24.094 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - Plan changed from Project [key#2L, value1#3, value2#9]
+- SortMergeJoin [key#2L], [key#8L], Inner
   :- Sort [key#2L ASC NULLS FIRST], false, 0
   :  +- ShuffleQueryStage 0
   :     +- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
   :        +- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   :           +- *(1) Range (0, 1000000, step=1, splits=1)
   +- Sort [key#8L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
            +- *(2) Filter (value2#9 < 0.001)
               +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                  +- *(2) Range (0, 1000000, step=1, splits=1)
 to Project [key#2L, value1#3, value2#9]
+- BroadcastHashJoin [key#2L], [key#8L], Inner, BuildRight, false
   :- ShuffleQueryStage 0
   :  +- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
   :     +- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   :        +- *(1) Range (0, 1000000, step=1, splits=1)
   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=112]
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
            +- *(2) Filter (value2#9 < 0.001)
               +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                  +- *(2) Range (0, 1000000, step=1, splits=1)

16:56:24.099 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
optimizeQueryStage - org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions => old【
ShuffleQueryStage 1
+- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
   +- *(2) Filter (value2#9 < 0.001)
      +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
         +- *(2) Range (0, 1000000, step=1, splits=1)
】AQE ==> new【
AQEShuffleRead coalesced
+- ShuffleQueryStage 1
   +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
      +- *(2) Filter (value2#9 < 0.001)
         +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
            +- *(2) Range (0, 1000000, step=1, splits=1)
】


16:56:24.111 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.BroadcastQueryStageExec - Materialize query stage BroadcastQueryStageExec: 2, query stage plan:
BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=117]
+- AQEShuffleRead coalesced
   +- ShuffleQueryStage 1
      +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
         +- *(2) Filter (value2#9 < 0.001)
            +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
               +- *(2) Range (0, 1000000, step=1, splits=1)

16:56:24.118 [broadcast-exchange-0] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$collect$2
16:56:24.121 [broadcast-exchange-0] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++
16:56:24.133 [broadcast-exchange-0] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
16:56:24.135 [broadcast-exchange-0] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
16:56:24.136 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
16:56:24.136 [broadcast-exchange-0] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 9 took 0.000079 seconds
16:56:24.137 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
16:56:24.137 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
16:56:24.138 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions
16:56:24.138 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
16:56:24.138 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
16:56:24.138 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:56:24.139 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 3 (name=$anonfun$withThreadLocalCaptured$1 at FutureTask.java:266;jobs=2))
16:56:24.139 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
16:56:24.139 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[9] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
16:56:24.139 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 3)
16:56:24.156 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 7.3 KiB, free 8.4 GiB)
16:56:24.156 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_2 locally took 0 ms
16:56:24.156 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_2 without replication took 0 ms
16:56:24.157 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 8.4 GiB)
16:56:24.157 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 172.16.43.180, 55390, None)
16:56:24.157 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 172.16.43.180:55390 (size: 3.9 KiB, free: 8.4 GiB)
16:56:24.157 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:56:24.157 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
16:56:24.157 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_2_piece0 locally took 0 ms
16:56:24.157 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_2_piece0 without replication took 0 ms
16:56:24.157 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1513
16:56:24.158 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))
16:56:24.158 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks resource profile 0
16:56:24.159 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 3.0: 2
16:56:24.160 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 1 ms
16:56:24.160 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 3.0: NODE_LOCAL, ANY
16:56:24.160 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_3.0, runningTasks: 0
16:56:24.162 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 2) (172.16.43.180, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
16:56:24.162 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 2)
16:56:24.162 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (3, 0) -> 1
16:56:24.163 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_2
16:56:24.163 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
16:56:24.207 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 1
16:56:24.208 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.MapOutputTrackerMaster - Convert map statuses for shuffle 1, mappers 0-1, partitions 0-200
16:56:24.226 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
16:56:24.232 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (24.2 KiB) non-empty blocks including 1 (24.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
16:56:24.233 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 10 ms
16:56:24.233 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_1_1_0_200,0)
16:56:24.234 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_1_1_0_200
16:56:24.238 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 15 ms
16:56:24.253 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 2). 14936 bytes result sent to driver
16:56:24.253 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (3, 0) -> 0
16:56:24.253 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_3.0, runningTasks: 0
16:56:24.253 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NODE_LOCAL, so moving to locality level ANY
16:56:24.254 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 2) in 93 ms on 172.16.43.180 (executor driver) (1/1)
16:56:24.254 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool
16:56:24.255 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0.100 s
16:56:24.255 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 2, remaining stages = 1
16:56:24.255 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 3, remaining stages = 0
16:56:24.255 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
16:56:24.256 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 3: Stage finished
16:56:24.256 [broadcast-exchange-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0.120197 s
16:56:24.269 [broadcast-exchange-0] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 0 acquired 1040.0 KiB for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@6c56c893
16:56:24.271 [broadcast-exchange-0] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, bigint, false]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */
/* 030 */
/* 031 */     long value_0 = i.getLong(0);
/* 032 */     mutableStateArray_0[0].write(0, value_0);
/* 033 */     return (mutableStateArray_0[0].getRow());
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */ }

16:56:24.271 [broadcast-exchange-0] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator -
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */
/* 030 */
/* 031 */     long value_0 = i.getLong(0);
/* 032 */     mutableStateArray_0[0].write(0, value_0);
/* 033 */     return (mutableStateArray_0[0].getRow());
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */ }

16:56:24.277 [broadcast-exchange-0] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.047333 ms
16:56:24.298 [broadcast-exchange-0] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 0 acquired 32.0 KiB for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@6c56c893
16:56:24.298 [broadcast-exchange-0] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 0 release 16.0 KiB from org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@6c56c893
16:56:24.302 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 1056.0 KiB, free 8.4 GiB)
16:56:24.302 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_3 locally took 0 ms
16:56:24.302 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_3 without replication took 0 ms
16:56:24.306 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.0 KiB, free 8.4 GiB)
16:56:24.307 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 172.16.43.180, 55390, None)
16:56:24.307 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 172.16.43.180:55390 (size: 23.0 KiB, free: 8.4 GiB)
16:56:24.307 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:56:24.307 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_3_piece0
16:56:24.307 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_3_piece0 locally took 0 ms
16:56:24.307 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_3_piece0 without replication took 0 ms
16:56:24.307 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Created broadcast 3 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
16:56:24.309 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#2L = key#8L))
16:56:24.309 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#2L) | rightKeys:List(key#8L)
org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ =>【
ReturnAnswer
+- Project [key#2L, value1#3, value2#9]
   +- Join Inner, (key#2L = key#8L)
      :- LogicalQueryStage RepartitionByExpression [key#2L], ShuffleQueryStage 0
      +- LogicalQueryStage LogicalQueryStage RepartitionByExpression [key#8L], ShuffleQueryStage 1, BroadcastQueryStage 2
】 ==> 【
PlanLater Project [key#2L, value1#3, value2#9]
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ =>【
Project [key#2L, value1#3, value2#9]
+- Join Inner, (key#2L = key#8L)
   :- LogicalQueryStage RepartitionByExpression [key#2L], ShuffleQueryStage 0
   +- LogicalQueryStage LogicalQueryStage RepartitionByExpression [key#8L], ShuffleQueryStage 1, BroadcastQueryStage 2
】 ==> 【
Project [key#2L, value1#3, value2#9]
+- PlanLater Join Inner, (key#2L = key#8L)
】

16:56:24.310 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#2L = key#8L))
16:56:24.310 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#2L) | rightKeys:List(key#8L)
org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ =>【
Join Inner, (key#2L = key#8L)
:- LogicalQueryStage RepartitionByExpression [key#2L], ShuffleQueryStage 0
+- LogicalQueryStage LogicalQueryStage RepartitionByExpression [key#8L], ShuffleQueryStage 1, BroadcastQueryStage 2
】 ==> 【
BroadcastHashJoin [key#2L], [key#8L], Inner, BuildRight, false
:- PlanLater LogicalQueryStage RepartitionByExpression [key#2L], ShuffleQueryStage 0
+- PlanLater LogicalQueryStage LogicalQueryStage RepartitionByExpression [key#8L], ShuffleQueryStage 1, BroadcastQueryStage 2
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ =>【
LogicalQueryStage RepartitionByExpression [key#2L], ShuffleQueryStage 0
】 ==> 【
ShuffleQueryStage 0
+- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
   +- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
      +- *(1) Range (0, 1000000, step=1, splits=1)
】

org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ =>【
LogicalQueryStage LogicalQueryStage RepartitionByExpression [key#8L], ShuffleQueryStage 1, BroadcastQueryStage 2
】 ==> 【
BroadcastQueryStage 2
+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=117]
   +- AQEShuffleRead coalesced
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
            +- *(2) Filter (value2#9 < 0.001)
               +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                  +- *(2) Range (0, 1000000, step=1, splits=1)
】

16:56:24.312 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(0), advisory target size: 67108864, actual target size 14118682, minimum partition size: 1048576
optimizeQueryStage - org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions => old【
Project [key#2L, value1#3, value2#9]
+- BroadcastHashJoin [key#2L], [key#8L], Inner, BuildRight, false
   :- ShuffleQueryStage 0
   :  +- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
   :     +- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   :        +- *(1) Range (0, 1000000, step=1, splits=1)
   +- BroadcastQueryStage 2
      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=117]
         +- AQEShuffleRead coalesced
            +- ShuffleQueryStage 1
               +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
                  +- *(2) Filter (value2#9 < 0.001)
                     +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                        +- *(2) Range (0, 1000000, step=1, splits=1)
】AQE ==> new【
Project [key#2L, value1#3, value2#9]
+- BroadcastHashJoin [key#2L], [key#8L], Inner, BuildRight, false
   :- AQEShuffleRead coalesced
   :  +- ShuffleQueryStage 0
   :     +- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
   :        +- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   :           +- *(1) Range (0, 1000000, step=1, splits=1)
   +- BroadcastQueryStage 2
      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=117]
         +- AQEShuffleRead coalesced
            +- ShuffleQueryStage 1
               +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
                  +- *(2) Filter (value2#9 < 0.001)
                     +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                        +- *(2) Range (0, 1000000, step=1, splits=1)
】


org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.CollapseCodegenStages =>【
Project [key#2L, value1#3, value2#9]
+- BroadcastHashJoin [key#2L], [key#8L], Inner, BuildRight, false
   :- AQEShuffleRead coalesced
   :  +- ShuffleQueryStage 0
   :     +- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
   :        +- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   :           +- *(1) Range (0, 1000000, step=1, splits=1)
   +- BroadcastQueryStage 2
      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=117]
         +- AQEShuffleRead coalesced
            +- ShuffleQueryStage 1
               +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
                  +- *(2) Filter (value2#9 < 0.001)
                     +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                        +- *(2) Range (0, 1000000, step=1, splits=1)
】AQE ==> 【
*(3) Project [key#2L, value1#3, value2#9]
+- *(3) BroadcastHashJoin [key#2L], [key#8L], Inner, BuildRight, false
   :- AQEShuffleRead coalesced
   :  +- ShuffleQueryStage 0
   :     +- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
   :        +- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   :           +- *(1) Range (0, 1000000, step=1, splits=1)
   +- BroadcastQueryStage 2
      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=117]
         +- AQEShuffleRead coalesced
            +- ShuffleQueryStage 1
               +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
                  +- *(2) Filter (value2#9 < 0.001)
                     +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                        +- *(2) Range (0, 1000000, step=1, splits=1)
】

16:56:24.318 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.doExecute ==> after get final plan:
*(3) Project [key#2L, value1#3, value2#9]
+- *(3) BroadcastHashJoin [key#2L], [key#8L], Inner, BuildRight, false
   :- AQEShuffleRead coalesced
   :  +- ShuffleQueryStage 0
   :     +- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
   :        +- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   :           +- *(1) Range (0, 1000000, step=1, splits=1)
   +- BroadcastQueryStage 2
      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=117]
         +- AQEShuffleRead coalesced
            +- ShuffleQueryStage 1
               +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
                  +- *(2) Filter (value2#9 < 0.001)
                     +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                        +- *(2) Range (0, 1000000, step=1, splits=1)

16:56:24.320 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_3
16:56:24.321 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
16:56:24.326 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator -
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage3(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=3
/* 006 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] bhj_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 012 */
/* 013 */   public GeneratedIteratorForCodegenStage3(Object[] references) {
/* 014 */     this.references = references;
/* 015 */   }
/* 016 */
/* 017 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 018 */     partitionIndex = index;
/* 019 */     this.inputs = inputs;
/* 020 */     inputadapter_input_0 = inputs[0];
/* 021 */
/* 022 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[0] /* broadcast */).value()).asReadOnlyCopy();
/* 023 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());
/* 024 */
/* 025 */     bhj_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);
/* 026 */     bhj_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);
/* 027 */
/* 028 */   }
/* 029 */
/* 030 */   protected void processNext() throws java.io.IOException {
/* 031 */     while ( inputadapter_input_0.hasNext()) {
/* 032 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 033 */
/* 034 */       long inputadapter_value_0 = inputadapter_row_0.getLong(0);
/* 035 */
/* 036 */       // generate join key for stream side
/* 037 */
/* 038 */       // find matches from HashedRelation
/* 039 */       UnsafeRow bhj_buildRow_0 = false ? null: (UnsafeRow)bhj_relation_0.getValue(inputadapter_value_0);
/* 040 */       if (bhj_buildRow_0 != null) {
/* 041 */         {
/* 042 */           ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 043 */
/* 044 */           // common sub-expressions
/* 045 */
/* 046 */           double inputadapter_value_1 = inputadapter_row_0.getDouble(1);
/* 047 */           double bhj_value_2 = bhj_buildRow_0.getDouble(1);
/* 048 */           bhj_mutableStateArray_0[1].reset();
/* 049 */
/* 050 */           bhj_mutableStateArray_0[1].write(0, inputadapter_value_0);
/* 051 */
/* 052 */           bhj_mutableStateArray_0[1].write(1, inputadapter_value_1);
/* 053 */
/* 054 */           bhj_mutableStateArray_0[1].write(2, bhj_value_2);
/* 055 */           append((bhj_mutableStateArray_0[1].getRow()));
/* 056 */
/* 057 */         }
/* 058 */       }
/* 059 */       if (shouldStop()) return;
/* 060 */     }
/* 061 */   }
/* 062 */
/* 063 */ }

16:56:24.336 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.641709 ms
16:56:24.367 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
16:56:24.368 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
16:56:24.369 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.doExecute was finished.
16:56:24.370 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - Final plan: *(3) Project [key#2L, value1#3, value2#9]
+- *(3) BroadcastHashJoin [key#2L], [key#8L], Inner, BuildRight, false
   :- AQEShuffleRead coalesced
   :  +- ShuffleQueryStage 0
   :     +- Exchange hashpartitioning(key#2L, 200), REPARTITION_BY_COL, [plan_id=67]
   :        +- *(1) Project [id#0L AS key#2L, rand(-8318323901480374779) AS value1#3]
   :           +- *(1) Range (0, 1000000, step=1, splits=1)
   +- BroadcastQueryStage 2
      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=117]
         +- AQEShuffleRead coalesced
            +- ShuffleQueryStage 1
               +- Exchange hashpartitioning(key#8L, 200), REPARTITION_BY_COL, [plan_id=81]
                  +- *(2) Filter (value2#9 < 0.001)
                     +- *(2) Project [id#6L AS key#8L, rand(-5138763574403943414) AS value2#9]
                        +- *(2) Range (0, 1000000, step=1, splits=1)

16:56:24.371 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.doExecute.finalPlanUpdate was finished.
16:56:24.373 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$write$21
16:56:24.374 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$write$21) is now cleaned +++
16:56:24.397 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.SparkContext - Starting job: saveAsTable at PhysicalPlanSpec.scala:224
16:56:24.397 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 11 took 0.000091 seconds
16:56:24.397 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
16:56:24.397 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
16:56:24.397 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 3 (saveAsTable at PhysicalPlanSpec.scala:224) with 1 output partitions
16:56:24.397 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (saveAsTable at PhysicalPlanSpec.scala:224)
16:56:24.397 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
16:56:24.398 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:56:24.398 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 5 (name=saveAsTable at PhysicalPlanSpec.scala:224;jobs=3))
16:56:24.398 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
16:56:24.398 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[11] at saveAsTable at PhysicalPlanSpec.scala:224), which has no missing parents
16:56:24.398 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 5)
16:56:24.414 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 211.4 KiB, free 8.4 GiB)
16:56:24.414 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_4 locally took 0 ms
16:56:24.414 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_4 without replication took 0 ms
16:56:24.415 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 75.7 KiB, free 8.4 GiB)
16:56:24.415 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, 172.16.43.180, 55390, None)
16:56:24.416 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 172.16.43.180:55390 (size: 75.7 KiB, free: 8.4 GiB)
16:56:24.416 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
16:56:24.416 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_4_piece0
16:56:24.416 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_4_piece0 locally took 0 ms
16:56:24.416 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_4_piece0 without replication took 0 ms
16:56:24.416 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1513
16:56:24.416 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at saveAsTable at PhysicalPlanSpec.scala:224) (first 15 tasks are for partitions Vector(0))
16:56:24.416 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks resource profile 0
16:56:24.417 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 5.0: 2
16:56:24.417 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
16:56:24.417 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 5.0: NODE_LOCAL, ANY
16:56:24.417 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_5.0, runningTasks: 0
16:56:24.417 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 3) (172.16.43.180, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
16:56:24.417 [Executor task launch worker for task 0.0 in stage 5.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 3)
16:56:24.418 [Executor task launch worker for task 0.0 in stage 5.0 (TID 3)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (5, 0) -> 1
16:56:24.418 [Executor task launch worker for task 0.0 in stage 5.0 (TID 3)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_4
16:56:24.418 [Executor task launch worker for task 0.0 in stage 5.0 (TID 3)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
16:56:24.440 [Executor task launch worker for task 0.0 in stage 5.0 (TID 3)] DEBUG org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 0
16:56:24.440 [Executor task launch worker for task 0.0 in stage 5.0 (TID 3)] DEBUG org.apache.spark.MapOutputTrackerMaster - Convert map statuses for shuffle 0, mappers 0-1, partitions 0-200
16:56:24.441 [Executor task launch worker for task 0.0 in stage 5.0 (TID 3)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
16:56:24.441 [Executor task launch worker for task 0.0 in stage 5.0 (TID 3)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (13.5 MiB) non-empty blocks including 1 (13.5 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
16:56:24.441 [Executor task launch worker for task 0.0 in stage 5.0 (TID 3)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:56:24.441 [Executor task launch worker for task 0.0 in stage 5.0 (TID 3)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_0_0_0_200,0)
16:56:24.441 [Executor task launch worker for task 0.0 in stage 5.0 (TID 3)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_0_0_0_200
16:56:24.442 [Executor task launch worker for task 0.0 in stage 5.0 (TID 3)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 1 ms
16:56:24.445 [Executor task launch worker for task 0.0 in stage 5.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
16:56:24.445 [Executor task launch worker for task 0.0 in stage 5.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
16:56:24.505 [Executor task launch worker for task 0.0 in stage 5.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "key",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "value1",
    "type" : "double",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "value2",
    "type" : "double",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int64 key;
  required double value1;
  required double value2;
}


16:56:25.019 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 1
16:56:25.030 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 1
16:56:25.030 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 1
16:56:25.032 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_1
16:56:25.032 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 of size 14736 dropped from memory (free 8972694089)
16:56:25.033 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_1_piece0
16:56:25.034 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 of size 7092 dropped from memory (free 8972701181)
16:56:25.034 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 172.16.43.180, 55390, None)
16:56:25.034 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 172.16.43.180:55390 in memory (size: 6.9 KiB, free: 8.4 GiB)
16:56:25.034 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:56:25.034 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
16:56:25.035 [block-manager-storage-async-thread-pool-2] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 1, response is 0
16:56:25.035 [block-manager-storage-async-thread-pool-2] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 172.16.43.180:55389
16:56:25.036 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 2
16:56:25.036 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 2
16:56:25.036 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 2
16:56:25.036 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_2_piece0
16:56:25.036 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 of size 3994 dropped from memory (free 8972705175)
16:56:25.036 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 172.16.43.180, 55390, None)
16:56:25.036 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 172.16.43.180:55390 in memory (size: 3.9 KiB, free: 8.4 GiB)
16:56:25.036 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:56:25.036 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
16:56:25.037 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_2
16:56:25.037 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 of size 7464 dropped from memory (free 8972712639)
16:56:25.037 [block-manager-storage-async-thread-pool-5] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 2, response is 0
16:56:25.037 [block-manager-storage-async-thread-pool-5] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 172.16.43.180:55389
16:56:25.038 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 0
16:56:25.038 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 0
16:56:25.038 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 0
16:56:25.038 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_0_piece0
16:56:25.038 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 of size 6954 dropped from memory (free 8972719593)
16:56:25.038 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 172.16.43.180, 55390, None)
16:56:25.038 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 172.16.43.180:55390 in memory (size: 6.8 KiB, free: 8.4 GiB)
16:56:25.039 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:56:25.039 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
16:56:25.039 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_0
16:56:25.039 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 of size 14280 dropped from memory (free 8972733873)
16:56:25.039 [block-manager-storage-async-thread-pool-8] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 0, response is 0
16:56:25.039 [block-manager-storage-async-thread-pool-8] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 172.16.43.180:55389
16:56:25.228 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.OutputCommitCoordinator - Commit allowed for stage=5.0, partition=0, task attempt 0
16:56:25.229 [Executor task launch worker for task 0.0 in stage 5.0 (TID 3)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202507291656246277066599318874386_0005_m_000000_3: Committed. Elapsed time: 0 ms.
16:56:25.234 [Executor task launch worker for task 0.0 in stage 5.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 3). 3726 bytes result sent to driver
16:56:25.234 [Executor task launch worker for task 0.0 in stage 5.0 (TID 3)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (5, 0) -> 0
16:56:25.234 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_5.0, runningTasks: 0
16:56:25.234 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NODE_LOCAL, so moving to locality level ANY
16:56:25.235 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 3) in 818 ms on 172.16.43.180 (executor driver) (1/1)
16:56:25.235 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool
16:56:25.235 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (saveAsTable at PhysicalPlanSpec.scala:224) finished in 0.832 s
16:56:25.235 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 5, remaining stages = 1
16:56:25.236 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 4, remaining stages = 0
16:56:25.236 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
16:56:25.236 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 5: Stage finished
16:56:25.236 [dag-scheduler-event-loop] DEBUG org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@14df329f)
16:56:25.236 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 finished: saveAsTable at PhysicalPlanSpec.scala:224, took 0.839442 s
16:56:25.237 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job 82018ee9-b8c7-4c90-b5c1-1883e3fc852c.
16:56:25.288 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Committing files staged for absolute locations Map()
16:56:25.288 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Create absolute parent directories: Set()
16:56:25.289 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job 82018ee9-b8c7-4c90-b5c1-1883e3fc852c committed. Elapsed time: 51 ms.
16:56:25.290 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job 82018ee9-b8c7-4c90-b5c1-1883e3fc852c.
16:56:25.405 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 4
16:56:25.406 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 4
16:56:25.406 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 4
16:56:25.406 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_4_piece0
16:56:25.406 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 of size 77540 dropped from memory (free 8972811413)
16:56:25.407 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, 172.16.43.180, 55390, None)
16:56:25.407 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 172.16.43.180:55390 in memory (size: 75.7 KiB, free: 8.4 GiB)
16:56:25.407 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
16:56:25.407 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_4_piece0
16:56:25.407 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_4
16:56:25.407 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 of size 216480 dropped from memory (free 8973027893)
16:56:25.408 [block-manager-storage-async-thread-pool-11] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 4, response is 0
16:56:25.408 [block-manager-storage-async-thread-pool-11] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 172.16.43.180:55389
16:56:25.428 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.datasources.DataSource - Some paths were ignored:

16:56:25.450 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 8 ms to list leaf files for 1 paths.
16:56:25.492 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  array<string> <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComplexDataTypeContext =>  array < string >
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
16:56:25.499 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.hive.HiveExternalCatalog - Persisting file based data source table `default`.`t3` into Hive metastore in Hive compatible format.
16:56:25.502 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  bigint <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  bigint
16:56:25.503 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: double
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  double <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  double
16:56:25.503 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: double
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  double <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  double
16:56:25.602 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.ql.session.SessionState - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
16:56:25.602 [ScalaTest-run-running-PhysicalPlanSpec] INFO  hive.metastore - Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
16:56:25.602 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG hive.ql.metadata.Hive - Creating new db. db = org.apache.hadoop.hive.ql.metadata.Hive@12aa2cca, needsRefresh = false, db.isCurrentUserOwner = true
16:56:25.602 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG hive.ql.metadata.Hive - Closing current thread's connection to Hive Metastore.
16:56:25.603 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG hive.ql.metadata.Hive - Closing current thread's connection to Hive Metastore.
16:56:25.619 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG hive.log - DDL: struct t3 { i64 key, double value1, double value2}
16:56:25.675 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist
16:56:25.676 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.stats.jdbc.timeout does not exist
16:56:25.676 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.stats.retries.wait does not exist
16:56:25.683 [ScalaTest-run-running-PhysicalPlanSpec] INFO  hive.log - Updating table stats fast for t3
16:56:25.683 [ScalaTest-run-running-PhysicalPlanSpec] INFO  hive.log - Updated size of table t3 to 20958
16:56:35.353 [executor-heartbeater] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - removing (1, 0) from stageTCMP
16:56:35.353 [executor-heartbeater] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - removing (5, 0) from stageTCMP
16:56:35.353 [executor-heartbeater] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - removing (0, 0) from stageTCMP
16:56:35.353 [executor-heartbeater] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - removing (3, 0) from stageTCMP
16:57:22.169 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:22.183 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:22.351 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:22.358 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:22.364 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:22.367 [SparkUI-108] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:22.368 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:22.377 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:22.384 [SparkUI-108] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:22.385 [SparkUI-38] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:22.387 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:22.388 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:22.389 [SparkUI-34] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:22.393 [SparkUI-108] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:22.498 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.670 [SparkUI-108] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.694 [SparkUI-38] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.695 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.698 [SparkUI-38] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.699 [SparkUI-111] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.699 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.701 [SparkUI-112] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.702 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.702 [SparkUI-108] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.704 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.704 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.705 [SparkUI-112] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.706 [SparkUI-35] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.709 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.710 [SparkUI-113] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.712 [SparkUI-113] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.714 [SparkUI-34] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.715 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.715 [SparkUI-108] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.715 [SparkUI-38] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.717 [SparkUI-31] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.717 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.718 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.719 [SparkUI-110] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.721 [SparkUI-34] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.721 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:24.725 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:25.616 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:25.721 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:25.744 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:25.840 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:25.857 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:25.863 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:25.868 [SparkUI-111] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:25.871 [SparkUI-111] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:25.872 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:26.516 [SparkUI-110] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:26.538 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:26.539 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:26.539 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:26.539 [SparkUI-113] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:26.539 [SparkUI-112] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:26.540 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:26.549 [SparkUI-111] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:26.549 [SparkUI-35] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:26.549 [SparkUI-34] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:26.551 [SparkUI-111] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:26.552 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:26.555 [SparkUI-108] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:26.555 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:26.555 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.370 [SparkUI-113] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.395 [SparkUI-113] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.395 [SparkUI-35] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.396 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.396 [SparkUI-110] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.396 [SparkUI-112] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.396 [SparkUI-111] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.400 [SparkUI-111] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.401 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.402 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.402 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.402 [SparkUI-108] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.403 [SparkUI-35] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.406 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.406 [SparkUI-35] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.406 [SparkUI-112] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.408 [SparkUI-34] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.409 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.418 [SparkUI-35] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:57:28.418 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:21.488 [SparkUI-35] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:21.601 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:21.601 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:21.604 [SparkUI-111] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:21.606 [SparkUI-35] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:21.608 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:21.612 [SparkUI-111] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:21.614 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:21.616 [SparkUI-111] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:21.618 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:21.618 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:21.624 [SparkUI-35] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:28.376 [SparkUI-35] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:28.428 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:28.431 [SparkUI-111] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:28.431 [SparkUI-34] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:28.431 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:28.431 [SparkUI-108] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:28.432 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:28.436 [SparkUI-110] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:28.436 [SparkUI-112] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:28.436 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:28.436 [SparkUI-113] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:28.437 [SparkUI-31] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:28.439 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:28.439 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:28.441 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.576 [SparkUI-108] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.612 [SparkUI-108] DEBUG org.apache.spark.ui.scope.RDDOperationGraph - digraph G {
  subgraph clusterstage_0 {
    label="Stage 0";
    subgraph cluster2 {
      label="WholeStageCodegen (1)";
      0 [labelType="html" label="ParallelCollectionRDD [0]<br>saveAsTable at PhysicalPlanSpec.scala:224"];
      1 [labelType="html" label="MapPartitionsRDD [1]<br>saveAsTable at PhysicalPlanSpec.scala:224"];
      2 [labelType="html" label="MapPartitionsRDD [2]<br>saveAsTable at PhysicalPlanSpec.scala:224"];
    }
    subgraph cluster1 {
      label="Exchange";
      3 [labelType="html" label="MapPartitionsRDD [3]<br>saveAsTable at PhysicalPlanSpec.scala:224"];
    }
  }
  0->1;
  1->2;
  2->3;
}
16:58:30.643 [SparkUI-112] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.646 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.646 [SparkUI-113] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.646 [SparkUI-111] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.648 [SparkUI-34] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.650 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.650 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.650 [SparkUI-113] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.651 [SparkUI-112] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.651 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.652 [SparkUI-34] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.653 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.654 [SparkUI-110] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.654 [SparkUI-111] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.656 [SparkUI-31] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.658 [SparkUI-34] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.658 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.659 [SparkUI-108] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:30.660 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:33.454 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:33.475 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:33.477 [SparkUI-113] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:33.483 [SparkUI-110] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:33.486 [SparkUI-111] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:33.488 [SparkUI-112] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:33.489 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:33.489 [SparkUI-113] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:33.489 [SparkUI-111] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:33.489 [SparkUI-110] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:33.489 [SparkUI-108] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:33.491 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:33.493 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:33.495 [SparkUI-31] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:33.495 [SparkUI-108] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.146 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.166 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.166 [SparkUI-113] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.169 [SparkUI-112] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.169 [SparkUI-108] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.169 [SparkUI-110] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.169 [SparkUI-34] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.169 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.171 [SparkUI-34] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.172 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.173 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.173 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.175 [SparkUI-112] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.176 [SparkUI-31] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.176 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.176 [SparkUI-113] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.179 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.179 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.187 [SparkUI-113] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:36.189 [SparkUI-31] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:37.616 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:37.654 [SparkUI-113] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:37.655 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:37.660 [SparkUI-111] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:37.663 [SparkUI-34] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:37.665 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:37.665 [SparkUI-108] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:37.665 [SparkUI-111] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:37.665 [SparkUI-31] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:37.667 [SparkUI-110] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:37.668 [SparkUI-113] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:37.669 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:37.671 [SparkUI-31] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:37.671 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:37.671 [SparkUI-113] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.300 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.316 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.323 [SparkUI-108] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.326 [SparkUI-108] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.329 [SparkUI-108] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.329 [SparkUI-113] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.332 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.332 [SparkUI-110] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.333 [SparkUI-112] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.333 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.335 [SparkUI-111] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.336 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.337 [SparkUI-32] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.337 [SparkUI-31] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.338 [SparkUI-113] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.338 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.341 [SparkUI-33] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.341 [SparkUI-37] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.349 [SparkUI-109] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
16:58:39.350 [SparkUI-112] DEBUG org.apache.spark.SecurityManager - user=null aclsEnabled=false viewAcls=juntzhang viewAclsGroups=
Disconnected from the target VM, address: '127.0.0.1:55376', transport: 'socket'

Process finished with exit code 255
