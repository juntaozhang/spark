/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/bin/java -agentlib:jdwp=transport=dt_socket,address=127.0.0.1:53460,suspend=y,server=n -javaagent:/Users/juntzhang/Library/Caches/JetBrains/IdeaIC2024.1/captureAgent/debugger-agent.jar -Dfile.encoding=UTF-8 -classpath "/Users/juntzhang/Library/Application Support/JetBrains/IdeaIC2024.1/plugins/Scala/lib/runners.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/cat.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/crs-agent.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/js-engine.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/legacy8ujsse.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/openeddsa.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/openjsse.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/rhino-1.7R4.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/xml-factory-wrapper-1.0.0.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/openJDK-1.8.362.1/jre/lib/rt.jar:/Users/juntzhang/src/github/apache/spark/examples/target/scala-2.12/test-classes:/Users/juntzhang/src/github/apache/spark/examples/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/juntzhang/src/github/apache/spark/core/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/apache/avro/avro/1.11.0/avro-1.11.0.jar:/Users/juntzhang/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.13.4/jackson-core-2.13.4.jar:/Users/juntzhang/.m2/repository/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar:/Users/juntzhang/.m2/repository/org/apache/avro/avro-mapred/1.11.0/avro-mapred-1.11.0.jar:/Users/juntzhang/.m2/repository/org/apache/avro/avro-ipc/1.11.0/avro-ipc-1.11.0.jar:/Users/juntzhang/.m2/repository/org/tukaani/xz/1.8/xz-1.8.jar:/Users/juntzhang/.m2/repository/com/twitter/chill_2.12/0.10.0/chill_2.12-0.10.0.jar:/Users/juntzhang/.m2/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/Users/juntzhang/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/juntzhang/.m2/repository/com/twitter/chill-java/0.10.0/chill-java-0.10.0.jar:/Users/juntzhang/.m2/repository/org/apache/xbean/xbean-asm9-shaded/4.20/xbean-asm9-shaded-4.20.jar:/Users/juntzhang/.m2/repository/org/apache/hadoop/hadoop-client-api/3.3.2/hadoop-client-api-3.3.2.jar:/Users/juntzhang/.m2/repository/org/apache/hadoop/hadoop-client-runtime/3.3.2/hadoop-client-runtime-3.3.2.jar:/Users/juntzhang/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/Users/juntzhang/src/github/apache/spark/launcher/target/scala-2.12/classes:/Users/juntzhang/src/github/apache/spark/common/kvstore/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/juntzhang/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.13.4/jackson-annotations-2.13.4.jar:/Users/juntzhang/.m2/repository/org/rocksdb/rocksdbjni/6.20.3/rocksdbjni-6.20.3.jar:/Users/juntzhang/src/github/apache/spark/common/network-common/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/Users/juntzhang/.m2/repository/com/google/crypto/tink/tink/1.6.1/tink-1.6.1.jar:/Users/juntzhang/src/github/apache/spark/common/network-shuffle/target/scala-2.12/classes:/Users/juntzhang/src/github/apache/spark/common/unsafe/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/Users/juntzhang/.m2/repository/org/apache/curator/curator-recipes/2.13.0/curator-recipes-2.13.0.jar:/Users/juntzhang/.m2/repository/org/apache/curator/curator-framework/2.13.0/curator-framework-2.13.0.jar:/Users/juntzhang/.m2/repository/org/apache/curator/curator-client/2.13.0/curator-client-2.13.0.jar:/Users/juntzhang/.m2/repository/org/apache/zookeeper/zookeeper/3.6.2/zookeeper-3.6.2.jar:/Users/juntzhang/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/juntzhang/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.6.2/zookeeper-jute-3.6.2.jar:/Users/juntzhang/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-plus/9.4.48.v20220622/jetty-plus-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-webapp/9.4.48.v20220622/jetty-webapp-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-xml/9.4.48.v20220622/jetty-xml-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-jndi/9.4.48.v20220622/jetty-jndi-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-security/9.4.48.v20220622/jetty-security-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-util/9.4.48.v20220622/jetty-util-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-server/9.4.48.v20220622/jetty-server-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-io/9.4.48.v20220622/jetty-io-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-http/9.4.48.v20220622/jetty-http-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-continuation/9.4.48.v20220622/jetty-continuation-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-servlet/9.4.48.v20220622/jetty-servlet-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-util-ajax/9.4.48.v20220622/jetty-util-ajax-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-proxy/9.4.48.v20220622/jetty-proxy-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-client/9.4.48.v20220622/jetty-client-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/jetty-servlets/9.4.48.v20220622/jetty-servlets-9.4.48.v20220622.jar:/Users/juntzhang/.m2/repository/jakarta/servlet/jakarta.servlet-api/4.0.3/jakarta.servlet-api-4.0.3.jar:/Users/juntzhang/.m2/repository/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/Users/juntzhang/.m2/repository/org/apache/commons/commons-lang3/3.12.0/commons-lang3-3.12.0.jar:/Users/juntzhang/.m2/repository/org/apache/commons/commons-text/1.9/commons-text-1.9.jar:/Users/juntzhang/.m2/repository/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar:/Users/juntzhang/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/juntzhang/.m2/repository/org/apache/commons/commons-collections4/4.4/commons-collections4-4.4.jar:/Users/juntzhang/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/Users/juntzhang/.m2/repository/org/slf4j/slf4j-api/1.7.32/slf4j-api-1.7.32.jar:/Users/juntzhang/.m2/repository/org/slf4j/jul-to-slf4j/1.7.32/jul-to-slf4j-1.7.32.jar:/Users/juntzhang/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.32/jcl-over-slf4j-1.7.32.jar:/Users/juntzhang/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.17.2/log4j-slf4j-impl-2.17.2.jar:/Users/juntzhang/.m2/repository/org/apache/logging/log4j/log4j-api/2.17.2/log4j-api-2.17.2.jar:/Users/juntzhang/.m2/repository/org/apache/logging/log4j/log4j-core/2.17.2/log4j-core-2.17.2.jar:/Users/juntzhang/.m2/repository/org/apache/logging/log4j/log4j-1.2-api/2.17.2/log4j-1.2-api-2.17.2.jar:/Users/juntzhang/.m2/repository/com/ning/compress-lzf/1.1/compress-lzf-1.1.jar:/Users/juntzhang/.m2/repository/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar:/Users/juntzhang/.m2/repository/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar:/Users/juntzhang/.m2/repository/com/github/luben/zstd-jni/1.5.2-1/zstd-jni-1.5.2-1.jar:/Users/juntzhang/.m2/repository/org/roaringbitmap/RoaringBitmap/0.9.25/RoaringBitmap-0.9.25.jar:/Users/juntzhang/.m2/repository/org/roaringbitmap/shims/0.9.25/shims-0.9.25.jar:/Users/juntzhang/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.2.0/scala-xml_2.12-1.2.0.jar:/Users/juntzhang/.m2/repository/org/scala-lang/scala-reflect/2.12.15/scala-reflect-2.12.15.jar:/Users/juntzhang/.m2/repository/org/json4s/json4s-jackson_2.12/3.7.0-M11/json4s-jackson_2.12-3.7.0-M11.jar:/Users/juntzhang/.m2/repository/org/json4s/json4s-core_2.12/3.7.0-M11/json4s-core_2.12-3.7.0-M11.jar:/Users/juntzhang/.m2/repository/org/json4s/json4s-ast_2.12/3.7.0-M11/json4s-ast_2.12-3.7.0-M11.jar:/Users/juntzhang/.m2/repository/org/json4s/json4s-scalap_2.12/3.7.0-M11/json4s-scalap_2.12-3.7.0-M11.jar:/Users/juntzhang/.m2/repository/org/glassfish/jersey/core/jersey-client/2.36/jersey-client-2.36.jar:/Users/juntzhang/.m2/repository/jakarta/ws/rs/jakarta.ws.rs-api/2.1.6/jakarta.ws.rs-api-2.1.6.jar:/Users/juntzhang/.m2/repository/org/glassfish/hk2/external/jakarta.inject/2.6.1/jakarta.inject-2.6.1.jar:/Users/juntzhang/.m2/repository/org/glassfish/jersey/core/jersey-common/2.36/jersey-common-2.36.jar:/Users/juntzhang/.m2/repository/jakarta/annotation/jakarta.annotation-api/1.3.5/jakarta.annotation-api-1.3.5.jar:/Users/juntzhang/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.3/osgi-resource-locator-1.0.3.jar:/Users/juntzhang/.m2/repository/org/glassfish/jersey/core/jersey-server/2.36/jersey-server-2.36.jar:/Users/juntzhang/.m2/repository/jakarta/validation/jakarta.validation-api/2.0.2/jakarta.validation-api-2.0.2.jar:/Users/juntzhang/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.36/jersey-container-servlet-2.36.jar:/Users/juntzhang/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.36/jersey-container-servlet-core-2.36.jar:/Users/juntzhang/.m2/repository/org/glassfish/jersey/inject/jersey-hk2/2.36/jersey-hk2-2.36.jar:/Users/juntzhang/.m2/repository/org/glassfish/hk2/hk2-locator/2.6.1/hk2-locator-2.6.1.jar:/Users/juntzhang/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.6.1/aopalliance-repackaged-2.6.1.jar:/Users/juntzhang/.m2/repository/org/glassfish/hk2/hk2-api/2.6.1/hk2-api-2.6.1.jar:/Users/juntzhang/.m2/repository/org/glassfish/hk2/hk2-utils/2.6.1/hk2-utils-2.6.1.jar:/Users/juntzhang/.m2/repository/org/javassist/javassist/3.25.0-GA/javassist-3.25.0-GA.jar:/Users/juntzhang/.m2/repository/io/netty/netty-all/4.1.74.Final/netty-all-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-buffer/4.1.74.Final/netty-buffer-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-codec/4.1.74.Final/netty-codec-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-common/4.1.74.Final/netty-common-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-handler/4.1.74.Final/netty-handler-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-tcnative-classes/2.0.48.Final/netty-tcnative-classes-2.0.48.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-resolver/4.1.74.Final/netty-resolver-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-transport/4.1.74.Final/netty-transport-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-transport-classes-epoll/4.1.74.Final/netty-transport-classes-epoll-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.74.Final/netty-transport-native-unix-common-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-transport-classes-kqueue/4.1.74.Final/netty-transport-classes-kqueue-4.1.74.Final.jar:/Users/juntzhang/.m2/repository/io/netty/netty-transport-native-epoll/4.1.74.Final/netty-transport-native-epoll-4.1.74.Final-linux-x86_64.jar:/Users/juntzhang/.m2/repository/io/netty/netty-transport-native-epoll/4.1.74.Final/netty-transport-native-epoll-4.1.74.Final-linux-aarch_64.jar:/Users/juntzhang/.m2/repository/io/netty/netty-transport-native-kqueue/4.1.74.Final/netty-transport-native-kqueue-4.1.74.Final-osx-x86_64.jar:/Users/juntzhang/.m2/repository/io/netty/netty-transport-native-kqueue/4.1.74.Final/netty-transport-native-kqueue-4.1.74.Final-osx-aarch_64.jar:/Users/juntzhang/.m2/repository/com/clearspring/analytics/stream/2.9.6/stream-2.9.6.jar:/Users/juntzhang/.m2/repository/io/dropwizard/metrics/metrics-core/4.2.7/metrics-core-4.2.7.jar:/Users/juntzhang/.m2/repository/io/dropwizard/metrics/metrics-jvm/4.2.7/metrics-jvm-4.2.7.jar:/Users/juntzhang/.m2/repository/io/dropwizard/metrics/metrics-json/4.2.7/metrics-json-4.2.7.jar:/Users/juntzhang/.m2/repository/io/dropwizard/metrics/metrics-graphite/4.2.7/metrics-graphite-4.2.7.jar:/Users/juntzhang/.m2/repository/io/dropwizard/metrics/metrics-jmx/4.2.7/metrics-jmx-4.2.7.jar:/Users/juntzhang/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.13.4.1/jackson-databind-2.13.4.1.jar:/Users/juntzhang/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.13.4/jackson-module-scala_2.12-2.13.4.jar:/Users/juntzhang/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/Users/juntzhang/.m2/repository/org/apache/ivy/ivy/2.5.0/ivy-2.5.0.jar:/Users/juntzhang/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/juntzhang/.m2/repository/net/razorvine/pickle/1.2/pickle-1.2.jar:/Users/juntzhang/.m2/repository/net/sf/py4j/py4j/0.10.9.5/py4j-0.10.9.5.jar:/Users/juntzhang/src/github/apache/spark/common/tags/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/apache/commons/commons-crypto/1.1.0/commons-crypto-1.1.0.jar:/Users/juntzhang/src/github/apache/spark/streaming/target/scala-2.12/classes:/Users/juntzhang/src/github/apache/spark/mllib/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.12/1.1.2/scala-parser-combinators_2.12-1.1.2.jar:/Users/juntzhang/src/github/apache/spark/sql/core/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/com/univocity/univocity-parsers/2.9.1/univocity-parsers-2.9.1.jar:/Users/juntzhang/src/github/apache/spark/common/sketch/target/scala-2.12/classes:/Users/juntzhang/src/github/apache/spark/sql/catalyst/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/codehaus/janino/janino/3.0.16/janino-3.0.16.jar:/Users/juntzhang/.m2/repository/org/codehaus/janino/commons-compiler/3.0.16/commons-compiler-3.0.16.jar:/Users/juntzhang/.m2/repository/org/antlr/antlr4-runtime/4.8/antlr4-runtime-4.8.jar:/Users/juntzhang/.m2/repository/org/apache/arrow/arrow-vector/7.0.0/arrow-vector-7.0.0.jar:/Users/juntzhang/.m2/repository/org/apache/arrow/arrow-format/7.0.0/arrow-format-7.0.0.jar:/Users/juntzhang/.m2/repository/org/apache/arrow/arrow-memory-core/7.0.0/arrow-memory-core-7.0.0.jar:/Users/juntzhang/.m2/repository/com/google/flatbuffers/flatbuffers-java/1.12.0/flatbuffers-java-1.12.0.jar:/Users/juntzhang/.m2/repository/org/apache/arrow/arrow-memory-netty/7.0.0/arrow-memory-netty-7.0.0.jar:/Users/juntzhang/.m2/repository/org/apache/orc/orc-core/1.7.6/orc-core-1.7.6.jar:/Users/juntzhang/.m2/repository/org/apache/orc/orc-shims/1.7.6/orc-shims-1.7.6.jar:/Users/juntzhang/.m2/repository/io/airlift/aircompressor/0.21/aircompressor-0.21.jar:/Users/juntzhang/.m2/repository/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/Users/juntzhang/.m2/repository/org/threeten/threeten-extra/1.5.0/threeten-extra-1.5.0.jar:/Users/juntzhang/.m2/repository/org/apache/orc/orc-mapreduce/1.7.6/orc-mapreduce-1.7.6.jar:/Users/juntzhang/.m2/repository/org/apache/hive/hive-storage-api/2.7.2/hive-storage-api-2.7.2.jar:/Users/juntzhang/.m2/repository/org/apache/parquet/parquet-column/1.12.2/parquet-column-1.12.2.jar:/Users/juntzhang/.m2/repository/org/apache/parquet/parquet-common/1.12.2/parquet-common-1.12.2.jar:/Users/juntzhang/.m2/repository/org/apache/parquet/parquet-encoding/1.12.2/parquet-encoding-1.12.2.jar:/Users/juntzhang/.m2/repository/org/apache/parquet/parquet-hadoop/1.12.2/parquet-hadoop-1.12.2.jar:/Users/juntzhang/.m2/repository/org/apache/parquet/parquet-format-structures/1.12.2/parquet-format-structures-1.12.2.jar:/Users/juntzhang/.m2/repository/org/apache/parquet/parquet-jackson/1.12.2/parquet-jackson-1.12.2.jar:/Users/juntzhang/src/github/apache/spark/mllib-local/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/scalanlp/breeze_2.12/1.2/breeze_2.12-1.2.jar:/Users/juntzhang/.m2/repository/org/scalanlp/breeze-macros_2.12/1.2/breeze-macros_2.12-1.2.jar:/Users/juntzhang/.m2/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/Users/juntzhang/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/juntzhang/.m2/repository/com/github/wendykierp/JTransforms/3.1/JTransforms-3.1.jar:/Users/juntzhang/.m2/repository/pl/edu/icm/JLargeArrays/1.5/JLargeArrays-1.5.jar:/Users/juntzhang/.m2/repository/com/chuusai/shapeless_2.12/2.3.7/shapeless_2.12-2.3.7.jar:/Users/juntzhang/.m2/repository/org/typelevel/spire_2.12/0.17.0/spire_2.12-0.17.0.jar:/Users/juntzhang/.m2/repository/org/typelevel/spire-macros_2.12/0.17.0/spire-macros_2.12-0.17.0.jar:/Users/juntzhang/.m2/repository/org/typelevel/spire-platform_2.12/0.17.0/spire-platform_2.12-0.17.0.jar:/Users/juntzhang/.m2/repository/org/typelevel/spire-util_2.12/0.17.0/spire-util_2.12-0.17.0.jar:/Users/juntzhang/.m2/repository/org/typelevel/algebra_2.12/2.0.1/algebra_2.12-2.0.1.jar:/Users/juntzhang/.m2/repository/org/typelevel/cats-kernel_2.12/2.1.1/cats-kernel_2.12-2.1.1.jar:/Users/juntzhang/.m2/repository/org/scala-lang/modules/scala-collection-compat_2.12/2.1.1/scala-collection-compat_2.12-2.1.1.jar:/Users/juntzhang/.m2/repository/org/jpmml/pmml-model/1.4.8/pmml-model-1.4.8.jar:/Users/juntzhang/.m2/repository/org/glassfish/jaxb/jaxb-runtime/2.3.2/jaxb-runtime-2.3.2.jar:/Users/juntzhang/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/2.3.2/jakarta.xml.bind-api-2.3.2.jar:/Users/juntzhang/.m2/repository/com/sun/istack/istack-commons-runtime/3.0.8/istack-commons-runtime-3.0.8.jar:/Users/juntzhang/.m2/repository/dev/ludovic/netlib/blas/2.2.1/blas-2.2.1.jar:/Users/juntzhang/.m2/repository/dev/ludovic/netlib/lapack/2.2.1/lapack-2.2.1.jar:/Users/juntzhang/.m2/repository/dev/ludovic/netlib/arpack/2.2.1/arpack-2.2.1.jar:/Users/juntzhang/src/github/apache/spark/sql/hive/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/apache/hive/hive-common/2.3.9/hive-common-2.3.9.jar:/Users/juntzhang/.m2/repository/commons-cli/commons-cli/1.5.0/commons-cli-1.5.0.jar:/Users/juntzhang/.m2/repository/jline/jline/2.14.6/jline-2.14.6.jar:/Users/juntzhang/.m2/repository/com/tdunning/json/1.8/json-1.8.jar:/Users/juntzhang/.m2/repository/com/github/joshelser/dropwizard-metrics-hadoop-metrics2-reporter/0.1.2/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/Users/juntzhang/.m2/repository/org/apache/hive/hive-exec/2.3.9/hive-exec-2.3.9-core.jar:/Users/juntzhang/.m2/repository/org/apache/hive/hive-vector-code-gen/2.3.9/hive-vector-code-gen-2.3.9.jar:/Users/juntzhang/.m2/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/Users/juntzhang/.m2/repository/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/Users/juntzhang/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/Users/juntzhang/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/juntzhang/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/Users/juntzhang/.m2/repository/org/apache/hive/hive-metastore/2.3.9/hive-metastore-2.3.9.jar:/Users/juntzhang/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/Users/juntzhang/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/juntzhang/.m2/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/Users/juntzhang/.m2/repository/com/zaxxer/HikariCP/2.5.1/HikariCP-2.5.1.jar:/Users/juntzhang/.m2/repository/org/datanucleus/datanucleus-api-jdo/4.2.4/datanucleus-api-jdo-4.2.4.jar:/Users/juntzhang/.m2/repository/org/datanucleus/datanucleus-rdbms/4.1.19/datanucleus-rdbms-4.1.19.jar:/Users/juntzhang/.m2/repository/commons-pool/commons-pool/1.5.4/commons-pool-1.5.4.jar:/Users/juntzhang/.m2/repository/commons-dbcp/commons-dbcp/1.4/commons-dbcp-1.4.jar:/Users/juntzhang/.m2/repository/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar:/Users/juntzhang/.m2/repository/javax/transaction/jta/1.1/jta-1.1.jar:/Users/juntzhang/.m2/repository/org/datanucleus/javax.jdo/3.2.0-m3/javax.jdo-3.2.0-m3.jar:/Users/juntzhang/.m2/repository/javax/transaction/transaction-api/1.1/transaction-api-1.1.jar:/Users/juntzhang/.m2/repository/org/apache/hive/hive-serde/2.3.9/hive-serde-2.3.9.jar:/Users/juntzhang/.m2/repository/org/apache/hive/hive-shims/2.3.9/hive-shims-2.3.9.jar:/Users/juntzhang/.m2/repository/org/apache/hive/shims/hive-shims-common/2.3.9/hive-shims-common-2.3.9.jar:/Users/juntzhang/.m2/repository/org/apache/hive/shims/hive-shims-0.23/2.3.9/hive-shims-0.23-2.3.9.jar:/Users/juntzhang/.m2/repository/org/apache/hive/shims/hive-shims-scheduler/2.3.9/hive-shims-scheduler-2.3.9.jar:/Users/juntzhang/.m2/repository/org/apache/hive/hive-llap-common/2.3.9/hive-llap-common-2.3.9.jar:/Users/juntzhang/.m2/repository/org/apache/hive/hive-llap-client/2.3.9/hive-llap-client-2.3.9.jar:/Users/juntzhang/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/Users/juntzhang/.m2/repository/org/apache/httpcomponents/httpcore/4.4.14/httpcore-4.4.14.jar:/Users/juntzhang/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/Users/juntzhang/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/Users/juntzhang/.m2/repository/joda-time/joda-time/2.10.13/joda-time-2.10.13.jar:/Users/juntzhang/.m2/repository/org/jodd/jodd-core/3.5.2/jodd-core-3.5.2.jar:/Users/juntzhang/.m2/repository/org/datanucleus/datanucleus-core/4.1.17/datanucleus-core-4.1.17.jar:/Users/juntzhang/.m2/repository/org/apache/thrift/libthrift/0.12.0/libthrift-0.12.0.jar:/Users/juntzhang/.m2/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/Users/juntzhang/.m2/repository/org/apache/derby/derby/10.14.2.0/derby-10.14.2.0.jar:/Users/juntzhang/src/github/apache/spark/graphx/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/Users/juntzhang/src/github/apache/spark/external/kafka-0-10/target/scala-2.12/classes:/Users/juntzhang/src/github/apache/spark/external/kafka-0-10-token-provider/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/apache/kafka/kafka-clients/2.8.1/kafka-clients-2.8.1.jar:/Users/juntzhang/src/github/apache/spark/external/kafka-0-10-sql/target/scala-2.12/classes:/Users/juntzhang/.m2/repository/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar:/Users/juntzhang/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/Users/juntzhang/.m2/repository/org/scalacheck/scalacheck_2.12/1.15.4/scalacheck_2.12-1.15.4.jar:/Users/juntzhang/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/Users/juntzhang/.m2/repository/org/scala-lang/scala-library/2.12.15/scala-library-2.12.15.jar:/Users/juntzhang/.m2/repository/com/github/scopt/scopt_2.12/3.7.1/scopt_2.12-3.7.1.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest_2.12/3.3.0-SNAP3/scalatest_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-core_2.12/3.3.0-SNAP3/scalatest-core_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-compatible/3.3.0-SNAP3/scalatest-compatible-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalactic/scalactic_2.12/3.3.0-SNAP3/scalactic_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-featurespec_2.12/3.3.0-SNAP3/scalatest-featurespec_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-flatspec_2.12/3.3.0-SNAP3/scalatest-flatspec_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-freespec_2.12/3.3.0-SNAP3/scalatest-freespec_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-funsuite_2.12/3.3.0-SNAP3/scalatest-funsuite_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-funspec_2.12/3.3.0-SNAP3/scalatest-funspec_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-propspec_2.12/3.3.0-SNAP3/scalatest-propspec_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-refspec_2.12/3.3.0-SNAP3/scalatest-refspec_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-wordspec_2.12/3.3.0-SNAP3/scalatest-wordspec_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-diagrams_2.12/3.3.0-SNAP3/scalatest-diagrams_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-matchers-core_2.12/3.3.0-SNAP3/scalatest-matchers-core_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-shouldmatchers_2.12/3.3.0-SNAP3/scalatest-shouldmatchers_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatest/scalatest-mustmatchers_2.12/3.3.0-SNAP3/scalatest-mustmatchers_2.12-3.3.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatestplus/scalacheck-1-15_2.12/3.3.0.0-SNAP3/scalacheck-1-15_2.12-3.3.0.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/scalatestplus/mockito-4-2_2.12/3.2.11.0/mockito-4-2_2.12-3.2.11.0.jar:/Users/juntzhang/.m2/repository/org/mockito/mockito-core/4.2.0/mockito-core-4.2.0.jar:/Users/juntzhang/.m2/repository/net/bytebuddy/byte-buddy/1.12.4/byte-buddy-1.12.4.jar:/Users/juntzhang/.m2/repository/net/bytebuddy/byte-buddy-agent/1.12.4/byte-buddy-agent-1.12.4.jar:/Users/juntzhang/.m2/repository/org/objenesis/objenesis/3.2/objenesis-3.2.jar:/Users/juntzhang/.m2/repository/org/scalatestplus/selenium-3-141_2.12/3.3.0.0-SNAP3/selenium-3-141_2.12-3.3.0.0-SNAP3.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-java/3.141.59/selenium-java-3.141.59.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-api/3.141.59/selenium-api-3.141.59.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-chrome-driver/3.141.59/selenium-chrome-driver-3.141.59.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-edge-driver/3.141.59/selenium-edge-driver-3.141.59.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-firefox-driver/3.141.59/selenium-firefox-driver-3.141.59.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-ie-driver/3.141.59/selenium-ie-driver-3.141.59.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-opera-driver/3.141.59/selenium-opera-driver-3.141.59.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-remote-driver/3.141.59/selenium-remote-driver-3.141.59.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-safari-driver/3.141.59/selenium-safari-driver-3.141.59.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/selenium-support/3.141.59/selenium-support-3.141.59.jar:/Users/juntzhang/.m2/repository/org/apache/commons/commons-exec/1.3/commons-exec-1.3.jar:/Users/juntzhang/.m2/repository/com/squareup/okhttp3/okhttp/3.11.0/okhttp-3.11.0.jar:/Users/juntzhang/.m2/repository/com/squareup/okio/okio/1.14.0/okio-1.14.0.jar:/Users/juntzhang/.m2/repository/org/seleniumhq/selenium/htmlunit-driver/2.50.0/htmlunit-driver-2.50.0.jar:/Users/juntzhang/.m2/repository/net/sourceforge/htmlunit/htmlunit/2.50.0/htmlunit-2.50.0.jar:/Users/juntzhang/.m2/repository/xalan/xalan/2.7.2/xalan-2.7.2.jar:/Users/juntzhang/.m2/repository/xalan/serializer/2.7.2/serializer-2.7.2.jar:/Users/juntzhang/.m2/repository/org/apache/httpcomponents/httpmime/4.5.13/httpmime-4.5.13.jar:/Users/juntzhang/.m2/repository/net/sourceforge/htmlunit/htmlunit-core-js/2.50.0/htmlunit-core-js-2.50.0.jar:/Users/juntzhang/.m2/repository/net/sourceforge/htmlunit/neko-htmlunit/2.50.0/neko-htmlunit-2.50.0.jar:/Users/juntzhang/.m2/repository/xerces/xercesImpl/2.12.2/xercesImpl-2.12.2.jar:/Users/juntzhang/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/Users/juntzhang/.m2/repository/net/sourceforge/htmlunit/htmlunit-cssparser/1.7.0/htmlunit-cssparser-1.7.0.jar:/Users/juntzhang/.m2/repository/commons-net/commons-net/3.8.0/commons-net-3.8.0.jar:/Users/juntzhang/.m2/repository/org/brotli/dec/0.1.2/dec-0.1.2.jar:/Users/juntzhang/.m2/repository/com/shapesecurity/salvation2/3.0.0/salvation2-3.0.0.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/websocket/websocket-client/9.4.40.v20210413/websocket-client-9.4.40.v20210413.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/websocket/websocket-common/9.4.40.v20210413/websocket-common-9.4.40.v20210413.jar:/Users/juntzhang/.m2/repository/org/eclipse/jetty/websocket/websocket-api/9.4.40.v20210413/websocket-api-9.4.40.v20210413.jar:/Users/juntzhang/.m2/repository/junit/junit/4.13.2/junit-4.13.2.jar:/Users/juntzhang/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/juntzhang/.m2/repository/com/github/sbt/junit-interface/0.13.3/junit-interface-0.13.3.jar:/Users/juntzhang/src/github/apache/spark/core/target/spark-core_2.12-3.3.1.jar:/Users/juntzhang/src/github/apache/spark/launcher/target/spark-launcher_2.12-3.3.1.jar:/Users/juntzhang/src/github/apache/spark/common/kvstore/target/spark-kvstore_2.12-3.3.1.jar:/Users/juntzhang/src/github/apache/spark/common/network-common/target/spark-network-common_2.12-3.3.1.jar:/Users/juntzhang/src/github/apache/spark/common/network-shuffle/target/spark-network-shuffle_2.12-3.3.1.jar:/Users/juntzhang/src/github/apache/spark/common/unsafe/target/spark-unsafe_2.12-3.3.1.jar:/Users/juntzhang/src/github/apache/spark/common/tags/target/spark-tags_2.12-3.3.1.jar:/Users/juntzhang/src/github/apache/spark/streaming/target/spark-streaming_2.12-3.3.1.jar:/Users/juntzhang/src/github/apache/spark/mllib/target/spark-mllib_2.12-3.3.1.jar:/Users/juntzhang/src/github/apache/spark/sql/core/target/spark-sql_2.12-3.3.1.jar:/Users/juntzhang/src/github/apache/spark/common/sketch/target/spark-sketch_2.12-3.3.1.jar:/Users/juntzhang/src/github/apache/spark/sql/catalyst/target/spark-catalyst_2.12-3.3.1.jar:/Users/juntzhang/src/github/apache/spark/mllib-local/target/spark-mllib-local_2.12-3.3.1.jar:/Users/juntzhang/src/github/apache/spark/sql/hive/target/spark-hive_2.12-3.3.1.jar:/Users/juntzhang/src/github/apache/spark/graphx/target/spark-graphx_2.12-3.3.1.jar:/Users/juntzhang/src/github/apache/spark/external/kafka-0-10/target/spark-streaming-kafka-0-10_2.12-3.3.1.jar:/Users/juntzhang/src/github/apache/spark/external/kafka-0-10-token-provider/target/spark-token-provider-kafka-0-10_2.12-3.3.1.jar:/Users/juntzhang/src/github/apache/spark/external/kafka-0-10-sql/target/spark-sql-kafka-0-10_2.12-3.3.1.jar:/Applications/IntelliJ IDEA CE.app/Contents/lib/idea_rt.jar" org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner -s cn.juntaozhang.example.spark.PhysicalPlanSpec -testName "Dynamic Partition Pruning(DPP)" -showProgressMessages true
Testing started at 13:31 ...
Connected to the target VM, address: '127.0.0.1:53460', transport: 'socket'
13:31:38.923 [ScalaTest-run] INFO  org.apache.spark.SparkContext - Running Spark version 3.3.1
13:31:38.983 [ScalaTest-run] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13:31:39.035 [ScalaTest-run] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
13:31:39.035 [ScalaTest-run] INFO  org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
13:31:39.035 [ScalaTest-run] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
13:31:39.035 [ScalaTest-run] INFO  org.apache.spark.SparkContext - Submitted application: LogicalPlanTest
13:31:39.060 [ScalaTest-run] INFO  org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
13:31:39.069 [ScalaTest-run] INFO  org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
13:31:39.069 [ScalaTest-run] INFO  org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
13:31:39.149 [ScalaTest-run] INFO  org.apache.spark.SecurityManager - Changing view acls to: juntzhang
13:31:39.149 [ScalaTest-run] INFO  org.apache.spark.SecurityManager - Changing modify acls to: juntzhang
13:31:39.150 [ScalaTest-run] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
13:31:39.150 [ScalaTest-run] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
13:31:39.150 [ScalaTest-run] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(juntzhang); groups with view permissions: Set(); users  with modify permissions: Set(juntzhang); groups with modify permissions: Set()
13:31:39.235 [ScalaTest-run] DEBUG io.netty.util.internal.logging.InternalLoggerFactory - Using SLF4J as the default logging framework
13:31:39.245 [ScalaTest-run] DEBUG io.netty.util.internal.InternalThreadLocalMap - -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
13:31:39.245 [ScalaTest-run] DEBUG io.netty.util.internal.InternalThreadLocalMap - -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
13:31:39.254 [ScalaTest-run] DEBUG io.netty.channel.MultithreadEventLoopGroup - -Dio.netty.eventLoopThreads: 20
13:31:39.282 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - -Dio.netty.noUnsafe: false
13:31:39.283 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - Java version: 8
13:31:39.283 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - sun.misc.Unsafe.theUnsafe: available
13:31:39.283 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - sun.misc.Unsafe.copyMemory: available
13:31:39.284 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - java.nio.Buffer.address: available
13:31:39.284 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - direct buffer constructor: available
13:31:39.284 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - java.nio.Bits.unaligned: available, true
13:31:39.284 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
13:31:39.284 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - java.nio.DirectByteBuffer.<init>(long, int): available
13:31:39.285 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - sun.misc.Unsafe: available
13:31:39.285 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.tmpdir: /var/folders/dc/_x4vzy4100s9qz48fghk10h80000gq/T (java.io.tmpdir)
13:31:39.285 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.bitMode: 64 (sun.arch.data.model)
13:31:39.285 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - Platform: MacOS
13:31:39.285 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.maxDirectMemory: 15271460864 bytes
13:31:39.285 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.uninitializedArrayAllocationThreshold: -1
13:31:39.286 [ScalaTest-run] DEBUG io.netty.util.internal.CleanerJava6 - java.nio.ByteBuffer.cleaner(): available
13:31:39.286 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.noPreferDirect: false
13:31:39.287 [ScalaTest-run] DEBUG io.netty.channel.nio.NioEventLoop - -Dio.netty.noKeySetOptimization: false
13:31:39.287 [ScalaTest-run] DEBUG io.netty.channel.nio.NioEventLoop - -Dio.netty.selectorAutoRebuildThreshold: 512
13:31:39.291 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - org.jctools-core.MpscChunkedArrayQueue: available
13:31:39.303 [ScalaTest-run] DEBUG io.netty.util.ResourceLeakDetector - -Dio.netty.leakDetection.level: simple
13:31:39.303 [ScalaTest-run] DEBUG io.netty.util.ResourceLeakDetector - -Dio.netty.leakDetection.targetRecords: 4
13:31:39.305 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.numHeapArenas: 20
13:31:39.305 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.numDirectArenas: 20
13:31:39.305 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.pageSize: 8192
13:31:39.305 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxOrder: 11
13:31:39.305 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.chunkSize: 16777216
13:31:39.305 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.smallCacheSize: 256
13:31:39.305 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.normalCacheSize: 64
13:31:39.305 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxCachedBufferCapacity: 32768
13:31:39.305 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.cacheTrimInterval: 8192
13:31:39.305 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.cacheTrimIntervalMillis: 0
13:31:39.305 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.useCacheForAllThreads: true
13:31:39.305 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
13:31:39.331 [ScalaTest-run] DEBUG io.netty.channel.DefaultChannelId - -Dio.netty.processId: 83626 (auto-detected)
13:31:39.332 [ScalaTest-run] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv4Stack: false
13:31:39.332 [ScalaTest-run] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv6Addresses: false
13:31:39.336 [ScalaTest-run] DEBUG io.netty.util.NetUtilInitializations - Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
13:31:39.336 [ScalaTest-run] DEBUG io.netty.util.NetUtil - Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
13:31:39.338 [ScalaTest-run] DEBUG io.netty.channel.DefaultChannelId - -Dio.netty.machineId: bc:d0:74:ff:fe:74:c6:da (auto-detected)
13:31:39.372 [ScalaTest-run] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.allocator.type: pooled
13:31:39.372 [ScalaTest-run] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.threadLocalDirectBufferSize: 0
13:31:39.373 [ScalaTest-run] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.maxThreadLocalCharBufferSize: 16384
13:31:39.384 [ScalaTest-run] DEBUG org.apache.spark.network.server.TransportServer - Shuffle server started on port: 53476
13:31:39.398 [ScalaTest-run] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 53476.
13:31:39.399 [ScalaTest-run] DEBUG org.apache.spark.SparkEnv - Using serializer: class org.apache.spark.serializer.JavaSerializer
13:31:39.433 [ScalaTest-run] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
13:31:39.433 [ScalaTest-run] DEBUG org.apache.spark.MapOutputTrackerMasterEndpoint - init
13:31:39.478 [ScalaTest-run] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
13:31:39.491 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
13:31:39.491 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
13:31:39.495 [ScalaTest-run] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
13:31:39.527 [ScalaTest-run] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /private/var/folders/dc/_x4vzy4100s9qz48fghk10h80000gq/T/blockmgr-253b3faf-574f-44ff-935a-c94d22999f94
13:31:39.529 [ScalaTest-run] DEBUG org.apache.spark.storage.DiskBlockManager - Adding shutdown hook
13:31:39.530 [ScalaTest-run] DEBUG org.apache.spark.util.ShutdownHookManager - Adding shutdown hook
13:31:39.568 [ScalaTest-run] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 8.4 GiB
13:31:39.583 [ScalaTest-run] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
13:31:39.584 [ScalaTest-run] DEBUG org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - init
13:31:39.599 [ScalaTest-run] DEBUG org.apache.spark.SecurityManager - Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
13:31:39.735 [ScalaTest-run] DEBUG org.apache.spark.ui.JettyUtils - Using requestHeaderSize: 8192
13:31:39.754 [ScalaTest-run] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
13:31:39.878 [ScalaTest-run] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host 192.168.31.245
13:31:39.886 [ScalaTest-run] INFO  org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
13:31:39.906 [ScalaTest-run] DEBUG org.apache.spark.network.server.TransportServer - Shuffle server started on port: 53481
13:31:39.906 [ScalaTest-run] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53481.
13:31:39.906 [ScalaTest-run] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.31.245:53481
13:31:39.908 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
13:31:39.913 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.245, 53481, None)
13:31:39.916 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.245
13:31:39.917 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.31.245:53481 with 8.4 GiB RAM, BlockManagerId(driver, 192.168.31.245, 53481, None)
13:31:39.919 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.245, 53481, None)
13:31:39.925 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.31.245, 53481, None)
13:31:40.075 [ScalaTest-run] DEBUG org.apache.spark.SparkContext - Adding shutdown hook



13:31:40.294 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
13:31:40.295 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.app.name -> LogicalPlanTest
13:31:40.295 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.master -> local
13:31:40.295 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.internal.SharedState - Applying static initial session options to SparkConf: spark.sql.catalogImplementation -> hive
13:31:40.299 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/Users/juntzhang/src/github/apache/spark/spark-warehouse'.
13:31:41.488 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: 
      SELECT f.id, f.value, d.event
      FROM fact_table f
      JOIN dim_table d
      ON f.date = d.date
      WHERE d.event = 'New Year'
    
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  SELECTf.id,f.value,d.eventFROMfact_tablefJOINdim_tabledONf.date=d.dateWHEREd.event='New Year' <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  SELECTf.id,f.value,d.eventFROMfact_tablefJOINdim_tabledONf.date=d.dateWHEREd.event='New Year' <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryContext =>  SELECTf.id,f.value,d.eventFROMfact_tablefJOINdim_tabledONf.date=d.dateWHEREd.event='New Year' 
org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext =>  SELECTf.id,f.value,d.event FROMfact_tablefJOINdim_tabledONf.date=d.date WHEREd.event='New Year'
org.apache.spark.sql.catalyst.parser.SqlBaseParser$FromClauseContext =>  FROM fact_tablefJOINdim_tabledONf.date=d.date
org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableNameContext =>  fact_table f
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  fact_table
org.apache.spark.sql.catalyst.parser.SqlBaseParser$JoinRelationContext =>   JOIN dim_tabled ONf.date=d.date
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  f.date=d.date
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComparisonContext =>  f.date = d.date
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  f . date
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  f
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  d . date
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  d
org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableNameContext =>  dim_table d
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  dim_table
org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext =>  SELECTf.id,f.value,d.event FROMfact_tablefJOINdim_tabledONf.date=d.date WHEREd.event='New Year'
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  f.id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  f.id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  f . id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  f
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  f.value
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  f.value
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  f . value
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  f
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  d.event
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  d.event
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  d . event
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  d
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  d.event='New Year'
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComparisonContext =>  d.event = 'New Year'
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  d . event
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  d
org.apache.spark.sql.catalyst.parser.SqlBaseParser$StringLiteralContext =>  'New Year'
org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryOrganizationContext => 
13:31:41.804 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: spark_grouping_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleMultipartIdentifierContext =>  spark_grouping_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  spark_grouping_id
13:31:42.148 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
13:31:42.460 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 2.3.9) is file:/Users/juntzhang/src/github/apache/spark/spark-warehouse
13:31:42.571 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.stats.jdbc.timeout does not exist
13:31:42.571 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.stats.retries.wait does not exist
13:31:42.942 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG com.jolbox.bonecp.BoneCPDataSource - JDBC URL = jdbc:derby:;databaseName=metastore_db;create=true, Username = APP, partitions = 1, max (per partition) = 10, min (per partition) = 0, idle max age = 60 min, idle test period = 240 min, strategy = DEFAULT
13:31:44.135 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG com.jolbox.bonecp.BoneCPDataSource - JDBC URL = jdbc:derby:;databaseName=metastore_db;create=true, Username = APP, partitions = 1, max (per partition) = 10, min (per partition) = 0, idle max age = 60 min, idle test period = 240 min, strategy = DEFAULT
13:31:44.474 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.metastore.ObjectStore - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
13:31:44.474 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.metastore.ObjectStore - setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore juntzhang@192.168.31.245
13:31:44.864 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  int <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  int
13:31:44.869 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  int <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  int
13:31:44.870 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
13:31:44.888 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  array<string> <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComplexDataTypeContext =>  array < string >
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
13:31:44.967 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
13:31:44.968 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations => old
'Project ['f.id, 'f.value, 'd.event]
+- 'Filter ('d.event = New Year)
   +- 'Join Inner, ('f.date = 'd.date)
      :- 'SubqueryAlias f
      :  +- 'UnresolvedRelation [fact_table], [], false
      +- 'SubqueryAlias d
         +- 'UnresolvedRelation [dim_table], [], false
==> new
'Project ['f.id, 'f.value, 'd.event]
+- 'Filter ('d.event = New Year)
   +- 'Join Inner, ('f.date = 'd.date)
      :- 'SubqueryAlias f
      :  +- 'SubqueryAlias spark_catalog.default.fact_table
      :     +- 'UnresolvedCatalogRelation `default`.`fact_table`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [], false
      +- 'SubqueryAlias d
         +- 'SubqueryAlias spark_catalog.default.dim_table
            +- 'UnresolvedCatalogRelation `default`.`dim_table`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [], false



org.apache.spark.sql.execution.datasources.FindDataSourceTable => old
'Project ['f.id, 'f.value, 'd.event]
+- 'Filter ('d.event = New Year)
   +- 'Join Inner, ('f.date = 'd.date)
      :- 'SubqueryAlias f
      :  +- 'SubqueryAlias spark_catalog.default.fact_table
      :     +- 'UnresolvedCatalogRelation `default`.`fact_table`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [], false
      +- 'SubqueryAlias d
         +- 'SubqueryAlias spark_catalog.default.dim_table
            +- 'UnresolvedCatalogRelation `default`.`dim_table`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [], false
==> new
'Project ['f.id, 'f.value, 'd.event]
+- 'Filter ('d.event = New Year)
   +- 'Join Inner, ('f.date = 'd.date)
      :- SubqueryAlias f
      :  +- SubqueryAlias spark_catalog.default.fact_table
      :     +- Relation default.fact_table[id#0,value#1,date#2] parquet
      +- SubqueryAlias d
         +- SubqueryAlias spark_catalog.default.dim_table
            +- Relation default.dim_table[event#3,date#4] parquet



org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences => old
'Project ['f.id, 'f.value, 'd.event]
+- 'Filter ('d.event = New Year)
   +- 'Join Inner, ('f.date = 'd.date)
      :- SubqueryAlias f
      :  +- SubqueryAlias spark_catalog.default.fact_table
      :     +- Relation default.fact_table[id#0,value#1,date#2] parquet
      +- SubqueryAlias d
         +- SubqueryAlias spark_catalog.default.dim_table
            +- Relation default.dim_table[event#3,date#4] parquet
==> new
Project [id#0, value#1, event#3]
+- Filter (event#3 = New Year)
   +- Join Inner, (date#2 = date#4)
      :- SubqueryAlias f
      :  +- SubqueryAlias spark_catalog.default.fact_table
      :     +- Relation default.fact_table[id#0,value#1,date#2] parquet
      +- SubqueryAlias d
         +- SubqueryAlias spark_catalog.default.dim_table
            +- Relation default.dim_table[event#3,date#4] parquet



org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAliases => old
'Project [unresolvedalias(cast(id#0 as string), None), unresolvedalias(cast(value#1 as string), None), unresolvedalias(cast(event#3 as string), None)]
+- Project [id#0, value#1, event#3]
   +- Filter (event#3 = New Year)
      +- Join Inner, (date#2 = date#4)
         :- SubqueryAlias f
         :  +- SubqueryAlias spark_catalog.default.fact_table
         :     +- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- SubqueryAlias d
            +- SubqueryAlias spark_catalog.default.dim_table
               +- Relation default.dim_table[event#3,date#4] parquet
==> new
Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, cast(event#3 as string) AS event#15]
+- Project [id#0, value#1, event#3]
   +- Filter (event#3 = New Year)
      +- Join Inner, (date#2 = date#4)
         :- SubqueryAlias f
         :  +- SubqueryAlias spark_catalog.default.fact_table
         :     +- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- SubqueryAlias d
            +- SubqueryAlias spark_catalog.default.dim_table
               +- Relation default.dim_table[event#3,date#4] parquet



org.apache.spark.sql.catalyst.analysis.ResolveTimeZone => old
Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, cast(event#3 as string) AS event#15]
+- Project [id#0, value#1, event#3]
   +- Filter (event#3 = New Year)
      +- Join Inner, (date#2 = date#4)
         :- SubqueryAlias f
         :  +- SubqueryAlias spark_catalog.default.fact_table
         :     +- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- SubqueryAlias d
            +- SubqueryAlias spark_catalog.default.dim_table
               +- Relation default.dim_table[event#3,date#4] parquet
==> new
Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, cast(event#3 as string) AS event#15]
+- Project [id#0, value#1, event#3]
   +- Filter (event#3 = New Year)
      +- Join Inner, (date#2 = date#4)
         :- SubqueryAlias f
         :  +- SubqueryAlias spark_catalog.default.fact_table
         :     +- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- SubqueryAlias d
            +- SubqueryAlias spark_catalog.default.dim_table
               +- Relation default.dim_table[event#3,date#4] parquet



org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis => old
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, cast(event#3 as string) AS event#15]
      +- Project [id#0, value#1, event#3]
         +- Filter (event#3 = New Year)
            +- Join Inner, (date#2 = date#4)
               :- SubqueryAlias f
               :  +- SubqueryAlias spark_catalog.default.fact_table
               :     +- Relation default.fact_table[id#0,value#1,date#2] parquet
               +- SubqueryAlias d
                  +- SubqueryAlias spark_catalog.default.dim_table
                     +- Relation default.dim_table[event#3,date#4] parquet
==> new
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, cast(event#3 as string) AS event#15]
      +- Project [id#0, value#1, event#3]
         +- Filter (event#3 = New Year)
            +- Join Inner, (date#2 = date#4)
               :- Relation default.fact_table[id#0,value#1,date#2] parquet
               +- Relation default.dim_table[event#3,date#4] parquet



org.apache.spark.sql.catalyst.optimizer.PushDownPredicates => old
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, cast(event#3 as string) AS event#15]
      +- Project [id#0, value#1, event#3]
         +- Filter (event#3 = New Year)
            +- Join Inner, (date#2 = date#4)
               :- Relation default.fact_table[id#0,value#1,date#2] parquet
               +- Relation default.dim_table[event#3,date#4] parquet
==> new
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, cast(event#3 as string) AS event#15]
      +- Project [id#0, value#1, event#3]
         +- Join Inner, (date#2 = date#4)
            :- Relation default.fact_table[id#0,value#1,date#2] parquet
            +- Filter (event#3 = New Year)
               +- Relation default.dim_table[event#3,date#4] parquet



org.apache.spark.sql.catalyst.optimizer.CollapseProject => old
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, cast(event#3 as string) AS event#15]
      +- Project [id#0, value#1, event#3]
         +- Join Inner, (date#2 = date#4)
            :- Relation default.fact_table[id#0,value#1,date#2] parquet
            +- Filter (event#3 = New Year)
               +- Relation default.dim_table[event#3,date#4] parquet
==> new
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, cast(event#3 as string) AS event#15]
      +- Join Inner, (date#2 = date#4)
         :- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- Filter (event#3 = New Year)
            +- Relation default.dim_table[event#3,date#4] parquet



org.apache.spark.sql.catalyst.optimizer.SimplifyCasts => old
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, cast(event#3 as string) AS event#15]
      +- Join Inner, (date#2 = date#4)
         :- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- Filter (event#3 = New Year)
            +- Relation default.dim_table[event#3,date#4] parquet
==> new
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3 AS event#15]
      +- Join Inner, (date#2 = date#4)
         :- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- Filter (event#3 = New Year)
            +- Relation default.dim_table[event#3,date#4] parquet



org.apache.spark.sql.catalyst.optimizer.RemoveRedundantAliases => old
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3 AS event#15]
      +- Join Inner, (date#2 = date#4)
         :- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- Filter (event#3 = New Year)
            +- Relation default.dim_table[event#3,date#4] parquet
==> new
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
      +- Join Inner, (date#2 = date#4)
         :- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- Filter (event#3 = New Year)
            +- Relation default.dim_table[event#3,date#4] parquet



org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints => old
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
      +- Join Inner, (date#2 = date#4)
         :- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- Filter (event#3 = New Year)
            +- Relation default.dim_table[event#3,date#4] parquet
==> new
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
      +- Join Inner, (date#2 = date#4)
         :- Filter isnotnull(date#2)
         :  +- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- Filter isnotnull(date#4)
            +- Filter (isnotnull(event#3) AND (event#3 = New Year))
               +- Relation default.dim_table[event#3,date#4] parquet



org.apache.spark.sql.catalyst.optimizer.PushDownPredicates => old
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
      +- Join Inner, (date#2 = date#4)
         :- Filter isnotnull(date#2)
         :  +- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- Filter isnotnull(date#4)
            +- Filter (isnotnull(event#3) AND (event#3 = New Year))
               +- Relation default.dim_table[event#3,date#4] parquet
==> new
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
      +- Join Inner, (date#2 = date#4)
         :- Filter isnotnull(date#2)
         :  +- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
            +- Relation default.dim_table[event#3,date#4] parquet



13:31:46.981 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  int <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  int
13:31:46.981 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  int <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  int
13:31:46.981 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
13:31:47.126 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 30 ms to list leaf files for 3 paths.
13:31:47.149 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
13:31:47.149 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
13:31:47.192 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 3 ms to list leaf files for 2 paths.
org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions => old
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
      +- Join Inner, (date#2 = date#4)
         :- Filter isnotnull(date#2)
         :  +- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
            +- Relation default.dim_table[event#3,date#4] parquet
==> new
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
      +- Join Inner, (date#2 = date#4)
         :- Project [id#0, value#1, date#2]
         :  +- Filter isnotnull(date#2)
         :     +- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- Project [event#3, date#4]
            +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
               +- Relation default.dim_table[event#3,date#4] parquet



org.apache.spark.sql.catalyst.optimizer.RemoveNoopOperators => old
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
      +- Join Inner, (date#2 = date#4)
         :- Project [id#0, value#1, date#2]
         :  +- Filter isnotnull(date#2)
         :     +- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- Project [event#3, date#4]
            +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
               +- Relation default.dim_table[event#3,date#4] parquet
==> new
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
      +- Join Inner, (date#2 = date#4)
         :- Filter isnotnull(date#2)
         :  +- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
            +- Relation default.dim_table[event#3,date#4] parquet



13:31:47.210 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((date#2 = date#4))
13:31:47.211 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(date#2) | rightKeys:List(date#4)
13:31:47.213 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((date#2 = date#4))
13:31:47.213 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(date#2) | rightKeys:List(date#4)
org.apache.spark.sql.execution.dynamicpruning.PartitionPruning => old
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
      +- Join Inner, (date#2 = date#4)
         :- Filter isnotnull(date#2)
         :  +- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
            +- Relation default.dim_table[event#3,date#4] parquet
==> new
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
      +- Join Inner, (date#2 = date#4)
         :- Filter dynamicpruning#19 [date#2]
         :  :  +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
         :  :     +- Relation default.dim_table[event#3,date#4] parquet
         :  +- Filter isnotnull(date#2)
         :     +- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
            +- Relation default.dim_table[event#3,date#4] parquet



org.apache.spark.sql.catalyst.optimizer.PushDownPredicates => old
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
      +- Join Inner, (date#2 = date#4)
         :- Filter dynamicpruning#19 [date#2]
         :  :  +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
         :  :     +- Relation default.dim_table[event#3,date#4] parquet
         :  +- Filter isnotnull(date#2)
         :     +- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
            +- Relation default.dim_table[event#3,date#4] parquet
==> new
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
      +- Join Inner, (date#2 = date#4)
         :- Filter (isnotnull(date#2) AND dynamicpruning#19 [date#2])
         :  :  +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
         :  :     +- Relation default.dim_table[event#3,date#4] parquet
         :  +- Relation default.fact_table[id#0,value#1,date#2] parquet
         +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
            +- Relation default.dim_table[event#3,date#4] parquet



org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ =>
ReturnAnswer
+- GlobalLimit 21
   +- LocalLimit 21
      +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
         +- Join Inner, (date#2 = date#4)
            :- Filter (isnotnull(date#2) AND dynamicpruning#19 [date#2])
            :  :  +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
            :  :     +- Relation default.dim_table[event#3,date#4] parquet
            :  +- Relation default.fact_table[id#0,value#1,date#2] parquet
            +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
               +- Relation default.dim_table[event#3,date#4] parquet
 ==> 
CollectLimit 21
+- PlanLater Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]


org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ =>
Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
+- Join Inner, (date#2 = date#4)
   :- Filter (isnotnull(date#2) AND dynamicpruning#19 [date#2])
   :  :  +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
   :  :     +- Relation default.dim_table[event#3,date#4] parquet
   :  +- Relation default.fact_table[id#0,value#1,date#2] parquet
   +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
      +- Relation default.dim_table[event#3,date#4] parquet
 ==> 
Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
+- PlanLater Join Inner, (date#2 = date#4)


13:31:47.346 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((date#2 = date#4))
13:31:47.346 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(date#2) | rightKeys:List(date#4)
13:31:47.349 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((date#2 = date#4))
13:31:47.349 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(date#2) | rightKeys:List(date#4)
org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ =>
Join Inner, (date#2 = date#4)
:- Filter (isnotnull(date#2) AND dynamicpruning#19 [date#2])
:  :  +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
:  :     +- Relation default.dim_table[event#3,date#4] parquet
:  +- Relation default.fact_table[id#0,value#1,date#2] parquet
+- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
   +- Relation default.dim_table[event#3,date#4] parquet
 ==> 
BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
:- PlanLater Filter (isnotnull(date#2) AND dynamicpruning#19 [date#2])
+- PlanLater Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))


13:31:47.365 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.DataSourceStrategy - Pruning directories with: isnotnull(date#2),dynamicpruning#19 [date#2]
13:31:47.366 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
13:31:47.367 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
13:31:47.367 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<id: int, value: int>
org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.datasources.FileSourceStrategy$ =>
Filter (isnotnull(date#2) AND dynamicpruning#19 [date#2])
:  +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
:     +- Relation default.dim_table[event#3,date#4] parquet
+- Relation default.fact_table[id#0,value#1,date#2] parquet
 ==> 
FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruning#19 [date#2]], PushedFilters: [], ReadSchema: struct<id:int,value:int>
   +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
      +- Relation default.dim_table[event#3,date#4] parquet


13:31:47.386 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.DataSourceStrategy - Pruning directories with: isnotnull(date#4)
13:31:47.393 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(event),EqualTo(event,New Year)
13:31:47.393 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(event#3),(event#3 = New Year)
13:31:47.394 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<event: string>
org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.datasources.FileSourceStrategy$ =>
Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
+- Relation default.dim_table[event#3,date#4] parquet
 ==> 
Project [event#3, date#4]
+- Filter (isnotnull(event#3) AND (event#3 = New Year))
   +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>


org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ =>
ReturnAnswer
+- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
   +- Relation default.dim_table[event#3,date#4] parquet
 ==> 
PlanLater Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))


13:31:47.414 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.DataSourceStrategy - Pruning directories with: isnotnull(date#4)
13:31:47.414 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(event),EqualTo(event,New Year)
13:31:47.414 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(event#3),(event#3 = New Year)
13:31:47.415 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<event: string>
org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.datasources.FileSourceStrategy$ =>
Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
+- Relation default.dim_table[event#3,date#4] parquet
 ==> 
Project [event#3, date#4]
+- Filter (isnotnull(event#3) AND (event#3 = New Year))
   +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>


13:31:47.432 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan - Adaptive execution enabled for plan: 
Project [event#3, date#4]
+- Filter (isnotnull(event#3) AND (event#3 = New Year))
   +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.RemoveRedundantProjects$ =>
Project [event#3, date#4]
+- Filter (isnotnull(event#3) AND (event#3 = New Year))
   +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
AQE ==> 
Filter (isnotnull(event#3) AND (event#3 = New Year))
+- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>


13:31:47.510 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan - Adaptive execution enabled for plan: 
CollectLimit 21
+- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
   +- BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruning#19 [date#2]], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
      :        +- Relation default.dim_table[event#3,date#4] parquet
      +- Project [event#3, date#4]
         +- Filter (isnotnull(event#3) AND (event#3 = New Year))
            +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.RemoveRedundantProjects$ =>
CollectLimit 21
+- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
   +- BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruningexpression(date#2 IN dynamicpruning#19)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#19, 0, true, Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4)), [date#4]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter (isnotnull(event#3) AND (event#3 = New Year))
      :              +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
      +- Project [event#3, date#4]
         +- Filter (isnotnull(event#3) AND (event#3 = New Year))
            +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
AQE ==> 
CollectLimit 21
+- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
   +- BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruningexpression(date#2 IN dynamicpruning#19)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#19, 0, true, Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4)), [date#4]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter (isnotnull(event#3) AND (event#3 = New Year))
      :              +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
      +- Filter (isnotnull(event#3) AND (event#3 = New Year))
         +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>


org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements =>
CollectLimit 21
+- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
   +- BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruningexpression(date#2 IN dynamicpruning#19)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#19, 0, true, Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4)), [date#4]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter (isnotnull(event#3) AND (event#3 = New Year))
      :              +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
      +- Filter (isnotnull(event#3) AND (event#3 = New Year))
         +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
AQE ==> 
CollectLimit 21
+- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
   +- BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruningexpression(date#2 IN dynamicpruning#19)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#19, 0, true, Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4)), [date#4]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter (isnotnull(event#3) AND (event#3 = New Year))
      :              +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
      +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=36]
         +- Filter (isnotnull(event#3) AND (event#3 = New Year))
            +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>


org.apache.spark.sql.execution.QueryExecution$ - org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan =>
CollectLimit 21
+- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
   +- BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruning#19 [date#2]], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
      :        +- Relation default.dim_table[event#3,date#4] parquet
      +- Project [event#3, date#4]
         +- Filter (isnotnull(event#3) AND (event#3 = New Year))
            +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
 ==> 
AdaptiveSparkPlan isFinalPlan=false
+- CollectLimit 21
   +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
      +- BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
         :- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruningexpression(date#2 IN dynamicpruning#19)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
         :     +- SubqueryAdaptiveBroadcast dynamicpruning#19, 0, true, Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4)), [date#4]
         :        +- AdaptiveSparkPlan isFinalPlan=false
         :           +- Filter (isnotnull(event#3) AND (event#3 = New Year))
         :              +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=36]
            +- Filter (isnotnull(event#3) AND (event#3 = New Year))
               +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>


org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer => old
'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, StructField(id,StringType,true), StructField(value,StringType,true), StructField(event,StringType,true)).toString, getcolumnbyordinal(1, StructField(id,StringType,true), StructField(value,StringType,true), StructField(event,StringType,true)).toString, getcolumnbyordinal(2, StructField(id,StringType,true), StructField(value,StringType,true), StructField(event,StringType,true)).toString, StructField(id,StringType,true), StructField(value,StringType,true), StructField(event,StringType,true))), obj#20: org.apache.spark.sql.Row
+- LocalRelation <empty>, [id#13, value#14, event#15]
==> new
DeserializeToObject createexternalrow(id#13.toString, value#14.toString, event#15.toString, StructField(id,StringType,true), StructField(value,StringType,true), StructField(event,StringType,true)), obj#20: org.apache.spark.sql.Row
+- LocalRelation <empty>, [id#13, value#14, event#15]




======================================================
== AdaptiveSparkPlanExec 
== executeCollect
======================================================

13:31:47.590 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeCollect ==> before get final plan: 
CollectLimit 21
+- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
   +- BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruningexpression(date#2 IN dynamicpruning#19)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#19, 0, true, Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4)), [date#4]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter (isnotnull(event#3) AND (event#3 = New Year))
      :              +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
      +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=36]
         +- Filter (isnotnull(event#3) AND (event#3 = New Year))
            +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions =>
BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=36]
+- Filter (isnotnull(event#3) AND (event#3 = New Year))
   +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
AQE ==> 
BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=46]
+- Filter (isnotnull(event#3) AND (event#3 = New Year))
   +- ColumnarToRow
      +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>


org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.CollapseCodegenStages =>
BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=46]
+- Filter (isnotnull(event#3) AND (event#3 = New Year))
   +- ColumnarToRow
      +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
AQE ==> 
BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=51]
+- *(1) Filter (isnotnull(event#3) AND (event#3 = New Year))
   +- *(1) ColumnarToRow
      +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>


13:31:47.627 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.BroadcastQueryStageExec - Materialize query stage BroadcastQueryStageExec: 0, query stage plan:
BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=51]
+- *(1) Filter (isnotnull(event#3) AND (event#3 = New Year))
   +- *(1) ColumnarToRow
      +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>

13:31:47.953 [broadcast-exchange-0] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private int columnartorow_batchIdx_0;
/* 010 */   private org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[] columnartorow_mutableStateArray_2 = new org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[2];
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] columnartorow_mutableStateArray_3 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 012 */   private org.apache.spark.sql.vectorized.ColumnarBatch[] columnartorow_mutableStateArray_1 = new org.apache.spark.sql.vectorized.ColumnarBatch[1];
/* 013 */   private scala.collection.Iterator[] columnartorow_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */     columnartorow_mutableStateArray_0[0] = inputs[0];
/* 023 */
/* 024 */     columnartorow_mutableStateArray_3[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 64);
/* 025 */     columnartorow_mutableStateArray_3[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 64);
/* 026 */
/* 027 */   }
/* 028 */
/* 029 */   private void columnartorow_nextBatch_0() throws java.io.IOException {
/* 030 */     if (columnartorow_mutableStateArray_0[0].hasNext()) {
/* 031 */       columnartorow_mutableStateArray_1[0] = (org.apache.spark.sql.vectorized.ColumnarBatch)columnartorow_mutableStateArray_0[0].next();
/* 032 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numInputBatches */).add(1);
/* 033 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(columnartorow_mutableStateArray_1[0].numRows());
/* 034 */       columnartorow_batchIdx_0 = 0;
/* 035 */       columnartorow_mutableStateArray_2[0] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(0);
/* 036 */       columnartorow_mutableStateArray_2[1] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(1);
/* 037 */
/* 038 */     }
/* 039 */   }
/* 040 */
/* 041 */   protected void processNext() throws java.io.IOException {
/* 042 */     if (columnartorow_mutableStateArray_1[0] == null) {
/* 043 */       columnartorow_nextBatch_0();
/* 044 */     }
/* 045 */     while ( columnartorow_mutableStateArray_1[0] != null) {
/* 046 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();
/* 047 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;
/* 048 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {
/* 049 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;
/* 050 */         do {
/* 051 */           boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);
/* 052 */           UTF8String columnartorow_value_0 = columnartorow_isNull_0 ? null : (columnartorow_mutableStateArray_2[0].getUTF8String(columnartorow_rowIdx_0));
/* 053 */
/* 054 */           boolean filter_value_2 = !columnartorow_isNull_0;
/* 055 */           if (!filter_value_2) continue;
/* 056 */
/* 057 */           boolean filter_value_3 = false;
/* 058 */           filter_value_3 = columnartorow_value_0.equals(((UTF8String) references[3] /* literal */));
/* 059 */           if (!filter_value_3) continue;
/* 060 */
/* 061 */           ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* numOutputRows */).add(1);
/* 062 */
/* 063 */           boolean columnartorow_isNull_1 = columnartorow_mutableStateArray_2[1].isNullAt(columnartorow_rowIdx_0);
/* 064 */           UTF8String columnartorow_value_1 = columnartorow_isNull_1 ? null : (columnartorow_mutableStateArray_2[1].getUTF8String(columnartorow_rowIdx_0));
/* 065 */           columnartorow_mutableStateArray_3[1].reset();
/* 066 */
/* 067 */           columnartorow_mutableStateArray_3[1].zeroOutNullBytes();
/* 068 */
/* 069 */           columnartorow_mutableStateArray_3[1].write(0, columnartorow_value_0);
/* 070 */
/* 071 */           if (columnartorow_isNull_1) {
/* 072 */             columnartorow_mutableStateArray_3[1].setNullAt(1);
/* 073 */           } else {
/* 074 */             columnartorow_mutableStateArray_3[1].write(1, columnartorow_value_1);
/* 075 */           }
/* 076 */           append((columnartorow_mutableStateArray_3[1].getRow()));
/* 077 */
/* 078 */         } while(false);
/* 079 */         if (shouldStop()) { columnartorow_batchIdx_0 = columnartorow_rowIdx_0 + 1; return; }
/* 080 */       }
/* 081 */       columnartorow_batchIdx_0 = columnartorow_numRows_0;
/* 082 */       columnartorow_mutableStateArray_1[0] = null;
/* 083 */       columnartorow_nextBatch_0();
/* 084 */     }
/* 085 */   }
/* 086 */
/* 087 */ }

===========  BroadcastExchange  ================
13:31:48.122 [broadcast-exchange-0] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 203.549083 ms
13:31:48.217 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 355.1 KiB, free 8.4 GiB)
13:31:48.218 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_0 locally took 60 ms
13:31:48.219 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_0 without replication took 60 ms
13:31:48.610 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 8.4 GiB)
13:31:48.612 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.245, 53481, None)
13:31:48.613 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.31.245:53481 (size: 34.3 KiB, free: 8.4 GiB)
13:31:48.616 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
13:31:48.616 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
13:31:48.617 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_0_piece0 locally took 8 ms
13:31:48.617 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_0_piece0 without replication took 8 ms
13:31:48.617 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Created broadcast 0 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
13:31:48.628 [broadcast-exchange-0] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - Selected 2 partitions out of 2, pruned 0.0% partitions.
13:31:48.641 [broadcast-exchange-0] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 8389676 bytes, open cost is considered as scanning 4194304 bytes.
13:31:48.714 [broadcast-exchange-0] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
13:31:48.726 [broadcast-exchange-0] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
13:31:48.758 [broadcast-exchange-0] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$collect$2
13:31:48.764 [broadcast-exchange-0] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++
13:31:48.848 [broadcast-exchange-0] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
13:31:48.852 [broadcast-exchange-0] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
13:31:48.855 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
13:31:48.856 [broadcast-exchange-0] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 3 took 0.000468 seconds
13:31:48.872 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
13:31:48.888 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions
13:31:48.888 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
13:31:48.888 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:31:48.889 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:31:48.891 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 0 (name=$anonfun$withThreadLocalCaptured$1 at FutureTask.java:266;jobs=0))
13:31:48.891 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
13:31:48.892 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
13:31:48.892 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 0)
13:31:48.919 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 13.9 KiB, free 8.4 GiB)
13:31:48.920 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_1 locally took 1 ms
13:31:48.920 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_1 without replication took 1 ms
13:31:48.921 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 8.4 GiB)
13:31:48.921 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.245, 53481, None)
13:31:48.921 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.31.245:53481 (size: 6.2 KiB, free: 8.4 GiB)
13:31:48.921 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
13:31:48.921 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
13:31:48.921 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_1_piece0 locally took 1 ms
13:31:48.922 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_1_piece0 without replication took 1 ms
13:31:48.922 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1513
13:31:48.936 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))
13:31:48.945 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
13:31:48.972 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 0.0: 0
13:31:48.974 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 1 ms
13:31:48.975 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
13:31:48.982 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
13:31:48.982 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
13:31:48.991 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (192.168.31.245, executor driver, partition 0, PROCESS_LOCAL, 5339 bytes) taskResourceAssignments Map()
13:31:49.007 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
13:31:49.014 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 1
13:31:49.044 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_1
13:31:49.045 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
13:31:49.210 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_table/date=2023-01-02/part-00000-b2e57dcb-0c26-4096-9d5a-0f681fab2b2f.c000.snappy.parquet, range: 0-569, partition values: [2023-01-02]
13:31:49.211 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_0
13:31:49.211 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
13:31:49.529 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  optional binary event (STRING);
}

Parquet clipped schema:
message spark_schema {
  optional binary event (STRING);
}

Parquet requested schema:
message spark_schema {
  optional binary event (STRING);
}

Catalyst requested schema:
root
-- event: string (nullable = true)

       
13:31:49.539 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType(StructField(date,StringType,true)) [2023-01-02]
13:31:49.547 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_table/date=2023-01-01/part-00000-b2e57dcb-0c26-4096-9d5a-0f681fab2b2f.c000.snappy.parquet, range: 0-499, partition values: [2023-01-01]
13:31:49.558 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  optional binary event (STRING);
}

Parquet clipped schema:
message spark_schema {
  optional binary event (STRING);
}

Parquet requested schema:
message spark_schema {
  optional binary event (STRING);
}

Catalyst requested schema:
root
-- event: string (nullable = true)

       
13:31:49.607 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType(StructField(date,StringType,true)) [2023-01-01]
13:31:50.315 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1837 bytes result sent to driver
13:31:50.316 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 0
13:31:50.317 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
13:31:50.318 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
13:31:50.321 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1336 ms on 192.168.31.245 (executor driver) (1/1)
13:31:50.327 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
13:31:50.332 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1.417 s
13:31:50.334 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 0, remaining stages = 0
13:31:50.334 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
13:31:50.334 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
13:31:50.336 [broadcast-exchange-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 1.481481 s
13:31:50.352 [broadcast-exchange-0] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 0 acquired 1024.0 B for org.apache.spark.unsafe.map.BytesToBytesMap@ba92533
13:31:50.358 [broadcast-exchange-0] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(1);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(1));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

13:31:50.359 [broadcast-exchange-0] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(1);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(1));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

13:31:50.369 [broadcast-exchange-0] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.42325 ms
13:31:50.371 [broadcast-exchange-0] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 0 acquired 64.0 MiB for org.apache.spark.unsafe.map.BytesToBytesMap@ba92533
13:31:50.388 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 64.0 MiB, free 8.3 GiB)
13:31:50.388 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_2 locally took 0 ms
13:31:50.388 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_2 without replication took 0 ms
13:31:50.390 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 1
13:31:50.391 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 208.0 B, free 8.3 GiB)
13:31:50.400 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.245, 53481, None)
13:31:50.400 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.31.245:53481 (size: 208.0 B, free: 8.4 GiB)
13:31:50.401 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
13:31:50.401 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
13:31:50.401 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_2_piece0 locally took 9 ms
13:31:50.401 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 1
13:31:50.401 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_2_piece0 without replication took 9 ms
13:31:50.401 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 1
13:31:50.401 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Created broadcast 2 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
13:31:50.402 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_1
13:31:50.403 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 of size 14232 dropped from memory (free 8906617543)
13:31:50.403 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_1_piece0
13:31:50.404 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 of size 6345 dropped from memory (free 8906623888)
13:31:50.404 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.245, 53481, None)
13:31:50.405 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.31.245:53481 in memory (size: 6.2 KiB, free: 8.4 GiB)
13:31:50.405 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
13:31:50.405 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
13:31:50.405 [block-manager-storage-async-thread-pool-2] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 1, response is 0
13:31:50.406 [block-manager-storage-async-thread-pool-2] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.31.245:53476
===========  Broadcastjoinplan_id=51  ===================



13:31:50.410 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((date#2 = date#4))
13:31:50.410 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(date#2) | rightKeys:List(date#4)
org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ =>
ReturnAnswer
+- GlobalLimit 21
   +- LocalLimit 21
      +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
         +- Join Inner, (date#2 = date#4)
            :- Filter (isnotnull(date#2) AND dynamicpruning#19 [date#2])
            :  :  +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
            :  :     +- Relation default.dim_table[event#3,date#4] parquet
            :  +- Relation default.fact_table[id#0,value#1,date#2] parquet
            +- LogicalQueryStage Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4)), BroadcastQueryStage 0
 ==> 
CollectLimit 21
+- PlanLater Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]


org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ =>
Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
+- Join Inner, (date#2 = date#4)
   :- Filter (isnotnull(date#2) AND dynamicpruning#19 [date#2])
   :  :  +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
   :  :     +- Relation default.dim_table[event#3,date#4] parquet
   :  +- Relation default.fact_table[id#0,value#1,date#2] parquet
   +- LogicalQueryStage Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4)), BroadcastQueryStage 0
 ==> 
Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
+- PlanLater Join Inner, (date#2 = date#4)


13:31:50.413 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((date#2 = date#4))
13:31:50.413 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(date#2) | rightKeys:List(date#4)
org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ =>
Join Inner, (date#2 = date#4)
:- Filter (isnotnull(date#2) AND dynamicpruning#19 [date#2])
:  :  +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
:  :     +- Relation default.dim_table[event#3,date#4] parquet
:  +- Relation default.fact_table[id#0,value#1,date#2] parquet
+- LogicalQueryStage Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4)), BroadcastQueryStage 0
 ==> 
BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
:- PlanLater Filter (isnotnull(date#2) AND dynamicpruning#19 [date#2])
+- PlanLater LogicalQueryStage Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4)), BroadcastQueryStage 0


13:31:50.414 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.DataSourceStrategy - Pruning directories with: isnotnull(date#2),dynamicpruning#19 [date#2]
13:31:50.414 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
13:31:50.415 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
13:31:50.415 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<id: int, value: int>
org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.datasources.FileSourceStrategy$ =>
Filter (isnotnull(date#2) AND dynamicpruning#19 [date#2])
:  +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
:     +- Relation default.dim_table[event#3,date#4] parquet
+- Relation default.fact_table[id#0,value#1,date#2] parquet
 ==> 
FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruning#19 [date#2]], PushedFilters: [], ReadSchema: struct<id:int,value:int>
   +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
      +- Relation default.dim_table[event#3,date#4] parquet


org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2 - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ =>
LogicalQueryStage Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4)), BroadcastQueryStage 0
 ==> 
BroadcastQueryStage 0
+- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=51]
   +- *(1) Filter (isnotnull(event#3) AND (event#3 = New Year))
      +- *(1) ColumnarToRow
         +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>


org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.adaptive.PlanAdaptiveSubqueries =>
CollectLimit 21
+- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
   +- BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruning#19 [date#2]], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4))
      :        +- Relation default.dim_table[event#3,date#4] parquet
      +- BroadcastQueryStage 0
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=51]
            +- *(1) Filter (isnotnull(event#3) AND (event#3 = New Year))
               +- *(1) ColumnarToRow
                  +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
AQE ==> 
CollectLimit 21
+- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
   +- BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruningexpression(date#2 IN dynamicpruning#19)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#19, 0, true, Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4)), [date#4]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter (isnotnull(event#3) AND (event#3 = New Year))
      :              +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
      +- BroadcastQueryStage 0
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=51]
            +- *(1) Filter (isnotnull(event#3) AND (event#3 = New Year))
               +- *(1) ColumnarToRow
                  +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>


optimizeQueryStage - org.apache.spark.sql.execution.adaptive.PlanAdaptiveDynamicPruningFilters => old
CollectLimit 21
+- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
   +- BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruningexpression(date#2 IN dynamicpruning#19)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#19, 0, true, Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4)), [date#4]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter (isnotnull(event#3) AND (event#3 = New Year))
      :              +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
      +- BroadcastQueryStage 0
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=51]
            +- *(1) Filter (isnotnull(event#3) AND (event#3 = New Year))
               +- *(1) ColumnarToRow
                  +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
AQE ==> new
CollectLimit 21
+- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
   +- BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruningexpression(date#2 IN dynamicpruning#19)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryBroadcast dynamicpruning#19, 0, [date#4], [id=#84]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=78]
      :              +- Filter (isnotnull(event#3) AND (event#3 = New Year))
      :                 +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
      +- BroadcastQueryStage 0
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=51]
            +- *(1) Filter (isnotnull(event#3) AND (event#3 = New Year))
               +- *(1) ColumnarToRow
                  +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>



org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions =>
CollectLimit 21
+- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
   +- BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruningexpression(date#2 IN dynamicpruning#19)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryBroadcast dynamicpruning#19, 0, [date#4], [id=#84]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=78]
      :              +- Filter (isnotnull(event#3) AND (event#3 = New Year))
      :                 +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
      +- BroadcastQueryStage 0
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=51]
            +- *(1) Filter (isnotnull(event#3) AND (event#3 = New Year))
               +- *(1) ColumnarToRow
                  +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
AQE ==> 
CollectLimit 21
+- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
   +- BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
      :- ColumnarToRow
      :  +- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruningexpression(date#2 IN dynamicpruning#19)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :        +- SubqueryBroadcast dynamicpruning#19, 0, [date#4], [id=#84]
      :           +- AdaptiveSparkPlan isFinalPlan=false
      :              +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=78]
      :                 +- Filter (isnotnull(event#3) AND (event#3 = New Year))
      :                    +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
      +- BroadcastQueryStage 0
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=51]
            +- *(1) Filter (isnotnull(event#3) AND (event#3 = New Year))
               +- *(1) ColumnarToRow
                  +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>


org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.CollapseCodegenStages =>
CollectLimit 21
+- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
   +- BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
      :- ColumnarToRow
      :  +- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruningexpression(date#2 IN dynamicpruning#19)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :        +- SubqueryBroadcast dynamicpruning#19, 0, [date#4], [id=#84]
      :           +- AdaptiveSparkPlan isFinalPlan=false
      :              +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=78]
      :                 +- Filter (isnotnull(event#3) AND (event#3 = New Year))
      :                    +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
      +- BroadcastQueryStage 0
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=51]
            +- *(1) Filter (isnotnull(event#3) AND (event#3 = New Year))
               +- *(1) ColumnarToRow
                  +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
AQE ==> 
CollectLimit 21
+- *(2) Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
   +- *(2) BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
      :- *(2) ColumnarToRow
      :  +- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruningexpression(date#2 IN dynamicpruning#19)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :        +- SubqueryBroadcast dynamicpruning#19, 0, [date#4], [id=#84]
      :           +- AdaptiveSparkPlan isFinalPlan=false
      :              +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=78]
      :                 +- Filter (isnotnull(event#3) AND (event#3 = New Year))
      :                    +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
      +- BroadcastQueryStage 0
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=51]
            +- *(1) Filter (isnotnull(event#3) AND (event#3 = New Year))
               +- *(1) ColumnarToRow
                  +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>


# before get final plan: 
# CollectLimit 21
# +- Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
#    +- BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
#       :- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruningexpression(date#2 IN dynamicpruning#19)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
#       :     +- SubqueryAdaptiveBroadcast dynamicpruning#19, 0, true, Filter ((isnotnull(event#3) AND (event#3 = New Year)) AND isnotnull(date#4)), [date#4]
#       :        +- AdaptiveSparkPlan isFinalPlan=false
#       :           +- Filter (isnotnull(event#3) AND (event#3 = New Year))
#       :              +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
#       +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=36]
#          +- Filter (isnotnull(event#3) AND (event#3 = New Year))
#             +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>


13:31:50.436 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeCollect ==> after get final plan: 
CollectLimit 21
+- *(2) Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
   +- *(2) BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
      :- *(2) ColumnarToRow
      :  +- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruningexpression(date#2 IN dynamicpruning#19)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :        +- SubqueryBroadcast dynamicpruning#19, 0, [date#4], [id=#84]
      :           +- AdaptiveSparkPlan isFinalPlan=false
      :              +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=78]
      :                 +- Filter (isnotnull(event#3) AND (event#3 = New Year))
      :                    +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
      +- BroadcastQueryStage 0
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=51]
            +- *(1) Filter (isnotnull(event#3) AND (event#3 = New Year))
               +- *(1) ColumnarToRow
                  +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>

===============plan_id=51 =================================





===============plan_id=78 =================================
== plan
======================================================
13:31:50.439 [dynamicpruning-0] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.doExecuteBroadcast ==> before get final plan: 
BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=78]
+- Filter (isnotnull(event#3) AND (event#3 = New Year))
   +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions =>
BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=78]
+- Filter (isnotnull(event#3) AND (event#3 = New Year))
   +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
AQE ==> 
BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=103]
+- Filter (isnotnull(event#3) AND (event#3 = New Year))
   +- ColumnarToRow
      +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>


13:31:50.440 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_2
13:31:50.440 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.CollapseCodegenStages =>
BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=103]
+- Filter (isnotnull(event#3) AND (event#3 = New Year))
   +- ColumnarToRow
      +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
AQE ==> 
BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=108]
+- *(1) Filter (isnotnull(event#3) AND (event#3 = New Year))
   +- *(1) ColumnarToRow
      +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>


13:31:50.446 [dynamicpruning-0] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.doExecuteBroadcast ==> after get final plan: 
BroadcastQueryStage 1
+- ReusedExchange [event#3, date#4], BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=51]

13:31:50.447 [dynamicpruning-0] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.doExecuteBroadcast was finished.
13:31:50.447 [dynamicpruning-0] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - Final plan: BroadcastQueryStage 1
+- ReusedExchange [event#3, date#4], BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=51]

13:31:50.448 [dynamicpruning-0] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.doExecuteBroadcast.finalPlanUpdate was finished.
===============plan_id=78 doExecuteBroadcast=================================




13:31:50.452 [dynamicpruning-0] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

13:31:50.454 [dynamicpruning-0] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }


=================join scan table Broadcast ========================
13:31:50.455 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private int columnartorow_batchIdx_0;
/* 010 */   private org.apache.spark.sql.execution.joins.UnsafeHashedRelation bhj_relation_0;
/* 011 */   private org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[] columnartorow_mutableStateArray_2 = new org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[3];
/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] columnartorow_mutableStateArray_3 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[4];
/* 013 */   private org.apache.spark.sql.vectorized.ColumnarBatch[] columnartorow_mutableStateArray_1 = new org.apache.spark.sql.vectorized.ColumnarBatch[1];
/* 014 */   private scala.collection.Iterator[] columnartorow_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 015 */
/* 016 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 017 */     this.references = references;
/* 018 */   }
/* 019 */
/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 021 */     partitionIndex = index;
/* 022 */     this.inputs = inputs;
/* 023 */     columnartorow_mutableStateArray_0[0] = inputs[0];
/* 024 */
/* 025 */     columnartorow_mutableStateArray_3[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);
/* 026 */
/* 027 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.UnsafeHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[2] /* broadcast */).value()).asReadOnlyCopy();
/* 028 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());
/* 029 */
/* 030 */     columnartorow_mutableStateArray_3[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 031 */     columnartorow_mutableStateArray_3[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(5, 96);
/* 032 */     columnartorow_mutableStateArray_3[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);
/* 033 */
/* 034 */   }
/* 035 */
/* 036 */   private void columnartorow_nextBatch_0() throws java.io.IOException {
/* 037 */     if (columnartorow_mutableStateArray_0[0].hasNext()) {
/* 038 */       columnartorow_mutableStateArray_1[0] = (org.apache.spark.sql.vectorized.ColumnarBatch)columnartorow_mutableStateArray_0[0].next();
/* 039 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numInputBatches */).add(1);
/* 040 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(columnartorow_mutableStateArray_1[0].numRows());
/* 041 */       columnartorow_batchIdx_0 = 0;
/* 042 */       columnartorow_mutableStateArray_2[0] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(0);
/* 043 */       columnartorow_mutableStateArray_2[1] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(1);
/* 044 */       columnartorow_mutableStateArray_2[2] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(2);
/* 045 */
/* 046 */     }
/* 047 */   }
/* 048 */
/* 049 */   protected void processNext() throws java.io.IOException {
/* 050 */     if (columnartorow_mutableStateArray_1[0] == null) {
/* 051 */       columnartorow_nextBatch_0();
/* 052 */     }
/* 053 */     while ( columnartorow_mutableStateArray_1[0] != null) {
/* 054 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();
/* 055 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;
/* 056 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {
/* 057 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;
/* 058 */         boolean columnartorow_isNull_2 = columnartorow_mutableStateArray_2[2].isNullAt(columnartorow_rowIdx_0);
/* 059 */         UTF8String columnartorow_value_2 = columnartorow_isNull_2 ? null : (columnartorow_mutableStateArray_2[2].getUTF8String(columnartorow_rowIdx_0));
/* 060 */
/* 061 */         // generate join key for stream side
/* 062 */         columnartorow_mutableStateArray_3[1].reset();
/* 063 */
/* 064 */         columnartorow_mutableStateArray_3[1].zeroOutNullBytes();
/* 065 */
/* 066 */         if (columnartorow_isNull_2) {
/* 067 */           columnartorow_mutableStateArray_3[1].setNullAt(0);
/* 068 */         } else {
/* 069 */           columnartorow_mutableStateArray_3[1].write(0, columnartorow_value_2);
/* 070 */         }
/* 071 */         // find matches from HashedRelation
/* 072 */         UnsafeRow bhj_buildRow_0 = (columnartorow_mutableStateArray_3[1].getRow()).anyNull() ? null: (UnsafeRow)bhj_relation_0.getValue((columnartorow_mutableStateArray_3[1].getRow()));
/* 073 */         if (bhj_buildRow_0 != null) {
/* 074 */           {
/* 075 */             ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* numOutputRows */).add(1);
/* 076 */
/* 077 */             // common sub-expressions
/* 078 */
/* 079 */             boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);
/* 080 */             int columnartorow_value_0 = columnartorow_isNull_0 ? -1 : (columnartorow_mutableStateArray_2[0].getInt(columnartorow_rowIdx_0));
/* 081 */             boolean project_isNull_0 = columnartorow_isNull_0;
/* 082 */             UTF8String project_value_0 = null;
/* 083 */             if (!columnartorow_isNull_0) {
/* 084 */               project_value_0 = UTF8String.fromString(String.valueOf(columnartorow_value_0));
/* 085 */             }
/* 086 */             boolean columnartorow_isNull_1 = columnartorow_mutableStateArray_2[1].isNullAt(columnartorow_rowIdx_0);
/* 087 */             int columnartorow_value_1 = columnartorow_isNull_1 ? -1 : (columnartorow_mutableStateArray_2[1].getInt(columnartorow_rowIdx_0));
/* 088 */             boolean project_isNull_2 = columnartorow_isNull_1;
/* 089 */             UTF8String project_value_2 = null;
/* 090 */             if (!columnartorow_isNull_1) {
/* 091 */               project_value_2 = UTF8String.fromString(String.valueOf(columnartorow_value_1));
/* 092 */             }
/* 093 */             UTF8String bhj_value_1 = bhj_buildRow_0.getUTF8String(0);
/* 094 */             columnartorow_mutableStateArray_3[3].reset();
/* 095 */
/* 096 */             columnartorow_mutableStateArray_3[3].zeroOutNullBytes();
/* 097 */
/* 098 */             if (project_isNull_0) {
/* 099 */               columnartorow_mutableStateArray_3[3].setNullAt(0);
/* 100 */             } else {
/* 101 */               columnartorow_mutableStateArray_3[3].write(0, project_value_0);
/* 102 */             }
/* 103 */
/* 104 */             if (project_isNull_2) {
/* 105 */               columnartorow_mutableStateArray_3[3].setNullAt(1);
/* 106 */             } else {
/* 107 */               columnartorow_mutableStateArray_3[3].write(1, project_value_2);
/* 108 */             }
/* 109 */
/* 110 */             if (false) {
/* 111 */               columnartorow_mutableStateArray_3[3].setNullAt(2);
/* 112 */             } else {
/* 113 */               columnartorow_mutableStateArray_3[3].write(2, bhj_value_1);
/* 114 */             }
/* 115 */             append((columnartorow_mutableStateArray_3[3].getRow()));
/* 116 */
/* 117 */           }
/* 118 */         }
/* 119 */         if (shouldStop()) { columnartorow_batchIdx_0 = columnartorow_rowIdx_0 + 1; return; }
/* 120 */       }
/* 121 */       columnartorow_batchIdx_0 = columnartorow_numRows_0;
/* 122 */       columnartorow_mutableStateArray_1[0] = null;
/* 123 */       columnartorow_nextBatch_0();
/* 124 */     }
/* 125 */   }
/* 126 */
/* 127 */ }

13:31:50.461 [dynamicpruning-0] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.444667 ms
13:31:50.476 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 23.992375 ms


13:31:50.482 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 355.4 KiB, free 8.3 GiB)
13:31:50.482 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_3 locally took 2 ms
13:31:50.482 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_3 without replication took 2 ms
13:31:50.491 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 8.3 GiB)
13:31:50.491 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.245, 53481, None)
13:31:50.491 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.31.245:53481 (size: 34.4 KiB, free: 8.4 GiB)
13:31:50.492 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
13:31:50.492 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_3_piece0
13:31:50.492 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_3_piece0 locally took 1 ms
13:31:50.492 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_3_piece0 without replication took 1 ms
13:31:50.492 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.SparkContext - Created broadcast 3 from show at PhysicalPlanSpec.scala:189
13:31:50.500 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate - Generated predicate 'dynamicpruningexpression(input[0, string, true] IN dynamicpruning#19)':
/* 001 */ public SpecificPredicate generate(Object[] references) {
/* 002 */   return new SpecificPredicate(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.BasePredicate {
/* 006 */   private final Object[] references;
/* 007 */
/* 008 */
/* 009 */   public SpecificPredicate(Object[] references) {
/* 010 */     this.references = references;
/* 011 */
/* 012 */   }
/* 013 */
/* 014 */   public void initialize(int partitionIndex) {
/* 015 */
/* 016 */   }
/* 017 */
/* 018 */   public boolean eval(InternalRow i) {
/* 019 */
/* 020 */     boolean isNull_2 = i.isNullAt(0);
/* 021 */     UTF8String value_2 = isNull_2 ?
/* 022 */     null : (i.getUTF8String(0));
/* 023 */     boolean isNull_1 = isNull_2;
/* 024 */     boolean value_1 = false;
/* 025 */
/* 026 */     if (!isNull_2) {
/* 027 */
/* 028 */       value_1 = ((scala.collection.immutable.Set$Set1) references[0] /* set */).contains(value_2);
/* 029 */
/* 030 */
/* 031 */     }
/* 032 */     return !isNull_1 && value_1;
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */ }

13:31:50.500 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public SpecificPredicate generate(Object[] references) {
/* 002 */   return new SpecificPredicate(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.BasePredicate {
/* 006 */   private final Object[] references;
/* 007 */
/* 008 */
/* 009 */   public SpecificPredicate(Object[] references) {
/* 010 */     this.references = references;
/* 011 */
/* 012 */   }
/* 013 */
/* 014 */   public void initialize(int partitionIndex) {
/* 015 */
/* 016 */   }
/* 017 */
/* 018 */   public boolean eval(InternalRow i) {
/* 019 */
/* 020 */     boolean isNull_2 = i.isNullAt(0);
/* 021 */     UTF8String value_2 = isNull_2 ?
/* 022 */     null : (i.getUTF8String(0));
/* 023 */     boolean isNull_1 = isNull_2;
/* 024 */     boolean value_1 = false;
/* 025 */
/* 026 */     if (!isNull_2) {
/* 027 */
/* 028 */       value_1 = ((scala.collection.immutable.Set$Set1) references[0] /* set */).contains(value_2);
/* 029 */
/* 030 */
/* 031 */     }
/* 032 */     return !isNull_1 && value_1;
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */ }

13:31:50.507 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.7975 ms

==================rdd Job====================
13:31:50.508 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - Selected 3 partitions out of 3, pruned 0.0% partitions.
13:31:50.508 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194970 bytes, open cost is considered as scanning 4194304 bytes.
13:31:50.510 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
13:31:50.513 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
13:31:50.516 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$executeTake$2
13:31:50.522 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++
13:31:50.523 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
13:31:50.525 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
13:31:50.525 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.SparkContext - Starting job: show at PhysicalPlanSpec.scala:189
13:31:50.525 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 7 took 0.000067 seconds
13:31:50.525 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
13:31:50.525 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at PhysicalPlanSpec.scala:189) with 1 output partitions
13:31:50.525 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at PhysicalPlanSpec.scala:189)
13:31:50.525 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:31:50.526 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:31:50.527 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 1 (name=show at PhysicalPlanSpec.scala:189;jobs=1))
13:31:50.527 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
13:31:50.527 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[7] at show at PhysicalPlanSpec.scala:189), which has no missing parents
13:31:50.527 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 1)
13:31:50.532 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 15.9 KiB, free 8.3 GiB)
13:31:50.532 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_4 locally took 0 ms
13:31:50.532 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_4 without replication took 0 ms
13:31:50.533 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 8.3 GiB)
13:31:50.533 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, 192.168.31.245, 53481, None)
13:31:50.533 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.31.245:53481 (size: 6.8 KiB, free: 8.4 GiB)
13:31:50.533 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
13:31:50.533 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_4_piece0
13:31:50.533 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_4_piece0 locally took 1 ms
13:31:50.533 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_4_piece0 without replication took 1 ms
13:31:50.534 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1513
13:31:50.534 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at show at PhysicalPlanSpec.scala:189) (first 15 tasks are for partitions Vector(0))
13:31:50.534 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
13:31:50.534 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 1.0: 0
13:31:50.534 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
13:31:50.535 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 1.0: NO_PREF, ANY
13:31:50.535 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
13:31:50.535 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (192.168.31.245, executor driver, partition 0, PROCESS_LOCAL, 5103 bytes) taskResourceAssignments Map()
13:31:50.536 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
13:31:50.536 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (1, 0) -> 1
13:31:50.537 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_4
13:31:50.537 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
13:31:50.542 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_table/date=2023-01-01/part-00000-ac5e4448-1617-4d19-9bc2-027a719bfeae.c000.snappy.parquet, range: 0-666, partition values: [2023-01-01]
13:31:50.542 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_3
13:31:50.542 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
13:31:50.550 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Parquet clipped schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Parquet requested schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Catalyst requested schema:
root
-- id: integer (nullable = true)
-- value: integer (nullable = true)

       
13:31:50.551 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType(StructField(date,StringType,true)) [2023-01-01]
13:31:50.561 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 1804 bytes result sent to driver
13:31:50.561 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (1, 0) -> 0
13:31:50.562 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
13:31:50.562 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
13:31:50.563 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 27 ms on 192.168.31.245 (executor driver) (1/1)
13:31:50.563 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
13:31:50.563 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at PhysicalPlanSpec.scala:189) finished in 0.035 s
13:31:50.563 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 1, remaining stages = 0
13:31:50.563 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
13:31:50.563 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
13:31:50.564 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at PhysicalPlanSpec.scala:189, took 0.038786 s
13:31:50.564 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeCollect was finished.
13:31:50.568 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - Final plan: CollectLimit 21
+- *(2) Project [cast(id#0 as string) AS id#13, cast(value#1 as string) AS value#14, event#3]
   +- *(2) BroadcastHashJoin [date#2], [date#4], Inner, BuildRight, false
      :- *(2) ColumnarToRow
      :  +- FileScan parquet default.fact_table[id#0,value#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/fact_tab..., PartitionFilters: [isnotnull(date#2), dynamicpruningexpression(date#2 IN dynamicpruning#19)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :        +- SubqueryBroadcast dynamicpruning#19, 0, [date#4], [id=#84]
      :           +- AdaptiveSparkPlan isFinalPlan=true
                     +- == Final Plan ==
                        BroadcastQueryStage 1
                        +- ReusedExchange [event#3, date#4], BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=51]
                     +- == Initial Plan ==
                        BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=78]
                        +- Filter (isnotnull(event#3) AND (event#3 = New Year))
                           +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>
      +- BroadcastQueryStage 0
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=51]
            +- *(1) Filter (isnotnull(event#3) AND (event#3 = New Year))
               +- *(1) ColumnarToRow
                  +- FileScan parquet default.dim_table[event#3,date#4] Batched: true, DataFilters: [isnotnull(event#3), (event#3 = New Year)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/juntzhang/src/github/apache/spark/spark-warehouse/dim_tabl..., PartitionFilters: [isnotnull(date#4)], PushedFilters: [IsNotNull(event), EqualTo(event,New Year)], ReadSchema: struct<event:string>

13:31:50.568 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeCollect.finalPlanUpdate was finished.
==================executeCollect ====================





==================show row====================
13:31:50.598 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(value,StringType,true), StructField(event,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);
/* 024 */     if (false) {
/* 025 */       mutableRow.setNullAt(0);
/* 026 */     } else {
/* 027 */
/* 028 */       mutableRow.update(0, value_7);
/* 029 */     }
/* 030 */
/* 031 */     return mutableRow;
/* 032 */   }
/* 033 */
/* 034 */
/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {
/* 036 */     Object[] values_0 = new Object[3];
/* 037 */
/* 038 */     boolean isNull_2 = i.isNullAt(0);
/* 039 */     UTF8String value_2 = isNull_2 ?
/* 040 */     null : (i.getUTF8String(0));
/* 041 */     boolean isNull_1 = true;
/* 042 */     java.lang.String value_1 = null;
/* 043 */     if (!isNull_2) {
/* 044 */       isNull_1 = false;
/* 045 */       if (!isNull_1) {
/* 046 */
/* 047 */         Object funcResult_0 = null;
/* 048 */         funcResult_0 = value_2.toString();
/* 049 */         value_1 = (java.lang.String) funcResult_0;
/* 050 */
/* 051 */       }
/* 052 */     }
/* 053 */     if (isNull_1) {
/* 054 */       values_0[0] = null;
/* 055 */     } else {
/* 056 */       values_0[0] = value_1;
/* 057 */     }
/* 058 */
/* 059 */     boolean isNull_4 = i.isNullAt(1);
/* 060 */     UTF8String value_4 = isNull_4 ?
/* 061 */     null : (i.getUTF8String(1));
/* 062 */     boolean isNull_3 = true;
/* 063 */     java.lang.String value_3 = null;
/* 064 */     if (!isNull_4) {
/* 065 */       isNull_3 = false;
/* 066 */       if (!isNull_3) {
/* 067 */
/* 068 */         Object funcResult_1 = null;
/* 069 */         funcResult_1 = value_4.toString();
/* 070 */         value_3 = (java.lang.String) funcResult_1;
/* 071 */
/* 072 */       }
/* 073 */     }
/* 074 */     if (isNull_3) {
/* 075 */       values_0[1] = null;
/* 076 */     } else {
/* 077 */       values_0[1] = value_3;
/* 078 */     }
/* 079 */
/* 080 */     boolean isNull_6 = i.isNullAt(2);
/* 081 */     UTF8String value_6 = isNull_6 ?
/* 082 */     null : (i.getUTF8String(2));
/* 083 */     boolean isNull_5 = true;
/* 084 */     java.lang.String value_5 = null;
/* 085 */     if (!isNull_6) {
/* 086 */       isNull_5 = false;
/* 087 */       if (!isNull_5) {
/* 088 */
/* 089 */         Object funcResult_2 = null;
/* 090 */         funcResult_2 = value_6.toString();
/* 091 */         value_5 = (java.lang.String) funcResult_2;
/* 092 */
/* 093 */       }
/* 094 */     }
/* 095 */     if (isNull_5) {
/* 096 */       values_0[2] = null;
/* 097 */     } else {
/* 098 */       values_0[2] = value_5;
/* 099 */     }
/* 100 */
/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 102 */
/* 103 */     return value_0;
/* 104 */   }
/* 105 */
/* 106 */ }

13:31:50.600 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);
/* 024 */     if (false) {
/* 025 */       mutableRow.setNullAt(0);
/* 026 */     } else {
/* 027 */
/* 028 */       mutableRow.update(0, value_7);
/* 029 */     }
/* 030 */
/* 031 */     return mutableRow;
/* 032 */   }
/* 033 */
/* 034 */
/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {
/* 036 */     Object[] values_0 = new Object[3];
/* 037 */
/* 038 */     boolean isNull_2 = i.isNullAt(0);
/* 039 */     UTF8String value_2 = isNull_2 ?
/* 040 */     null : (i.getUTF8String(0));
/* 041 */     boolean isNull_1 = true;
/* 042 */     java.lang.String value_1 = null;
/* 043 */     if (!isNull_2) {
/* 044 */       isNull_1 = false;
/* 045 */       if (!isNull_1) {
/* 046 */
/* 047 */         Object funcResult_0 = null;
/* 048 */         funcResult_0 = value_2.toString();
/* 049 */         value_1 = (java.lang.String) funcResult_0;
/* 050 */
/* 051 */       }
/* 052 */     }
/* 053 */     if (isNull_1) {
/* 054 */       values_0[0] = null;
/* 055 */     } else {
/* 056 */       values_0[0] = value_1;
/* 057 */     }
/* 058 */
/* 059 */     boolean isNull_4 = i.isNullAt(1);
/* 060 */     UTF8String value_4 = isNull_4 ?
/* 061 */     null : (i.getUTF8String(1));
/* 062 */     boolean isNull_3 = true;
/* 063 */     java.lang.String value_3 = null;
/* 064 */     if (!isNull_4) {
/* 065 */       isNull_3 = false;
/* 066 */       if (!isNull_3) {
/* 067 */
/* 068 */         Object funcResult_1 = null;
/* 069 */         funcResult_1 = value_4.toString();
/* 070 */         value_3 = (java.lang.String) funcResult_1;
/* 071 */
/* 072 */       }
/* 073 */     }
/* 074 */     if (isNull_3) {
/* 075 */       values_0[1] = null;
/* 076 */     } else {
/* 077 */       values_0[1] = value_3;
/* 078 */     }
/* 079 */
/* 080 */     boolean isNull_6 = i.isNullAt(2);
/* 081 */     UTF8String value_6 = isNull_6 ?
/* 082 */     null : (i.getUTF8String(2));
/* 083 */     boolean isNull_5 = true;
/* 084 */     java.lang.String value_5 = null;
/* 085 */     if (!isNull_6) {
/* 086 */       isNull_5 = false;
/* 087 */       if (!isNull_5) {
/* 088 */
/* 089 */         Object funcResult_2 = null;
/* 090 */         funcResult_2 = value_6.toString();
/* 091 */         value_5 = (java.lang.String) funcResult_2;
/* 092 */
/* 093 */       }
/* 094 */     }
/* 095 */     if (isNull_5) {
/* 096 */       values_0[2] = null;
/* 097 */     } else {
/* 098 */       values_0[2] = value_5;
/* 099 */     }
/* 100 */
/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 102 */
/* 103 */     return value_0;
/* 104 */   }
/* 105 */
/* 106 */ }

13:31:50.611 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.29925 ms
+---+-----+--------+
| id|value|   event|
+---+-----+--------+
|  1|  100|New Year|
+---+-----+--------+

13:31:53.120 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 0
13:31:53.121 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 0
13:31:53.121 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 0
13:31:53.121 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_0_piece0
13:31:53.122 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 of size 35174 dropped from memory (free 8906236677)
13:31:53.122 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.245, 53481, None)
13:31:53.122 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.31.245:53481 in memory (size: 34.3 KiB, free: 8.4 GiB)
13:31:53.122 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
13:31:53.122 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
13:31:53.122 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_0
13:31:53.122 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 of size 363656 dropped from memory (free 8906600333)
13:31:53.123 [block-manager-storage-async-thread-pool-5] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 0, response is 0
13:31:53.123 [block-manager-storage-async-thread-pool-5] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.31.245:53476
13:31:53.123 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 4
13:31:53.124 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 4
13:31:53.124 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 4
13:31:53.124 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_4_piece0
13:31:53.124 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 of size 6946 dropped from memory (free 8906607279)
13:31:53.124 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, 192.168.31.245, 53481, None)
13:31:53.124 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.31.245:53481 in memory (size: 6.8 KiB, free: 8.4 GiB)
13:31:53.125 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
13:31:53.125 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_4_piece0
13:31:53.125 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_4
13:31:53.125 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 of size 16288 dropped from memory (free 8906623567)
13:31:53.125 [block-manager-storage-async-thread-pool-8] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 4, response is 0
13:31:53.125 [block-manager-storage-async-thread-pool-8] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.31.245:53476
13:31:53.126 [executor-heartbeater] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - removing (1, 0) from stageTCMP
13:31:53.126 [executor-heartbeater] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - removing (0, 0) from stageTCMP
13:31:53.126 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 3
13:31:53.126 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 3
13:31:53.126 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 3
13:31:53.126 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_3
13:31:53.126 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 of size 363912 dropped from memory (free 8906987479)
13:31:53.127 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_3_piece0
13:31:53.127 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 of size 35239 dropped from memory (free 8907022718)
13:31:53.127 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.245, 53481, None)
13:31:53.127 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 192.168.31.245:53481 in memory (size: 34.4 KiB, free: 8.4 GiB)
13:31:53.127 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
13:31:53.127 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_3_piece0
13:31:53.127 [block-manager-storage-async-thread-pool-11] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 3, response is 0
13:31:53.128 [block-manager-storage-async-thread-pool-11] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.31.245:53476
13:31:53.128 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 2
13:31:53.128 [block-manager-storage-async-thread-pool-12] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 2
13:31:53.128 [block-manager-storage-async-thread-pool-12] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 2
13:31:53.128 [block-manager-storage-async-thread-pool-12] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_2_piece0
13:31:53.129 [block-manager-storage-async-thread-pool-12] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 of size 208 dropped from memory (free 8907022926)
13:31:53.129 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.245, 53481, None)
13:31:53.129 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.31.245:53481 in memory (size: 208.0 B, free: 8.4 GiB)
13:31:53.129 [block-manager-storage-async-thread-pool-12] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
13:31:53.129 [block-manager-storage-async-thread-pool-12] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
13:31:53.129 [block-manager-storage-async-thread-pool-12] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_2
13:31:53.129 [block-manager-storage-async-thread-pool-12] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 of size 67109912 dropped from memory (free 8974132838)
13:31:53.129 [block-manager-storage-async-thread-pool-14] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 2, response is 0
13:31:53.130 [block-manager-storage-async-thread-pool-14] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.31.245:53476

14:31:50.817 [ScalaTest-run] INFO  org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.31.245:4040
14:31:50.830 [dispatcher-event-loop-0] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
14:31:50.879 [ScalaTest-run] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
14:31:50.879 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManager - BlockManager stopped
14:31:50.882 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
14:31:50.884 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
14:31:50.919 [ScalaTest-run] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext

14:31:50.925 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager - Shutdown hook called
14:31:50.925 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager - Deleting directory /private/var/folders/dc/_x4vzy4100s9qz48fghk10h80000gq/T/spark-ac6106ee-3999-4b35-a98e-5f749ba555bd
Disconnected from the target VM, address: '127.0.0.1:53460', transport: 'socket'

Process finished with exit code 0
